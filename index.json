[{"authors":["Irena Maček","Dave R Clark","Nataša Šibanc","Gerald Moser","Dominik Vodnik","Christoph Müller","Alex J Dumbrell"],"categories":null,"content":"","date":1561330800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572957317,"objectID":"e4211cf87a79aa63ffaa8deb9791221f","permalink":"/publication/macek_et_al/","publishdate":"2019-06-24T00:00:00+01:00","relpermalink":"/publication/macek_et_al/","section":"publication","summary":"The ecological impacts of long‐term elevated atmospheric CO2 (eCO2) levels on soil microbiota remain largely unknown. This is particularly true for the arbuscular mycorrhizal (AM) fungi, which form mutualistic associations with over two‐thirds of terrestrial plant species and are entirely dependent on their plant hosts for carbon. Here, we use high‐resolution amplicon sequencing (Illumina, HiSeq) to quantify the response of AM fungal communities to the longest running (15 years) free‐air carbon dioxide enrichment (FACE) experiment in the Northern Hemisphere (GiFACE); providing the first evaluation of these responses from old‐growth (100 years) semi‐natural grasslands subjected to a 20% increase in atmospheric CO2. eCO2 significantly increased AM fungal richness but had a less‐pronounced impact on the composition of their communities. However, while broader changes in community composition were not observed, more subtle responses of specific AM fungal taxa were with populations both increasing and decreasing in abundance in response to eCO2. Most population‐level responses to eCO2 were not consistent through time, with a significant interaction between sampling time and eCO2 treatment being observed. This suggests that the temporal dynamics of AM fungal populations may be disturbed by anthropogenic stressors. As AM fungi are functionally differentiated, with different taxa providing different benefits to host plants, changes in population densities in response to eCO2 may significantly impact terrestrial plant communities and their productivity. Thus, predictions regarding future terrestrial ecosystems must consider changes both aboveground and belowground, but avoid relying on broad‐scale community‐level responses of soil microbes observed on single occasions.","tags":["biodiversity","climate change","elevated CO2","long‐term experiments","microbial diversity","next‐generation sequencing"],"title":"Impacts of Long‐Term Elevated Atmospheric CO₂ Concentrations on Communities of Arbuscular Mycorrhizal Fungi","type":"publication"},{"authors":null,"categories":[],"content":" Recently, I’ve been collaborating with several colleagues on projects focussing on the microbial ecology of hydrocarbon degradation. One of the aspects we’ve been thinking about, is whether the composition of the microbial community can reflect the level of hydrocarbon exposure. As it turns out, we aren’t the first to consider this, as Mariana Lozada and colleagues came up with an “Ecological Hydrocarbon Exposure Index”, which uses the composition and structure of the microbial community to quantify the level of hydrocarbon exposure in a given environment (Lozada et al., 2014). To their credit, the authors provided R code to calculate their index in a supplementary PDF on the journal article page.\nHowever, the original code was designed to run using a Mothur formatted taxonomy file, which is not useful for users who might prefer other taxonomy assignment pipelines. Therefore, I decided to rewrite their script into a more useful function format, that can be applied to an OTU table with any taxonomy format. The function is hosted in my GitHub based package ecolFudge, and an example of how to use this function is presented below.\n# first lets import a small OTU table as an example otus \u0026lt;- read.csv(\u0026quot;example_OTU_table.csv\u0026quot;) # look at structure of OTU table # note that our genus level taxonomy assignment is in \u0026quot;Genus\u0026quot; column # and our samples all start with \u0026quot;THx\u0026quot; str(otus) ## \u0026#39;data.frame\u0026#39;: 3488 obs. of 26 variables: ## $ OTU : Factor w/ 3488 levels \u0026quot;OTU_1\u0026quot;,\u0026quot;OTU_10\u0026quot;,..: 1 1103 2205 2823 2934 3045 3156 3267 3378 2 ... ## $ THxT1x1Bac : int 3 3429 4653 57 1 190 555 58 0 1 ... ## $ THxT1x2Bac : int 1 24761 8476 144 11 1149 10748 628 3 19 ... ## $ THxT1x3Bac : int 0 1301 511 13 0 72 101 61 1 0 ... ## $ THxT21x1Bac : int 9955 5 52 332 871 127 3 33 207 52 ... ## $ THxT21x2Bac : int 1148 51 273 1956 2237 222 18 23 144 94 ... ## $ THxT21x3Bac : int 4155 17 328 2427 6246 886 34 198 1632 113 ... ## $ THxT3x1Bac : int 3 7232 972 392 4 397 496 3002 0 19 ... ## $ THxT3x2Bac : int 1 10328 939 798 22 177 962 3440 1 28 ... ## $ THxT3x3Bac : int 1 12733 3892 1559 15 517 1960 2511 0 61 ... ## $ THxT7x1Bac : int 31 1583 913 3835 2841 689 253 1392 1789 726 ... ## $ THxT7x2Bac : int 4774 1422 614 2181 3066 352 461 1351 15 2378 ... ## $ THxT7x3Bac : int 2651 2689 592 6684 1862 425 210 4090 617 1226 ... ## $ sum : int 572326 423114 308751 239756 239329 159273 158127 152364 137010 118647 ... ## $ Domain : Factor w/ 1 level \u0026quot;Bacteria\u0026quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ Confidence_domain: num 1 1 1 1 1 1 1 1 1 1 ... ## $ Phylum : Factor w/ 37 levels \u0026quot;Acetothermia\u0026quot;,..: 32 32 32 32 32 32 32 32 32 32 ... ## $ Confidence_phylum: num 1 1 1 1 1 1 1 1 1 1 ... ## $ Class : Factor w/ 76 levels \u0026quot;Acetothermia_genera_incertae_sedis\u0026quot;,..: 45 45 45 45 45 45 39 45 15 15 ... ## $ Confidence_class : num 1 1 1 1 1 1 1 1 1 1 ... ## $ Order : Factor w/ 142 levels \u0026quot;Acanthopleuribacterales\u0026quot;,..: 96 96 9 9 136 107 24 96 110 110 ... ## $ Confidence_order : num 1 1 1 0.99 1 1 1 1 1 1 ... ## $ Family : Factor w/ 283 levels \u0026quot;Acanthopleuribacteraceae\u0026quot;,..: 11 183 212 64 202 214 40 183 222 222 ... ## $ Confidence_family: num 1 1 0.98 0.94 1 1 1 1 1 1 ... ## $ Genus : Factor w/ 878 levels \u0026quot;Acanthopleuribacter\u0026quot;,..: 32 556 644 170 189 653 79 807 656 654 ... ## $ Confidence_genus : num 1 1 0.98 0.94 1 0.97 1 1 0.48 0.99 ... # we first need to create a vector of sample names that we wish to calculate # the index for samples \u0026lt;- grep(\u0026quot;THx\u0026quot;, colnames(otus), value = T) # now we can load the ecolFudge package # if you haven\u0026#39;t installed it, install it using the devtools package: # devtools::install_github(\u0026quot;dave-clark/ecolFudge\u0026quot;) library(ecolFudge) # now we simply run the index function, giving it the name of our OTU table # the vector of sample column names, and the column with the genus level # taxonomy assignments sampleExposure \u0026lt;- ehei(otus, taxonomyCol = \u0026quot;Genus\u0026quot;, sampleCols = samples) ## Calculating Ecological Hydrocarbon exposure index for 12 sample(s) # view the results head(sampleExposure) ## sample exposure ## THxT1x1Bac THxT1x1Bac 0.4419401 ## THxT1x2Bac THxT1x2Bac 0.3618828 ## THxT1x3Bac THxT1x3Bac 0.3674791 ## THxT21x1Bac THxT21x1Bac 0.7187549 ## THxT21x2Bac THxT21x2Bac 0.3846276 ## THxT21x3Bac THxT21x3Bac 0.5097866 summary(sampleExposure$exposure) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.3619 0.3952 0.4759 0.4904 0.5648 0.7188 hist(sampleExposure$exposure, xlab = \u0026quot;Ecological hydrocarbon exposure index\u0026quot;) Hopefully this will be useful to any oil microbiologists out there, particularly those who are forging their own bioinformatics pipelines. As always, feel free to contact me if you have any questions/comments.\nReferences Lozada, M., Marcos, M.S., Commendatore, M.G., Gil, M.N. \u0026amp; Dionisi, H.M. (2014) The Bacterial Community Structure of Hydrocarbon-Polluted Marine Environments as the Basis for the Definition of an Ecological Index of Hydrocarbon Exposure. Microbes and Environments, ME14028.\n   ","date":1551830400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572957317,"objectID":"25a233ab091542a3dbd796bd689d0dfa","permalink":"/post/calculating-the-ecological-hydrocarbon-exposure-index-in-r/","publishdate":"2019-03-06T00:00:00Z","relpermalink":"/post/calculating-the-ecological-hydrocarbon-exposure-index-in-r/","section":"post","summary":"Recently, I’ve been collaborating with several colleagues on projects focussing on the microbial ecology of hydrocarbon degradation. One of the aspects we’ve been thinking about, is whether the composition of the microbial community can reflect the level of hydrocarbon exposure. As it turns out, we aren’t the first to consider this, as Mariana Lozada and colleagues came up with an “Ecological Hydrocarbon Exposure Index”, which uses the composition and structure of the microbial community to quantify the level of hydrocarbon exposure in a given environment (Lozada et al.","tags":[],"title":"Calculating the Ecological Hydrocarbon Exposure Index in R","type":"post"},{"authors":null,"categories":[],"content":" The data.table package has become my favourite R package for all things data handling. Unlike the “tidyverse” suite of packages, the syntax is more akin to base data.frame syntax, meaning I was able to pick it up quite quickly. It is also incredibly quick, and the parallel data import/export functions (fread \u0026amp; fwrite) are a real gift for working with larger data tables, like OTU tables, which can contain several hundred columns, and many thousands of rows. The only thing I found data.table lacked was a function to transpose data in a convenient way.\nLet me demonstrate what I mean with some examples. Let’s load a small toy dataset that is topologically similar to an OTU table (e.g. samples as cols, species abundances as rows).\n# load data.table and vegan packages library(data.table) library(vegan) ## Loading required package: permute ## Loading required package: lattice ## This is vegan 2.5-6 # load the Barro Colorado Island tree dataset data(BCI) # coerce to a data.table # keep the rownames, as this we\u0026#39;ll use this as a \u0026#39;sample\u0026#39; column bci \u0026lt;- as.data.table(BCI, keep.rownames = T) # make more realistic sample names and delete old col bci[, \u0026quot;:=\u0026quot;(sampleName = paste0(\u0026quot;sample_\u0026quot;, rn), rn = NULL)] Now the data represent something comparable to the OTU tables I am used to working with. Species are columns, whilst each row represents a sample. However, it is common to want to work with the data in the opposite format, with samples as columns and species as rows. Intuitively, one would normally transpose the data using the t function.\ntransBci \u0026lt;- t(bci) str(transBci) ## chr [1:226, 1:50] \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot; 0\u0026quot; \u0026quot;0\u0026quot; \u0026quot; 2\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;0\u0026quot; \u0026quot;25\u0026quot; \u0026quot;0\u0026quot; ... ## - attr(*, \u0026quot;dimnames\u0026quot;)=List of 2 ## ..$ : chr [1:226] \u0026quot;Abarema.macradenia\u0026quot; \u0026quot;Vachellia.melanoceras\u0026quot; \u0026quot;Acalypha.diversifolia\u0026quot; \u0026quot;Acalypha.macrostachya\u0026quot; ... ## ..$ : NULL However, as you can see, this causes problems. Having a sample column present in our data means that all the counts get coerced to character class when transposed. Plus, we’d have to manually set the sample row as our new column names, and then delete it from the data.\nAn alternate solution involves using melt and dcast functions to transpose the data…\ntransBci2 \u0026lt;- dcast(melt(bci, id.vars = \u0026quot;sampleName\u0026quot;), variable ~ sampleName) str(transBci2) ## Classes \u0026#39;data.table\u0026#39; and \u0026#39;data.frame\u0026#39;: 225 obs. of 51 variables: ## $ variable : Factor w/ 225 levels \u0026quot;Abarema.macradenia\u0026quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ sample_1 : int 0 0 0 0 0 0 2 0 0 0 ... ## $ sample_10: int 1 0 0 0 0 1 2 0 0 0 ... ## $ sample_11: int 0 0 0 0 0 0 10 0 0 0 ... ## $ sample_12: int 0 0 0 0 1 1 3 0 0 2 ... ## $ sample_13: int 0 0 0 0 1 1 1 0 1 1 ... ## $ sample_14: int 0 0 0 0 0 0 4 0 0 0 ... ## $ sample_15: int 0 0 0 0 2 0 2 0 0 0 ... ## $ sample_16: int 0 0 0 0 2 0 2 0 0 3 ... ## $ sample_17: int 0 0 0 0 0 1 2 0 0 2 ... ## $ sample_18: int 0 0 0 0 1 1 0 0 0 0 ... ## $ sample_19: int 0 0 0 0 0 1 1 0 0 1 ... ## $ sample_2 : int 0 0 0 0 0 0 1 0 0 0 ... ## $ sample_20: int 0 0 0 0 0 2 2 0 0 0 ... ## $ sample_21: int 0 0 0 0 0 1 2 0 0 1 ... ## $ sample_22: int 0 0 0 0 1 0 4 0 0 4 ... ## $ sample_23: int 0 0 0 0 0 0 1 0 0 0 ... ## $ sample_24: int 0 0 0 0 2 1 0 0 0 1 ... ## $ sample_25: int 0 0 0 0 0 1 2 0 0 0 ... ## $ sample_26: int 0 0 0 0 0 0 3 0 0 0 ... ## $ sample_27: int 0 0 0 0 1 4 3 0 0 3 ... ## $ sample_28: int 0 2 0 1 0 1 2 0 0 0 ... ## $ sample_29: int 0 0 0 0 1 0 1 0 0 0 ... ## $ sample_3 : int 0 0 0 0 0 0 2 0 0 0 ... ## $ sample_30: int 0 0 0 0 14 2 6 0 0 0 ... ## $ sample_31: int 0 0 0 0 5 0 4 0 0 0 ... ## $ sample_32: int 0 1 0 0 7 0 6 0 0 0 ... ## $ sample_33: int 0 0 0 0 3 1 3 0 0 1 ... ## $ sample_34: int 0 0 1 0 3 0 5 0 0 0 ... ## $ sample_35: int 0 0 0 0 6 0 8 0 0 0 ... ## $ sample_36: int 0 0 0 0 1 0 3 0 0 0 ... ## $ sample_37: int 0 0 0 0 2 0 4 0 0 0 ... ## $ sample_38: int 0 0 0 0 6 0 2 0 0 1 ... ## $ sample_39: int 0 0 0 0 9 0 3 0 0 1 ... ## $ sample_4 : int 0 0 0 0 3 0 18 0 0 0 ... ## $ sample_40: int 0 0 1 0 7 0 3 0 0 1 ... ## $ sample_41: int 0 0 0 0 0 1 11 0 0 0 ... ## $ sample_42: int 0 0 0 0 0 0 0 0 0 0 ... ## $ sample_43: int 0 0 0 0 0 1 3 0 0 0 ... ## $ sample_44: int 0 0 0 0 4 0 4 0 0 0 ... ## $ sample_45: int 0 0 0 0 0 0 0 0 0 0 ... ## $ sample_46: int 0 0 0 0 0 0 0 0 0 0 ... ## $ sample_47: int 0 0 0 0 2 0 1 0 0 1 ... ## $ sample_48: int 0 0 0 0 1 0 3 0 0 1 ... ## $ sample_49: int 0 0 0 0 0 0 6 0 0 1 ... ## $ sample_5 : int 0 0 0 0 1 1 3 0 0 1 ... ## $ sample_50: int 0 0 0 0 1 0 2 0 0 1 ... ## $ sample_6 : int 0 0 0 0 0 0 2 1 0 0 ... ## $ sample_7 : int 0 0 0 0 0 1 0 0 0 0 ... ## $ sample_8 : int 0 0 0 0 0 0 2 0 0 0 ... ## $ sample_9 : int 0 0 0 0 5 0 2 0 0 0 ... ## - attr(*, \u0026quot;.internal.selfref\u0026quot;)=\u0026lt;externalptr\u0026gt; ## - attr(*, \u0026quot;sorted\u0026quot;)= chr \u0026quot;variable\u0026quot; That’s better: the sample names are now in the correct place, we have a column with the species names, and the data have remained in the correct integer class. However, whilst that code may have run quite quickly, much larger datasets can slow it down. I therefore wrote a little function based on the original transpose function to get the same result as above, but quicker!\nThe function is called transDT and can be found in my GitHub hosted package, ecolFudge.\n# first install my package from github library(devtools) install_github(\u0026quot;dave-clark/ecolFudge\u0026quot;) # load ecolFudge package library(ecolFudge) # view the transDT function transDT ## function (dt, transCol, rowID) ## { ## newRowNames \u0026lt;- colnames(dt) ## newColNames \u0026lt;- dt[, transCol, with = F] ## transposedDt \u0026lt;- transpose(dt[, !colnames(dt) %in% transCol, ## with = F]) ## colnames(transposedDt) \u0026lt;- unlist(newColNames) ## transposedDt[, rowID] \u0026lt;- newRowNames[newRowNames != transCol] ## return(transposedDt) ## } ## \u0026lt;bytecode: 0x563c9a12bc50\u0026gt; ## \u0026lt;environment: namespace:ecolFudge\u0026gt; As you can see, the function takes three arguments. The first, dt, is simply the data.table you wish to transpose. The second, transCol, is the column that you wish to become your new column names. In our example, this would be the sampleName column. The third argument, rowID, is simply the name you would like to call the column with your new row identifiers (e.g. the column names in your original data). In this example, our new row identifiers are the names of the species, and so it makes sense to call this column species or something similar.\ntransBci3 \u0026lt;- transDT(bci, transCol=\u0026quot;sampleName\u0026quot;, rowID = \u0026quot;species\u0026quot;) # note that the species column has been placed as the last column... str(transBci3) ## Classes \u0026#39;data.table\u0026#39; and \u0026#39;data.frame\u0026#39;: 225 obs. of 51 variables: ## $ sample_1 : int 0 0 0 0 0 0 2 0 0 0 ... ## $ sample_2 : int 0 0 0 0 0 0 1 0 0 0 ... ## $ sample_3 : int 0 0 0 0 0 0 2 0 0 0 ... ## $ sample_4 : int 0 0 0 0 3 0 18 0 0 0 ... ## $ sample_5 : int 0 0 0 0 1 1 3 0 0 1 ... ## $ sample_6 : int 0 0 0 0 0 0 2 1 0 0 ... ## $ sample_7 : int 0 0 0 0 0 1 0 0 0 0 ... ## $ sample_8 : int 0 0 0 0 0 0 2 0 0 0 ... ## $ sample_9 : int 0 0 0 0 5 0 2 0 0 0 ... ## $ sample_10: int 1 0 0 0 0 1 2 0 0 0 ... ## $ sample_11: int 0 0 0 0 0 0 10 0 0 0 ... ## $ sample_12: int 0 0 0 0 1 1 3 0 0 2 ... ## $ sample_13: int 0 0 0 0 1 1 1 0 1 1 ... ## $ sample_14: int 0 0 0 0 0 0 4 0 0 0 ... ## $ sample_15: int 0 0 0 0 2 0 2 0 0 0 ... ## $ sample_16: int 0 0 0 0 2 0 2 0 0 3 ... ## $ sample_17: int 0 0 0 0 0 1 2 0 0 2 ... ## $ sample_18: int 0 0 0 0 1 1 0 0 0 0 ... ## $ sample_19: int 0 0 0 0 0 1 1 0 0 1 ... ## $ sample_20: int 0 0 0 0 0 2 2 0 0 0 ... ## $ sample_21: int 0 0 0 0 0 1 2 0 0 1 ... ## $ sample_22: int 0 0 0 0 1 0 4 0 0 4 ... ## $ sample_23: int 0 0 0 0 0 0 1 0 0 0 ... ## $ sample_24: int 0 0 0 0 2 1 0 0 0 1 ... ## $ sample_25: int 0 0 0 0 0 1 2 0 0 0 ... ## $ sample_26: int 0 0 0 0 0 0 3 0 0 0 ... ## $ sample_27: int 0 0 0 0 1 4 3 0 0 3 ... ## $ sample_28: int 0 2 0 1 0 1 2 0 0 0 ... ## $ sample_29: int 0 0 0 0 1 0 1 0 0 0 ... ## $ sample_30: int 0 0 0 0 14 2 6 0 0 0 ... ## $ sample_31: int 0 0 0 0 5 0 4 0 0 0 ... ## $ sample_32: int 0 1 0 0 7 0 6 0 0 0 ... ## $ sample_33: int 0 0 0 0 3 1 3 0 0 1 ... ## $ sample_34: int 0 0 1 0 3 0 5 0 0 0 ... ## $ sample_35: int 0 0 0 0 6 0 8 0 0 0 ... ## $ sample_36: int 0 0 0 0 1 0 3 0 0 0 ... ## $ sample_37: int 0 0 0 0 2 0 4 0 0 0 ... ## $ sample_38: int 0 0 0 0 6 0 2 0 0 1 ... ## $ sample_39: int 0 0 0 0 9 0 3 0 0 1 ... ## $ sample_40: int 0 0 1 0 7 0 3 0 0 1 ... ## $ sample_41: int 0 0 0 0 0 1 11 0 0 0 ... ## $ sample_42: int 0 0 0 0 0 0 0 0 0 0 ... ## $ sample_43: int 0 0 0 0 0 1 3 0 0 0 ... ## $ sample_44: int 0 0 0 0 4 0 4 0 0 0 ... ## $ sample_45: int 0 0 0 0 0 0 0 0 0 0 ... ## $ sample_46: int 0 0 0 0 0 0 0 0 0 0 ... ## $ sample_47: int 0 0 0 0 2 0 1 0 0 1 ... ## $ sample_48: int 0 0 0 0 1 0 3 0 0 1 ... ## $ sample_49: int 0 0 0 0 0 0 6 0 0 1 ... ## $ sample_50: int 0 0 0 0 1 0 2 0 0 1 ... ## $ species : chr \u0026quot;Abarema.macradenia\u0026quot; \u0026quot;Vachellia.melanoceras\u0026quot; \u0026quot;Acalypha.diversifolia\u0026quot; \u0026quot;Acalypha.macrostachya\u0026quot; ... ## - attr(*, \u0026quot;.internal.selfref\u0026quot;)=\u0026lt;externalptr\u0026gt; Now let’s see whether the transDT function can be faster than the dcast/melt method…\nlibrary(microbenchmark) speedTest \u0026lt;- microbenchmark( transDT(bci, transCol = \u0026quot;sampleName\u0026quot;, rowID = \u0026quot;species\u0026quot;), dcast(melt(bci, id.vars = \u0026quot;sampleName\u0026quot;), variable ~ sampleName), times = 50) # rename factor levels to neaten up results plot speedTest$expr \u0026lt;- factor(speedTest$expr, levels = levels(speedTest$expr), labels = c(\u0026quot;transDT\u0026quot;, \u0026quot;dcast/melt\u0026quot;)) boxplot(time/1000 ~ expr, data = speedTest, ylab = expression(paste(\u0026quot;Time (\u0026quot;, mu, \u0026quot;S)\u0026quot;)), xlab = \u0026quot;Method\u0026quot;) There you have it, transDT gives us the same result, but in a fraction of the time compared to the dcast/melt method, even on a relatively small dataset. I hope this is useful to other R users other than myself, if you have any questions, do get in touch!\n","date":1551312000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572962027,"objectID":"18203f422621578af0be3360a32a656a","permalink":"/post/a-custom-r-function-for-transposing-data-tables/","publishdate":"2019-02-28T00:00:00Z","relpermalink":"/post/a-custom-r-function-for-transposing-data-tables/","section":"post","summary":"The data.table package has become my favourite R package for all things data handling. Unlike the “tidyverse” suite of packages, the syntax is more akin to base data.frame syntax, meaning I was able to pick it up quite quickly. It is also incredibly quick, and the parallel data import/export functions (fread \u0026amp; fwrite) are a real gift for working with larger data tables, like OTU tables, which can contain several hundred columns, and many thousands of rows.","tags":[],"title":"A custom R function for transposing data.tables","type":"post"},{"authors":["A Khuzaim Alzarhani","**Dave R Clark**","Graham JC Underwood","Hilary Ford","TE Anne Cotton","Alex J Dumbrell"],"categories":null,"content":"","date":1548633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549892104,"objectID":"5d1409931b8bc512f52d02986d5ab594","permalink":"/publication/alzahrani_et_al/","publishdate":"2019-01-28T00:00:00Z","relpermalink":"/publication/alzahrani_et_al/","section":"publication","summary":"The composition and structure of plant-root-associated fungal communities are determined by local abiotic and biotic conditions. However, the relative influence and identity of relationships to abiotic and biotic factors may differ across environmental and ecological contexts, and fungal functional groups. Thus, understanding which aspects of root-associated fungal community ecology generalise across contexts is the first step towards a more predictive framework. We investigated how the relative importance of biotic and abiotic factors scale across environmental and ecological contexts using high-throughput sequencing (ca. 55 M Illumina metabarcoding sequences) of 260 plant-root-associated fungal communities from six UK salt marshes across two geographic regions (South-East and North-West England) in winter and summer. Levels of root-associated fungal diversity were comparable with forests and temperate grasslands, quadrupling previous estimates of salt-marsh fungal diversity. Whilst abiotic variables were generally most important, a range of site- and spatial scale-specific abiotic and biotic drivers of diversity and community composition were observed. Consequently, predictive models of diversity trained on one site, extrapolated poorly to others. Fungal taxa from the same functional groups responded similarly to the specific drivers of diversity and composition. Thus site, spatial scale and functional group are key factors that, if accounted for, may lead to a more predictive understanding of fungal community ecology.","tags":[],"title":"Are drivers of root-associated fungal community structure context specific?","type":"publication"},{"authors":null,"categories":[],"content":" After my post yesterday, documenting a faster parallelised version of the rarecurve function (quickRareCurve), I realised it’d be good to show a real world example using it on a reasonably large OTU table, to prove that it is indeed quicker than the original function. So, here we go.\n# Starting with an OTU table in which rows are samples, # Cols are OTUs/species dim(otuTable) ## [1] 48 5382 # the first column of my data is the sample names # so remember [, -1] to not include it # Lets inspect sample sizes with a simple histogram hist(rowSums(otuTable[, -1]), xlab = \u0026quot;Sample sizes\u0026quot;) In this example, our OTU table contains 48 samples and 5381 OTUs, plus a column with the sample names in.\nNow we can use microbenchmark again to compare the performance of the original rarecurve function to our faster parallel version, quickRareCurve. We will then plot the results using ggplot2. Warning: the code below will take some time to run!\nlibrary(microbenchmark) # benchmark the two functions with 3 replicates testResult \u0026lt;- microbenchmark( rarecurve(otuTable[, -1]), quickRareCurve(otuTable[, -1]), times = 3) library(ggplot2) # convert from nanoseconds to minutes by dividing # time by 6e+10 ggplot(testResult, aes(x = expr, y = time/6e+10)) + geom_boxplot() + labs(x = \u0026quot;Function\u0026quot;, y = \u0026quot;Time (minutes)\u0026quot;) + theme(axis.text = element_text(size = 16), axis.title = element_text(size = 18)) There you go, on a typical OTU table, the quickRareCurve function is far quicker, reducing the processing time from ~25 minutes to \u0026lt; 5 minutes. The more samples in your OTU table, and the more CPU cores available, the greater the increase in performance.\n","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551779906,"objectID":"21f2136b8bc01c1c261c49638aa33211","permalink":"/post/further-testing-of-quickrarecurve/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/post/further-testing-of-quickrarecurve/","section":"post","summary":"After my post yesterday, documenting a faster parallelised version of the rarecurve function (quickRareCurve), I realised it’d be good to show a real world example using it on a reasonably large OTU table, to prove that it is indeed quicker than the original function. So, here we go.\n# Starting with an OTU table in which rows are samples, # Cols are OTUs/species dim(otuTable) ## [1] 48 5382 # the first column of my data is the sample names # so remember [, -1] to not include it # Lets inspect sample sizes with a simple histogram hist(rowSums(otuTable[, -1]), xlab = \u0026quot;Sample sizes\u0026quot;) In this example, our OTU table contains 48 samples and 5381 OTUs, plus a column with the sample names in.","tags":[],"title":"Further testing of quickRareCurve","type":"post"},{"authors":null,"categories":[],"content":" Recently, the newest version of the popular ggplot2 graphics package was announced, and it has some nifty mapping features that I was keen to try out (read more here). Mainly, I was interested in the support for sf, or “simple features”, objects. This class of objects were created as part of a wider R package designed to make mapping and spatial analyses far easier.\nThe latest update of ggplot2 not only makes plotting from sf objects trivial, but also means that some quite nice map figures can be made with relatively little effort, as you’ll hopefully see below.\n# first you\u0026#39;ll need to update your version of ggplot to the latest version # install.packages(\u0026quot;ggplot2\u0026quot;) library(ggplot2) # now lets load some other packages that we\u0026#39;ll need to load and manipulate # spatial data in R library(mapdata) library(sf) library(lwgeom) Assuming you managed to get those packages installed and loaded ok (if you didn’t, you almost certainly have encountered some dependency issues), we can now start playing with some data. To begin, let’s load a world map to play with.\nmapBase \u0026lt;- map(\u0026quot;worldHires\u0026quot;, fill = T, plot = F) # now we need to coerce it to an \u0026quot;sf\u0026quot; object, and fix any mapBase \u0026lt;- st_as_sf(mapBase) # now let\u0026#39;s try cropping it to a region of Europe cropMap \u0026lt;- st_crop(mapBase, xmin = -15, xmax = 30, ymin = 30, ymax = 60) # note we get an error message about Self-intersection... # we can fix this using the lwgeom library... mapBase \u0026lt;- st_make_valid(mapBase) cropMap \u0026lt;- st_crop(mapBase, xmin = -15, xmax = 30, ymin = 30, ymax = 60) ## Warning in st_is_longlat(x): bounding box has potentially an invalid value ## range for longlat data ## although coordinates are longitude/latitude, st_intersection assumes that they are planar ## Warning: attribute variables are assumed to be spatially constant ## throughout all geometries # now it works fine... Now that we have a valid sf object to plot, we can start using the geom_sf function to start making some nice maps.\n# basic map to start with ggplot(cropMap) + geom_sf() Cool right? Notice a few neat things? Firstly, ggplot draws a nice graticule for you, complete with the correct “degrees” symbol and N, S, E, or W to denote the correct hemisphere. Secondly, go ahead and resize the map… What do you notice? Hopefully, you’ll see that the aspect ratio of the map has been fixed so that your map always projects correctly. This is a great feature and saves you some time faffing around trying to manually correct the aspect ratio.\nHaving explored some basic features, we can now start to tailor our map and add features to it using other packages, or data.\n# let\u0026#39;s say we want to highlight the location of Spain, clarify the axes, # and remove those annoying grid lines spainMap \u0026lt;- ggplot(cropMap, aes(fill = factor(ifelse(cropMap$ID == \u0026quot;Spain\u0026quot;, 1, 2)))) + geom_sf() + labs(x = \u0026quot;Longitude\u0026quot;, y = \u0026quot;Latitude\u0026quot;, fill = \u0026quot;\u0026quot;) + scale_fill_manual(values = c(\u0026quot;darkgrey\u0026quot;, \u0026quot;lightgrey\u0026quot;), labels = c(\u0026quot;Spain\u0026quot;, \u0026quot;Not Spain\u0026quot;)) + theme_bw() + theme(panel.grid = element_line(colour = \u0026quot;transparent\u0026quot;), axis.title = element_text(size = 18), axis.text = element_text(size = 16), legend.text = element_text(size = 14), legend.title = element_text(size = 14)) spainMap Pretty easy so far right? But what if we want to add some points to the map, for example to highlight sampling locations? Well, this is really easy too!\n# simulate some random coordinates within the limits of our map locations \u0026lt;- data.frame(lat = runif(25, 30, 60), long = runif(25, -15, 30)) # now convert this dataframe into an sf dataframe sfPoints \u0026lt;- st_as_sf(locations, coords = c(\u0026quot;long\u0026quot;, \u0026quot;lat\u0026quot;), crs = 4326) # and simply add another geom_sf layer to the plot to include the points pointsMap \u0026lt;- ggplot() + geom_sf(data = cropMap, aes(fill = factor(ifelse(cropMap$ID == \u0026quot;Spain\u0026quot;, 1, 2)))) + geom_sf(data = sfPoints, col = \u0026quot;red\u0026quot;, size = 3) + labs(x = \u0026quot;Longitude\u0026quot;, y = \u0026quot;Latitude\u0026quot;, fill = \u0026quot;\u0026quot;) + scale_fill_manual(values = c(\u0026quot;darkgrey\u0026quot;, \u0026quot;lightgrey\u0026quot;), labels = c(\u0026quot;Spain\u0026quot;, \u0026quot;Not Spain\u0026quot;)) + theme_bw() + theme(panel.grid = element_line(colour = \u0026quot;transparent\u0026quot;), axis.title = element_text(size = 18), axis.text = element_text(size = 16), legend.text = element_text(size = 14), legend.title = element_text(size = 14)) pointsMap Again, super simple. Now, I’d like to create a smaller map, that just shows Spain and any points inside it, so I can make a nice panel figure with the two maps side by side. Also, don’t forget a scale bar, which we can add using the ggspatial package.\nlibrary(ggspatial) # work out which points are in the polygon of interest spainPoints \u0026lt;- st_join(sfPoints, cropMap[cropMap$ID == \u0026quot;Spain\u0026quot;, ], join = st_intersects) ## although coordinates are longitude/latitude, st_intersects assumes that they are planar ## although coordinates are longitude/latitude, st_intersects assumes that they are planar spainZoom \u0026lt;- ggplot() + geom_sf(data = cropMap[cropMap$ID == \u0026quot;Spain\u0026quot;, ]) + annotation_scale(location = \u0026quot;br\u0026quot;, text_cex = 2) + geom_sf(data = spainPoints[spainPoints$ID == \u0026quot;Spain\u0026quot;, ], colour = \u0026quot;red\u0026quot;, size = 3) + theme_void() + theme(panel.grid = element_line(colour = \u0026quot;transparent\u0026quot;)) spainZoom ## Scale on map varies by more than 10%, scale bar may be inaccurate So, now we have our two maps sorted, we can arrange them side by side using the cowplot package.\nlibrary(cowplot) ## ## ******************************************************** ## Note: As of version 1.0.0, cowplot does not change the ## default ggplot2 theme anymore. To recover the previous ## behavior, execute: ## theme_set(theme_cowplot()) ## ******************************************************** plot_grid(spainZoom, pointsMap, labels = \u0026quot;AUTO\u0026quot;, rel_widths = c(0.6, 1)) ## Scale on map varies by more than 10%, scale bar may be inaccurate There you have it, combining the sf and newest ggplot2 packages allows you quickly and easily make some neat looking map figures!\n","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572962027,"objectID":"b1efda0aa675807ed4f85b1b3e7d2e9a","permalink":"/post/making-maps-with-ggplot2-and-sf/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/post/making-maps-with-ggplot2-and-sf/","section":"post","summary":"Recently, the newest version of the popular ggplot2 graphics package was announced, and it has some nifty mapping features that I was keen to try out (read more here). Mainly, I was interested in the support for sf, or “simple features”, objects. This class of objects were created as part of a wider R package designed to make mapping and spatial analyses far easier.\nThe latest update of ggplot2 not only makes plotting from sf objects trivial, but also means that some quite nice map figures can be made with relatively little effort, as you’ll hopefully see below.","tags":[],"title":"Making maps with ggplot2 and sf","type":"post"},{"authors":null,"categories":[],"content":" Let me start off by stating that I have enormous respect for Rob Edgar (creator of USEARCH, UPARSE etc.). His contributions to the field of bioinformatics, and indirectly to the fields of molecular and microbial ecology have been huge, you only need to look at his citation rates to see that! So this post is not intended as a criticism of him or his work in any way.\nThat said, I’ve recently been thinking about the deluge of new algorithms for picking Operational Taxonomic Units (OTUs) from molecular sequence datasets and wondering where, and how, USEARCH, UCLUST et al. will fit in.\nRob Edgar only made 32-bit versions of all his software freely available for users, with users having to pay $885 for a 64-bit license. A 32-bit license essentially limits the amount of RAM your computer can access whilst using this software to 4gb. So even if you had a cluster computer at your disposal, you’d still be shackled by the 4gb limit. A few years ago, when 454 pyrosequencing was the predominant sequencing platform, this wasn’t so much of a problem because the datasets were big, but not huge. However, with the rise of Illumina’s platforms such as the HiSeq and MiSeq, molecular datasets are becoming truly enormous, meaning more RAM is needed to work with them.\nFurthermore, the arrival of VSEARCH, a 64-bit open source work-alike of USEARCH, has meant that you now don’t need to shell out for an expensive 64-bit license to get USEARCH-esque results. This leads me to wonder, what is the future of USEARCH now that there are good, free, and open-source alternatives?\nLooking at the citation rates of Edgar’s papers, there are no obvious signs of it losing popularity… Data on the plot below were downloaded using Web of Science. It will be interesting to see if the popularity of this classic piece of bioinformatics software ever declines.\n","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546387210,"objectID":"4ca4ae3208eec7a09638bd6708c62fd0","permalink":"/post/the-future-of-usearch-a-closed-source-software-in-an-open-source-world/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/post/the-future-of-usearch-a-closed-source-software-in-an-open-source-world/","section":"post","summary":"Let me start off by stating that I have enormous respect for Rob Edgar (creator of USEARCH, UPARSE etc.). His contributions to the field of bioinformatics, and indirectly to the fields of molecular and microbial ecology have been huge, you only need to look at his citation rates to see that! So this post is not intended as a criticism of him or his work in any way.\nThat said, I’ve recently been thinking about the deluge of new algorithms for picking Operational Taxonomic Units (OTUs) from molecular sequence datasets and wondering where, and how, USEARCH, UCLUST et al.","tags":[],"title":"The future of USEARCH: A closed source software in an open source world","type":"post"},{"authors":null,"categories":[],"content":" When beginning analyses on microbial community data, it is often helpful to compute rarefaction curves. A rarefaction curve tells you about the rate at which new species/OTUs are detected as you increase the number of individuals/sequences sampled. It does this by taking random subsamples from 1, up to the size of your sample, and computing the number of species present in each subsample. Ideally, you want your rarefaction curves to be relatively flat, as this indicates that additional sampling would not likely yield further species.\nThe vegan package in R has a nice function for computing rarefaction curves for species by site abundance tables. However, for microbial datasets this function is often prohibitively slow. This is due to the fact that we often have large numbers of samples, and each sample may contain several thousand sequences, meaning that a large number of random subsamples are taken.\nPart of the problem is that the original function only makes use of a single processing core, meaning that random subsamples are computed serially, and each sample is processed serially. This means we could speed the function up by splitting samples across multiple cores. In short, we are parallelising the function.\nBelow is my attempt to modify the original code into a function that can use multiple processor cores to speed up the calculations.\n# you will need to install the parallel package before hand # and the vegan package if you dont have it library(vegan) quickRareCurve \u0026lt;- function (x, step = 1, sample, xlab = \u0026quot;Sample Size\u0026quot;, ylab = \u0026quot;Species\u0026quot;, label = TRUE, col, lty, max.cores = T, nCores = 1, ...) { require(parallel) x \u0026lt;- as.matrix(x) if (!identical(all.equal(x, round(x)), TRUE)) stop(\u0026quot;function accepts only integers (counts)\u0026quot;) if (missing(col)) col \u0026lt;- par(\u0026quot;col\u0026quot;) if (missing(lty)) lty \u0026lt;- par(\u0026quot;lty\u0026quot;) tot \u0026lt;- rowSums(x) # calculates library sizes S \u0026lt;- specnumber(x) # calculates n species for each sample if (any(S \u0026lt;= 0)) { message(\u0026quot;empty rows removed\u0026quot;) x \u0026lt;- x[S \u0026gt; 0, , drop = FALSE] tot \u0026lt;- tot[S \u0026gt; 0] S \u0026lt;- S[S \u0026gt; 0] } # removes any empty rows nr \u0026lt;- nrow(x) # number of samples col \u0026lt;- rep(col, length.out = nr) lty \u0026lt;- rep(lty, length.out = nr) # parallel mclapply # set number of cores mc \u0026lt;- getOption(\u0026quot;mc.cores\u0026quot;, ifelse(max.cores, detectCores(), nCores)) message(paste(\u0026quot;Using \u0026quot;, mc, \u0026quot; cores\u0026quot;)) out \u0026lt;- mclapply(seq_len(nr), mc.cores = mc, function(i) { n \u0026lt;- seq(1, tot[i], by = step) if (n[length(n)] != tot[i]) n \u0026lt;- c(n, tot[i]) drop(rarefy(x[i, ], n)) }) Nmax \u0026lt;- sapply(out, function(x) max(attr(x, \u0026quot;Subsample\u0026quot;))) Smax \u0026lt;- sapply(out, max) plot(c(1, max(Nmax)), c(1, max(Smax)), xlab = xlab, ylab = ylab, type = \u0026quot;n\u0026quot;, ...) if (!missing(sample)) { abline(v = sample) rare \u0026lt;- sapply(out, function(z) approx(x = attr(z, \u0026quot;Subsample\u0026quot;), y = z, xout = sample, rule = 1)$y) abline(h = rare, lwd = 0.5) } for (ln in seq_along(out)) { N \u0026lt;- attr(out[[ln]], \u0026quot;Subsample\u0026quot;) lines(N, out[[ln]], col = col[ln], lty = lty[ln], ...) } if (label) { ordilabel(cbind(tot, S), labels = rownames(x), ...) } invisible(out) } Most of the code is verbatim to the original, but you’ll notice a few extra arguments to specify, and a few extra lines that determine how many cores the function will use.\nEssentially, if you do not specify a number of cores, the function will default to using all available cores, which will allow the quickest calculation. Otherwise, you can specify max.cores = F, which will allow you to specify the number of cores using nCores. This is handy if you need some cores to remain usable whilst running the function.\nBelow is a usage example.\n# load some dummy data data(dune) quickRareCurve(dune) # will use all cores and print how many cores you have quickRareCurve(dune, max.cores = F, nCores = 2) # use only 2 cores We can compare the new function’s performance to the original using the microbenchmark package, as below.\nlibrary(microbenchmark) microbenchmark(rarecurve(dune), quickRareCurve(dune), times = 10) ## Unit: milliseconds ## expr min lq mean median uq ## rarecurve(dune) 66.17685 68.48628 69.23183 69.31825 70.03621 ## quickRareCurve(dune) 47.80692 82.52621 80.46054 84.06839 85.01588 ## max neval ## 71.67020 10 ## 86.31908 10 As you can see, for this small dataset, the original function is actually quicker. This is because it takes some time to distribute tasks among the cores. However, I’ve found for a typical OTU table (\u0026gt; 50 samples, ~ 15,000 sequences per sample), quickRareCurve can be around 3 times faster.\nFeel free to use it as you wish, but I’d be very grateful for any credit if you do use it.\n","date":1544832000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572962027,"objectID":"f3ab09c01c27caa25c786a5b982955b1","permalink":"/post/speeding-up-rarefaction-curves-for-microbial-community-ecology/","publishdate":"2018-12-15T00:00:00Z","relpermalink":"/post/speeding-up-rarefaction-curves-for-microbial-community-ecology/","section":"post","summary":"When beginning analyses on microbial community data, it is often helpful to compute rarefaction curves. A rarefaction curve tells you about the rate at which new species/OTUs are detected as you increase the number of individuals/sequences sampled. It does this by taking random subsamples from 1, up to the size of your sample, and computing the number of species present in each subsample. Ideally, you want your rarefaction curves to be relatively flat, as this indicates that additional sampling would not likely yield further species.","tags":[],"title":"Speeding up rarefaction curves for microbial community ecology","type":"post"},{"authors":null,"categories":[],"content":" If you use R to analyse and plot your data, then you’ve probably heard of and used the ggplot2 package, written by Hadley Wickham. ggplot2 is a highly flexible plotting package allowing you to create just about any kind of plot you can think of, and customise just about any aspect of your plot.\nHowever, ggplot2 is also known for it’s somewhat strange choice of default options (at least, they seem strange to me!). Therefore, it can seem like a lot of work to go from a basic plot to something that is approaching publication quality. Whilst there are many excellent guides out there to help with this process, I thought I’d weigh in with a few tips and personal preferences which have helped me to elevate my plots to another level.\nLet’s begin by loading the package and simulating some data to play around with.\nlibrary(ggplot2) myData \u0026lt;- data.frame(var1 = runif(1000, 0, 100), var2 = runif(1000, -10, 10), catVar = rep(c(1:4), times = 250)) head(myData) ## var1 var2 catVar ## 1 85.24026 -2.1282043 1 ## 2 37.48931 -3.9379807 2 ## 3 49.32137 0.8810468 3 ## 4 35.36416 -1.3170033 4 ## 5 88.40480 -6.7371313 1 ## 6 97.84301 -4.8468379 2 Hopefully that code should be fairly easy to understand, we’ve simulated two continuous variables (with a random distribution) and created a categorical variable.\nNow let’s create the most basic scatter plot possible with this data.\nplot1 \u0026lt;- ggplot(myData, aes(x = var1, y = var2)) + geom_point() plot1 That looks ok, but it’s not perfect. The text and points are quite small, the black points are bit abrasive on the eye and the grey background is a little distracting. Additionally, the axis labels are not informative and we are also obscuring potentially interesting trends by plotting all of the data from our different groups together. So, lets set about fixing those things.\nggplot(myData, aes(x = var1, y = var2)) + geom_point(size = 2, color = \u0026quot;grey\u0026quot;) + facet_wrap(~catVar) So now the points are a little bigger and grey, which makes them easier to look at and with an aesthetically nicer muted contrast. We’ve also split the data into 4 separate panels, so that we can seen any group specific trends far easier, and without the visual clutter of lots of different colour points (ideal when there is a monetary cost to publishing color figures). But that grey background is now obscuring things, and our axis labels are still hard to read and uninformative. Let’s fix those.\nggplot(myData, aes(x = var1, y = var2)) + geom_point(size = 2, color = \u0026quot;grey\u0026quot;) + facet_wrap(~catVar) + labs(x = \u0026quot;Variable 1\u0026quot;, y = \u0026quot;Variable 2\u0026quot;) + theme_bw() We’re starting to look quite good now, the points are now much more visible, yet not too harsh against the white background, and our axis labels now tell us something about the data (if it were real!). Now we can apply some final polish to really get this looking great!\nfinalPlot \u0026lt;- ggplot(myData, aes(x = var1, y = var2)) + geom_point(size = 2, color = \u0026quot;grey\u0026quot;) + facet_wrap(~catVar) + labs(x = \u0026quot;Variable 1\u0026quot;, y = \u0026quot;Variable 2\u0026quot;) + theme_bw() + theme(axis.text = element_text(size = 16), axis.title = element_text(size = 18), strip.text.x = element_text(size = 16), panel.grid.major = element_blank()) Done! The text around the plot is now much easier to see, and we’ve removed the major gridlines which means less distraction from the data. Everything is far clearer and easier to read.\nplot1 finalPlot One final tip, save your graphs as pdfs. PDFs are known as vector based graphics and maintain resolution far better than bitmap based graphics such as PNGs, JPEGs or TIFs. The added bonus is that many journals also request your figures to be in a vector based format, so you’re also helping your figures become published.\nggsave(\u0026quot;myPlot.pdf\u0026quot;, finalPlot, width = 8, height = 8, device = \u0026quot;pdf\u0026quot;) ","date":1544832000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572962027,"objectID":"667fc2ab4d5ffae45551a55f3f5fe1b8","permalink":"/post/ggplot2-from-default-to-delightful/","publishdate":"2018-12-15T00:00:00Z","relpermalink":"/post/ggplot2-from-default-to-delightful/","section":"post","summary":"If you use R to analyse and plot your data, then you’ve probably heard of and used the ggplot2 package, written by Hadley Wickham. ggplot2 is a highly flexible plotting package allowing you to create just about any kind of plot you can think of, and customise just about any aspect of your plot.\nHowever, ggplot2 is also known for it’s somewhat strange choice of default options (at least, they seem strange to me!","tags":[],"title":"ggplot2; From default to delightful","type":"post"},{"authors":null,"categories":[],"content":" I was recently asked by one of my PhD supervisors to help out on a paper by doing some metagenomic analyses. My mission was essentially to perform some taxonomic analyses of metagenomes and show how a metagenome generated in our lab related to these. So, naturally, I said yes, carried out the necessary analyses and proceeded to design a figure to show the result. I figured a dendrogram would be a nice way of showing compositional similarity between the community we studied and other communities. First of all, I set about creating a distance matrix and plotting the dendrogram like so:\nlibrary(vegan) ## Loading required package: permute ## Loading required package: lattice ## This is vegan 2.5-6 data(dune) # load some dummy data commDist \u0026lt;- vegdist(dune, \u0026quot;jaccard\u0026quot;) # calculate the community similarity distance matrix plot(hclust(commDist)) There you go. It’s easy to generate a basic dendrogram using any sort of distance matrix. So I then started tidying it up to make it a bit more presentable.\nsiteLabels \u0026lt;- paste(\u0026quot;Sample \u0026quot;, rownames(dune), sep = \u0026quot;\u0026quot;) # create some sample labels plot(hclust(commDist), labels = siteLabels, main = \u0026quot;\u0026quot;, sub = \u0026quot;\u0026quot;, xlab = \u0026quot;\u0026quot;, ylab = \u0026quot;Jaccard Similarity\u0026quot;, lwd = 2, cex = 1.2) Much nicer! We’ve gotten rid of the nonsense at the bottom, given a more informative y axis label and generally made things ‘nicer’. Except, when I saw the first draft, I saw my supervisor had rotated the plot so that the leaf tips pointed to the left. For some reason this just looked wrong to my eyes and so I opted to replot the dendrogram, in a more sensible orientation.\nNote that to do this, we have to change our approach slightly. For some stupid reason, when we want to plot a dendrogram horizontally, we have to coerce the hclust output to dendrogram class. Additionally, the ‘plot’ method for a dendrogram class object no longer accepts the labels parameter, so we must rename the rows of our dataframe and recalculate the distance matrix instead.\nrownames(dune) \u0026lt;- siteLabels # rename rows of dataframe with our sample labels created earlier commDist \u0026lt;- vegdist(dune, \u0026quot;jaccard\u0026quot;) # recalculate the distance matrix so that it features our new sample names par(mar = c(5, 1, 1, 5)) # adjust margins to make room for tip labels # replot the dendrogram, note that we can now remove the \u0026quot;main =\u0026quot; and \u0026quot;sub =\u0026quot; arguments # also remember to switch the x and y labels! plot(as.dendrogram(hclust(commDist)), xlab = \u0026quot;Jaccard Similarity\u0026quot;, lwd = 2, cex = 1.2, horiz = T) Perfect! Or at least I thought so. Turns out my supervisor liked the improved orientation but didn’t like the way the branches all ended at the same point. That should be easy to correct right? Wrong!\nI spent near enough an entire day trying to get this plot perfect without success. I went as far as exploring the ggdendro package which allows plotting of dendrograms in a ggplot-esque manner.\nThen I stumbled on a solution, enter dendextend! A quick peek of the package manual reveals some really awesome capabilities, I really urge you to take a look as some of the figures you can create are amazing. For my humble needs, this package solved all my problems easily, and in a couple of lines I’d created exactly the figure my supervisor wanted.\nifelse(\u0026quot;dendextend\u0026quot; %in% rownames(installed.packages()) == T, library(dendextend), install.packages(dendextend)) ## Registered S3 method overwritten by \u0026#39;dendextend\u0026#39;: ## method from ## rev.hclust vegan ## ## --------------------- ## Welcome to dendextend version 1.12.0 ## Type citation(\u0026#39;dendextend\u0026#39;) for how to cite the package. ## ## Type browseVignettes(package = \u0026#39;dendextend\u0026#39;) for the package vignette. ## The github page is: https://github.com/talgalili/dendextend/ ## ## Suggestions and bug-reports can be submitted at: https://github.com/talgalili/dendextend/issues ## Or contact: \u0026lt;tal.galili@gmail.com\u0026gt; ## ## To suppress this message use: suppressPackageStartupMessages(library(dendextend)) ## --------------------- ## ## Attaching package: \u0026#39;dendextend\u0026#39; ## The following object is masked from \u0026#39;package:permute\u0026#39;: ## ## shuffle ## The following object is masked from \u0026#39;package:stats\u0026#39;: ## ## cutree ## [1] \u0026quot;dendextend\u0026quot; # uncomment following line if this is the fist time you\u0026#39;ve installed this package! # library(dendextend) dend1 \u0026lt;- as.dendrogram(hclust(commDist)) wellHung \u0026lt;- hang.dendrogram(dend1) # the cheeky variable names are absolutely essential! plot_horiz.dendrogram(wellHung, side = F, xlab = \u0026quot;Jaccard Dissimilarity\u0026quot;) There you go, an awesome package which save me from wasting too many more days of work!\n","date":1544572800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572962027,"objectID":"a284a2e06326c386229d86141bee39c3","permalink":"/post/barking-up-the-wrong-tree-leaf-your-troubles-behind-with-dendextend/","publishdate":"2018-12-12T00:00:00Z","relpermalink":"/post/barking-up-the-wrong-tree-leaf-your-troubles-behind-with-dendextend/","section":"post","summary":"I was recently asked by one of my PhD supervisors to help out on a paper by doing some metagenomic analyses. My mission was essentially to perform some taxonomic analyses of metagenomes and show how a metagenome generated in our lab related to these. So, naturally, I said yes, carried out the necessary analyses and proceeded to design a figure to show the result. I figured a dendrogram would be a nice way of showing compositional similarity between the community we studied and other communities.","tags":[],"title":"Barking up the wrong tree? Leaf your troubles behind with dendextend","type":"post"},{"authors":null,"categories":[],"content":" I’ve recently started a chapter for my PhD which involves a meta-analysis of microbial ecological literature. This has involved looking through the methods, or more specifically the stats sections, of lots of studies to see whether they are suitable for inclusion in my analysis. One idiosyncracy I noted whilst doing this is the number of studies that refer to an “Adonis test”. As far as I am aware, adonis is simply the name of the function that carries out a permutational MANOVA (multivariate analysis of variance) or non-parametric MANOVA as the original author of this statistical test calls it (Anderson, 2001).\nA quick google search appears to suggest that one potential difference might be that the statistical test carried out by the adonis() function is capable of handling both categoric and continuous predictors. In contrast, non-parametric MANOVA is only able to handle categoric predictors. This difference sort of makes sense, but I’m still not convinced that we should be using the term “adonis test”.\nI was also interested to see how many microbial ecology studies actually refer to this so called “adonis test” to see whether we are to blame for this confusion. I conducted a highly rigorous and robust google scholar search (yep, really!) on the terms “adonis test” and “adonis test” + microb* and limited results from 2001-present.\nAs it turns out, 84% of the studies that mention “adonis test” also feature the word “microb*“, suggesting that we might be to blame! Also, only 11 of these studies used the term”MANOVA“, suggesting that the vast majority did not elaborate on the statistical procedure initiated by the adonis function.\nIf I’m completely wrong on this and there is such a thing as an adonis test then please do tell me. But it seems to be that we need to be a little more careful when describing the statistical methods we use…\n","date":1544572800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544627648,"objectID":"835e7d86bf1336dde4fec8bc89a2c96e","permalink":"/post/does-the-adonis-test-really-exist/","publishdate":"2018-12-12T00:00:00Z","relpermalink":"/post/does-the-adonis-test-really-exist/","section":"post","summary":"I’ve recently started a chapter for my PhD which involves a meta-analysis of microbial ecological literature. This has involved looking through the methods, or more specifically the stats sections, of lots of studies to see whether they are suitable for inclusion in my analysis. One idiosyncracy I noted whilst doing this is the number of studies that refer to an “Adonis test”. As far as I am aware, adonis is simply the name of the function that carries out a permutational MANOVA (multivariate analysis of variance) or non-parametric MANOVA as the original author of this statistical test calls it (Anderson, 2001).","tags":[],"title":"Does the Adonis test really exist?","type":"post"},{"authors":null,"categories":[],"content":" As a microbial ecologist, part of my job is to try and assign taxonomy to all of the microbial critters living in the habitats I study. For archaeal or bacterial 16S rRNA gene sequences this is relatively easy. The Ribosomal Database Project have a naive Bayesian classifier which is trained on a large curated database of archaeal and bacterial 16S rRNA gene sequences. Best of all, it is implemented in the popular bioinformatics pipeline Qiime, making it nice and easy to apply to your own data.\nBut what if you are for dealing with fungal ITS sequences instead? Well a recent paper from Deshpande et al piqued my interest for several reasons. The paper describes a new curated database of fungal ITS sequences and demonstrates that the RDP naive Bayesian classifier does a decent job of classifying ITS sequences when trained on this database.\nUnfortunately, using the RDP classifier on ITS sequences means going off piste so to speak and using the standalone classifier outside of Qiime. Given that most people never use the classifier outside of Qiime, I thought a brief demonstration might be useful. For all of the following, I’m assuming you’re using BioLinux and are at least somewhat familiar with the Linux terminal.\nFirst of all, clone the RDPtools github repo like so:\ngit clone https://github.com/rdpstaff/RDPTools.git Then, enter the newly created directory and set up the RDPtools software:\ncd RDPTools git submodule init git submodule updat make You’ll get a lot of text whizzing through the terminal, don’t worry, you haven’t yet entered the Matrix! Once these processes finish the classifier has installed and is ready for use. However, we must now download some additional files which will allow the classifier to classify our fungal ITS sequences. To download these files, type:\nwget \u0026quot;http://rdp.cme.msu.edu/download/rdpclassifiertraindata/data.tgz\u0026quot; This may take a few minutes depending on your connection speed. The files are packaged in what we call a tarball (kind of like a windows zip folder). To unpack them:\ntar -zxvf data.tgz Now when we list the contents of our directory, we can see a new directory, “data”, has been created. This folder contains all the files we need to use the Warcup based classifier (and also the UNITE and LSU data handily).\nI’m going to assume you’ve already clustered your ITS sequences into operational taxonomic units and picked a representative sequence for each OTU using Qiime or other means.\nNow we simply call the classifier, tell it to use the Warcup classification files, and tell it where out data is. Simple:\njava -Xmx1g -jar classifier.jar classify -t data/classifier/fungalits_warcup/rRNAClassifier.properties -o classifiedFungi.txt /path/to/data/fungalITS_repSet.fasta A new file (“classifiedFungi.txt”) will have been created, containing the taxonomic assignments of each fungal sequence, complete with confidence values, pretty cool! You can of course train the classifier yourself on a custom database, in which case, follow the steps here Anyway, I hope this was useful and demonstrates that venturing outside of Qiime is not necessarily that difficult!\n","date":1544572800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544627648,"objectID":"692844bc2d09446da26b5c80481ac924","permalink":"/post/know-your-fungi-a-brief-demo-of-the-new-fungal-classifier/","publishdate":"2018-12-12T00:00:00Z","relpermalink":"/post/know-your-fungi-a-brief-demo-of-the-new-fungal-classifier/","section":"post","summary":"As a microbial ecologist, part of my job is to try and assign taxonomy to all of the microbial critters living in the habitats I study. For archaeal or bacterial 16S rRNA gene sequences this is relatively easy. The Ribosomal Database Project have a naive Bayesian classifier which is trained on a large curated database of archaeal and bacterial 16S rRNA gene sequences. Best of all, it is implemented in the popular bioinformatics pipeline Qiime, making it nice and easy to apply to your own data.","tags":[],"title":"Know Your Fungi: A Brief Demo Of The New Fungal Classifier","type":"post"},{"authors":null,"categories":[],"content":" This is a quick post more to document some useful code than anything else. When conducting bioinformatic analyses using Qiime, one of the last steps is to cluster sequences into OTUs (operational taxonomic units) and assign taxonomy to them. You can then make an OTU table which contains all your OTUs and their associated taxonomy. Bam, easy!\nBut what to do if you haven’t/don’t want to use Qiime to cluster OTUs? For example, I often use Vsearch to cluster my OTUs and then may use Qiime to assign taxonomy via the RDP classifier. This means we need some way of uniting our already constructed OTU table with our taxonomy file.\nHere is a short piece of R code which will do the job nicely.\n# First, let\u0026#39;s read in our OTU table otuTable \u0026lt;- read.delim(\u0026quot;archOtuTab.txt\u0026quot;, header = T) # now to read in the taxonomy file generated by the RDP classifier in Qiime taxa \u0026lt;- read.table(\u0026quot;archaeaCentroids_tax_assignments.txt\u0026quot;, header = F, col.names= c(\u0026quot;OTUId\u0026quot;, \u0026quot;taxonomy\u0026quot;, \u0026quot;confidence\u0026quot;), stringsAsFactors = F, sep = \u0026quot;\\t\u0026quot;) otuTable \u0026lt;- merge(otuTable, taxa[, -3], by = \u0026quot;OTUId\u0026quot;) # this line then merges the OTU table with taxonomy. Order of OTUs/taxonomy is # not important, R will correctly match OTUs with their taxonomy string # Note that this line will not print the confidence column in the final OTU # table as it is often not that useful... # now we just need to write our completed OTU table to a file write.table(otuTable, \u0026quot;archaeaOtuTableTaxonomy.txt\u0026quot;, sep = \u0026quot;\\t\u0026quot;, quote = F, row.names = F) Done! A simple way of uniting taxonomy information with a standalone OTU table without using Qiime.\n","date":1544572800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544627648,"objectID":"c8794df960ccdc22681818ee845d4b5d","permalink":"/post/merging-taxonomy-with-non-qiime-otu-tables/","publishdate":"2018-12-12T00:00:00Z","relpermalink":"/post/merging-taxonomy-with-non-qiime-otu-tables/","section":"post","summary":"This is a quick post more to document some useful code than anything else. When conducting bioinformatic analyses using Qiime, one of the last steps is to cluster sequences into OTUs (operational taxonomic units) and assign taxonomy to them. You can then make an OTU table which contains all your OTUs and their associated taxonomy. Bam, easy!\nBut what to do if you haven’t/don’t want to use Qiime to cluster OTUs?","tags":[],"title":"Merging Taxonomy With Non-Qiime OTU Tables","type":"post"},{"authors":["**Dave R Clark**","Robert MW Ferguson","Danielle N Harris","Kirsty J Matthews Nicholass","Hannah J Prentice","Kate C Randall","Luli Randell","Scott L Warren","Alex J Dumbrell"],"categories":null,"content":"","date":1517443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549869893,"objectID":"c77a22119d40f4d36065480e71ee8fa4","permalink":"/publication/clark_et_al_wires/","publishdate":"2018-02-01T00:00:00Z","relpermalink":"/publication/clark_et_al_wires/","section":"publication","summary":"Microorganisms are ubiquitous and represent a taxonomically and functionally diverse component of freshwater environments of significant ecological importance. The bacteria, archaea, and microbial eukarya in freshwater systems support a range of ecosystem processes and functions, including mediating all major biogeochemical cycles, and therefore regulate the flow of multiple ecosystem services. Yet relative to conspicuous higher taxa, microbial ecology remains poorly understood. As the anthropocene progresses, the demand for freshwater–ecosystem services is both increasing with growing human population density, and by association, increasingly threatened from multiple and often interacting stressors, such as climate change, eutrophication, and chemical pollution. Thus, it is imperative to understand the ecology of microorganisms and their functional role in freshwater ecosystems if we are to manage the future of these environments effectively. To do this, researchers have developed a vast array of molecular tools that can illuminate the diversity, composition, and activity of microbial communities. Within this primer, we discuss the history of molecular approaches in microbial ecology, and highlight the scope of questions that these methods enable researchers to address. Using some recent case studies, we describe some exemplar research into the microbial ecology of freshwater systems, and emphasize how molecular methods can provide novel ecological insights. Finally, we detail some promising developments within this research field, and how these might shape the future research landscape of freshwater microbial ecology.","tags":["Archaea","Bacteria","Eukarya","Metagenetics","Metagenomics","Next-generation sequencing","Omics methods"],"title":"Streams of data from drops of water: 21st century molecular microbial ecology","type":"publication"},{"authors":["**Dave R Clark**","Mégane Mathieu","Léonie Mourot","Laurent Dufossé","Graham JC Underwood","Alex J Dumbrell","Terry J McGenity"],"categories":null,"content":"","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549869893,"objectID":"fc48cbe4445be5e5c340065dcc3624e2","permalink":"/publication/clark_et_al_halophiles/","publishdate":"2017-12-01T00:00:00Z","relpermalink":"/publication/clark_et_al_halophiles/","section":"publication","summary":"Aim: Biogeographical regions are the fundamental geographical units for grouping Earth's biodiversity. Biogeographical regionalization has been demonstrated for many higher taxa, such as terrestrial plants and vertebrates, but not in microbial communities. Therefore, we sought to test empirically whether microbial communities, or taxa, show patterns consistent with biogeographical regionalization. Location: Within halite (NaCl) crystals from coastal solar salterns of western Europe, the Mediterranean and east Africa. Time period: Modern (2006–2013). Major taxa studied: Archaea. Methods: Using high‐throughput Illumina amplicon sequencing, we generated the most high‐resolution characterization of halite‐associated archaeal communities to date, using samples from 17 locations. We grouped communities into biogeographical clusters based on community turnover to test whether these communities show biogeographical regionalization. To examine whether individual taxa, rather than communities, show biogeographical patterns, we also tested whether the relative abundance of individual genera may be indicative of a community's biogeographical origins using machine learning methods, specifically random forest classification. Results: We found that the rate of community turnover was greatest over subregional spatial scales (","tags":["Archaea","dispersal","halite","halophiles","machine learning","macroecology","next generation sequencing","regionalization"],"title":"Biogeography at the limits of life: Do extremophilic microbial communities show biogeographical regionalization?","type":"publication"},{"authors":["Shazia N Aslam","Alex J Dumbrell","Jamal S Sabir","Mohammed HZ Mutwakil","Mohammed MN Baeshen","Salah EM Abo‐Aba","**Dave R Clark**","Steven A Yates","Nabih A Baeshen","Graham JC Underwood","Terry J McGenity"],"categories":null,"content":"","date":1480550400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549869893,"objectID":"f97ea0d723f7d396236fb0a7d61d56a1","permalink":"/publication/aslam_et_al/","publishdate":"2016-12-01T00:00:00Z","relpermalink":"/publication/aslam_et_al/","section":"publication","summary":"Although desert soils support functionally important microbial communities that affect plant growth and influence many biogeochemical processes, the impact of future changes in precipitation patterns on the microbiota and their activities is largely unknown. We performed in‐situ experiments to investigate the effect of simulated rainfall on bacterial communities associated with the widespread perennial shrub, Rhazya stricta in Arabian desert soils. The bacterial community composition was distinct between three different soil compartments: surface biological crust, root‐attached, and the broader rhizosphere. Simulated rainfall had no significant effect on the overall bacterial community composition, but some population‐level responses were observed, especially in soil crusts where Betaproteobacteria, Sphingobacteria, and Bacilli became more abundant. Bacterial biomass in the nutrient‐rich crust increased three‐fold one week after watering, whereas it did not change in the rhizosphere, despite its much higher water retention. These findings indicate that between rainfall events, desert‐soil microbial communities enter into stasis, with limited species turnover, and reactivate rapidly and relatively uniformly when water becomes available. However, microbiota in the crust, which was relatively enriched in nutrients and organic matter, were primarily water‐limited, compared with the rhizosphere microbiota that were co‐limited by nutrients and water.","tags":[],"title":"Soil compartment is a major determinant of the impact of simulated rainfall on desert microbiota","type":"publication"},{"authors":["Alex J Dumbrell","Robert MW Ferguson","**Dave R Clark**"],"categories":null,"content":"","date":1477958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549869893,"objectID":"de2d2ea75263269f05eb113a4c73a382","permalink":"/publication/dumbrell_ferguson_clark/","publishdate":"2016-11-01T00:00:00Z","relpermalink":"/publication/dumbrell_ferguson_clark/","section":"publication","summary":"Quantifying the functional and taxonomic diversity of microbial assemblages is essential to understanding almost all aspects of microbial ecology. In recent years, the advent of Next Generation Sequencing (NGS) technology has accelerated this process. It is now common practice to target and amplify phylogenetic and/or functional marker genes, and use NGS approaches to characterise their diversity across multiple samples. However, all NGS approaches contain inherent methodological biases, producing both the high-quality data required by researchers, and in addition, erroneous sequences and noise. Careful bioinformatic analysis of NGS data is therefore required to quality filter and process sequences in order to avoid misleading inferences from artifactual results. A similar consideration must also be given to any downstream statistical analysis, as an incorrect choice of approach can also produce false conclusions. These various analytical steps and considerations can appear daunting to the uninitiated and may be perceived as a hurdle to completing research. In this chapter, we aim to provide the methods and guidance required to overcome this hurdle, and impart the skills required for a novice bioinformatician to produce a basic analysis of their NGS amplicon data. We focus on data produced using two common NGS technologies, the historically more widely used 454-pyrosequencer, and the currently more widely used Illumina platform. We cover methods for quality filtering and denoising data, picking Operational Taxonomic Units (OTUs), assigning taxonomy to sequences and basic statistical analyses required for hypothesis testing. Implementation of these methods is demonstrated for both of two commonly used pipelines (QIIME and mothur), and additional stand-alone packages (including R), providing the reader with maximum flexibility when analysing their data.","tags":["16S rRNA","454-Pyrosequencing","Amplicon","Bioinformatics","BioLinux","Illumina","Microbial ecology","Mothur","Next generation sequencing","QIIME"],"title":"Microbial community analysis by single-amplicon high-throughput next generation sequencing: data analysis–from raw output to ecology","type":"publication"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544102939,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":null,"title":"Slides","type":"slides"}]