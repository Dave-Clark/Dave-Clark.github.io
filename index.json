[{"authors":null,"categories":null,"content":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.main]] menu links to it in the config.toml.\n","date":1536447600,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1544102939,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"https://Dave-Clark.github.io/tutorial/","publishdate":"2018-09-09T00:00:00+01:00","relpermalink":"/tutorial/","section":"tutorial","summary":"This feature can be used for publishing content such as:\n Project or software documentation Online courses Tutorials  The parent folder may be renamed, for example, to docs for project documentation or course for creating an online course.\nTo disable this feature, either delete the parent folder, or set draft = true in the front matter of all its pages.\nAfter renaming or deleting the parent folder, you may wish to update any [[menu.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":[],"content":" When beginning analyses on microbial community data, it is often helpful to compute rarefaction curves. A rarefaction curve tells you about the rate at which new species/OTUs are detected as you increase the number of individuals/sequences sampled. It does this by taking random subsamples from 1, up to the size of your sample, and computing the number of species present in each subsample. Ideally, you want your rarefaction curves to be relatively flat, as this indicates that additional sampling would not likely yield further species.\nThe vegan package in R has a nice function for computing rarefaction curves for species by site abundance tables. However, for microbial datasets this function is often prohibitively slow. This is due to the fact that we often have large numbers of samples, and each sample may contain several thousand sequences, meaning that a large number of random subsamples are taken.\nPart of the problem is that the original function only makes use of a single processing core, meaning that random subsamples are computed serially, and each sample is processed serially. This means we could speed the function up by splitting samples across multiple cores. In short, we are parallelising the function.\nBelow is my attempt to modify the original code into a function that can use multiple processor cores to speed up the calculations.\n# you will need to install the parallel package before hand # and the vegan package if you dont have it library(vegan) quickRareCurve \u0026lt;- function (x, step = 1, sample, xlab = \u0026quot;Sample Size\u0026quot;, ylab = \u0026quot;Species\u0026quot;, label = TRUE, col, lty, max.cores = T, nCores = 1, ...) { require(parallel) x \u0026lt;- as.matrix(x) if (!identical(all.equal(x, round(x)), TRUE)) stop(\u0026quot;function accepts only integers (counts)\u0026quot;) if (missing(col)) col \u0026lt;- par(\u0026quot;col\u0026quot;) if (missing(lty)) lty \u0026lt;- par(\u0026quot;lty\u0026quot;) tot \u0026lt;- rowSums(x) # calculates library sizes S \u0026lt;- specnumber(x) # calculates n species for each sample if (any(S \u0026lt;= 0)) { message(\u0026quot;empty rows removed\u0026quot;) x \u0026lt;- x[S \u0026gt; 0, , drop = FALSE] tot \u0026lt;- tot[S \u0026gt; 0] S \u0026lt;- S[S \u0026gt; 0] } # removes any empty rows nr \u0026lt;- nrow(x) # number of samples col \u0026lt;- rep(col, length.out = nr) lty \u0026lt;- rep(lty, length.out = nr) # parallel mclapply # set number of cores mc \u0026lt;- getOption(\u0026quot;mc.cores\u0026quot;, ifelse(max.cores, detectCores(), nCores)) message(paste(\u0026quot;Using \u0026quot;, mc, \u0026quot; cores\u0026quot;)) out \u0026lt;- mclapply(seq_len(nr), mc.cores = mc, function(i) { n \u0026lt;- seq(1, tot[i], by = step) if (n[length(n)] != tot[i]) n \u0026lt;- c(n, tot[i]) drop(rarefy(x[i, ], n)) }) Nmax \u0026lt;- sapply(out, function(x) max(attr(x, \u0026quot;Subsample\u0026quot;))) Smax \u0026lt;- sapply(out, max) plot(c(1, max(Nmax)), c(1, max(Smax)), xlab = xlab, ylab = ylab, type = \u0026quot;n\u0026quot;, ...) if (!missing(sample)) { abline(v = sample) rare \u0026lt;- sapply(out, function(z) approx(x = attr(z, \u0026quot;Subsample\u0026quot;), y = z, xout = sample, rule = 1)$y) abline(h = rare, lwd = 0.5) } for (ln in seq_along(out)) { N \u0026lt;- attr(out[[ln]], \u0026quot;Subsample\u0026quot;) lines(N, out[[ln]], col = col[ln], lty = lty[ln], ...) } if (label) { ordilabel(cbind(tot, S), labels = rownames(x), ...) } invisible(out) } Most of the code is verbatim to the original, but you’ll notice a few extra arguments to specify, and a few extra lines that determine how many cores the function will use.\nEssentially, if you do not specify a number of cores, the function will default to using all available cores, which will allow the quickest calculation. Otherwise, you can specify max.cores = F, which will allow you to specify the number of cores using nCores. This is handy if you need some cores to remain usable whilst running the function.\nBelow is a usage example.\n# load some dummy data data(dune) quickRareCurve(dune) # will use all cores and print how many cores you have quickRareCurve(dune, max.cores = F, nCores = 2) # use only 2 cores We can compare the new function’s performance to the original using the microbenchmark package, as below.\nlibrary(microbenchmark) microbenchmark(rarecurve(dune), quickRareCurve(dune), times = 10) ## Unit: milliseconds ## expr min lq mean median uq ## rarecurve(dune) 35.18197 81.57729 79.22478 82.75713 84.7925 ## quickRareCurve(dune) 98.19293 99.29134 101.25665 100.35054 102.2720 ## max neval ## 90.18635 10 ## 107.72471 10 As you can see, for this small dataset, the original function is actually quicker. This is because it takes some time to distribute tasks among the cores. However, I’ve found for a typical OTU table (\u0026gt; 50 samples, ~ 15,000 sequences per sample), quickRareCurve can be around 3 times faster.\nFeel free to use it as you wish, but I’d be very grateful for any credit if you do use it.\n","date":1544832000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544832000,"objectID":"f3ab09c01c27caa25c786a5b982955b1","permalink":"https://Dave-Clark.github.io/post/speeding-up-rarefaction-curves-for-microbial-community-ecology/","publishdate":"2018-12-15T00:00:00Z","relpermalink":"/post/speeding-up-rarefaction-curves-for-microbial-community-ecology/","section":"post","summary":"When beginning analyses on microbial community data, it is often helpful to compute rarefaction curves. A rarefaction curve tells you about the rate at which new species/OTUs are detected as you increase the number of individuals/sequences sampled. It does this by taking random subsamples from 1, up to the size of your sample, and computing the number of species present in each subsample. Ideally, you want your rarefaction curves to be relatively flat, as this indicates that additional sampling would not likely yield further species.","tags":[],"title":"Speeding up rarefaction curves for microbial community ecology","type":"post"},{"authors":null,"categories":[],"content":" I was recently asked via twitter to share some code on how I implemented the facet_zoom() function, from the newly released ggforce package, to zoom in on a particular region of a map. I’ve no idea if this is the “best” way of creating these sorts of maps, but anyway, here goes…\nThis way of making maps requires a number of packages, so make sure you have these installed first, then load them like so…\nlibrary(mapdata) library(maptools) library(raster) library(rgeos) library(ggplot2) library(ggforce) library(ggsn) Our first step is to source the data that will form our map’s base layer. This comes from the mapdata package, and we will need the worldHires data. You’ll get an automatic graphical output which should be a map of the world.\nmapBase \u0026lt;- map(\u0026quot;worldHires\u0026quot;, fill = T) We now need to manipulate these data a bit to go from a map class object, to a SpatialPolygons object, which we can use with R’s GIS capabilities.\n# first extract the IDs of what will be each spatial polygon ids \u0026lt;- sapply(strsplit(mapBase$names, \u0026quot;:\u0026quot;), function(x) tail(x, n = 1)) # then, coerce map to spatial polygons object mapPoly \u0026lt;- map2SpatialPolygons(mapBase, IDs = ids, proj4string = CRS(\u0026quot;+proj=longlat +datum=WGS84\u0026quot;)) Next, we need to create an extent object to limit the spatial polygons to the region we are interested in plotting. The extent object will also need to share the same projection as the spatial polygons object. The coordinates in the extent call are given in the form Xmin, Xmax, Ymin, Ymax.\n# create an extent and coerce to spatial polygon class mapExtent \u0026lt;- as(extent(-20, 60, -40, 60), \u0026quot;SpatialPolygons\u0026quot;) # change projection to match the spatial polygon map proj4string(mapExtent) \u0026lt;- CRS(proj4string(mapPoly)) Now we use the gBuffer and gIntersection functions to clip the spatial polygon map down to our region of interest. The 0 width buffer applied helps mop up topology errors with polygons self-intersecting.\nbuffMap \u0026lt;- gBuffer(mapPoly, byid = TRUE, width = 0) cropMap \u0026lt;- gIntersection(buffMap, mapExtent, byid = TRUE) Phew, that’s all the nasty GIS stuff dealt with. The rest is fairly standard data manipulation, then making the map look pretty and informative.\nggplot is unable to deal with many spatial data formats, so we need to turn it into something it can read, a data.frame.\nmapDf \u0026lt;- fortify(cropMap, region = \u0026quot;id\u0026quot;) Now let’s simulate some points to plot onto our map.\nlocations \u0026lt;- data.frame(lat = runif(25, -40, 60), long = runif(25, -20, 60)) We now have enough to make a basic, but effective map.\nggplot() + geom_map(map = mapDf, data = mapDf, aes(x = long, y = lat, map_id = id), fill = \u0026quot;lightgrey\u0026quot;, color = \u0026quot;lightgrey\u0026quot;) + geom_point(data = locations, aes(x = long, y = lat, group = NULL)) Not bad, but we can do better. The background is annoying and needs to go, the points could be made bigger and clearer, and don’t forget to add a scalebar! Also, say there are lots of points in one region that we want to zoom in on, we could use the facet_zoom function to get a closer look at those points.\nIn order to use facet_zoom, we need to create a factor level in the locations data that we can use to subset the data. Here, I want to zoom in on all points with a longitude and latitude greater than 20, so I need to create a factor which reflects this.\nlocations$region \u0026lt;- ifelse(locations$lat \u0026gt; 20 \u0026amp; locations$long \u0026gt; 20, 1, 2) # this designates all points in my region of interest as 1\u0026#39;s, and # points outside the region are 2\u0026#39;s Now to tidy up our map.\nggplot() + geom_map(map = mapDf, data = mapDf, aes(x = long, y = lat, map_id = id), fill = \u0026quot;lightgrey\u0026quot;, color = \u0026quot;lightgrey\u0026quot;) + geom_point(data = locations, aes(x = long, y = lat, group = NULL), size = 2.5) + ggsn:::scalebar(mapDf, dist = 2000, location = \u0026quot;bottomright\u0026quot;, model = \u0026quot;WGS84\u0026quot;, dd2km = T, st.size = 5, anchor = c(x = 55, y = -40)) + facet_zoom(xy = region == 1, zoom.size = 1) + labs(x = \u0026quot;Latitude (decimal degrees)\u0026quot;, y = \u0026quot;Longitude (decimal degrees)\u0026quot;) + theme_bw() + theme(axis.text = element_text(size = 16), axis.title = element_text(size = 18), panel.grid.minor = element_blank(), panel.grid.major = element_blank()) Depending on the dimensions of your zoomed in region, you may want to play around with the zoom.size parameter to get the right aspect ratio. But there you have it, combining ggforce with ggplot2 to make lovely looking maps!\n","date":1544832000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544832000,"objectID":"0c53ddcc66fb7e97822cb7a7171b7f8f","permalink":"https://Dave-Clark.github.io/post/zoomed-maps-with-ggforce/","publishdate":"2018-12-15T00:00:00Z","relpermalink":"/post/zoomed-maps-with-ggforce/","section":"post","summary":"I was recently asked via twitter to share some code on how I implemented the facet_zoom() function, from the newly released ggforce package, to zoom in on a particular region of a map. I’ve no idea if this is the “best” way of creating these sorts of maps, but anyway, here goes…\nThis way of making maps requires a number of packages, so make sure you have these installed first, then load them like so…","tags":[],"title":"Zoomed maps with ggforce!","type":"post"},{"authors":null,"categories":[],"content":" If you use R to analyse and plot your data, then you’ve probably heard of and used the ggplot2 package, written by Hadley Wickham. ggplot2 is a highly flexible plotting package allowing you to create just about any kind of plot you can think of, and customise just about any aspect of your plot.\nHowever, ggplot2 is also known for it’s somewhat strange choice of default options (at least, they seem strange to me!). Therefore, it can seem like a lot of work to go from a basic plot to something that is approaching publication quality. Whilst there are many excellent guides out there to help with this process, I thought I’d weigh in with a few tips and personal preferences which have helped me to elevate my plots to another level.\nLet’s begin by loading the package and simulating some data to play around with.\nlibrary(ggplot2) myData \u0026lt;- data.frame(var1 = runif(1000, 0, 100), var2 = runif(1000, -10, 10), catVar = rep(c(1:4), times = 250)) head(myData) ## var1 var2 catVar ## 1 94.23994 -1.937782 1 ## 2 55.38800 3.144662 2 ## 3 47.82335 -2.186647 3 ## 4 26.30024 -4.740056 4 ## 5 58.41232 -8.794706 1 ## 6 46.38606 -6.323146 2 Hopefully that code should be fairly easy to understand, we’ve simulated two continuous variables (with a random distribution) and created a categorical variable.\nNow let’s create the most basic scatter plot possible with this data.\nplot1 \u0026lt;- ggplot(myData, aes(x = var1, y = var2)) + geom_point() plot1 That looks ok, but it’s not perfect. The text and points are quite small, the black points are bit abrasive on the eye and the grey background is a little distracting. Additionally, the axis labels are not informative and we are also obscuring potentially interesting trends by plotting all of the data from our different groups together. So, lets set about fixing those things.\nggplot(myData, aes(x = var1, y = var2)) + geom_point(size = 2, color = \u0026quot;grey\u0026quot;) + facet_wrap(~catVar) So now the points are a little bigger and grey, which makes them easier to look at and with an aesthetically nicer muted contrast. We’ve also split the data into 4 separate panels, so that we can seen any group specific trends far easier, and without the visual clutter of lots of different colour points (ideal when there is a monetary cost to publishing color figures). But that grey background is now obscuring things, and our axis labels are still hard to read and uninformative. Let’s fix those.\nggplot(myData, aes(x = var1, y = var2)) + geom_point(size = 2, color = \u0026quot;grey\u0026quot;) + facet_wrap(~catVar) + labs(x = \u0026quot;Variable 1\u0026quot;, y = \u0026quot;Variable 2\u0026quot;) + theme_bw() We’re starting to look quite good now, the points are now much more visible, yet not too harsh against the white background, and our axis labels now tell us something about the data (if it were real!). Now we can apply some final polish to really get this looking great!\nfinalPlot \u0026lt;- ggplot(myData, aes(x = var1, y = var2)) + geom_point(size = 2, color = \u0026quot;grey\u0026quot;) + facet_wrap(~catVar) + labs(x = \u0026quot;Variable 1\u0026quot;, y = \u0026quot;Variable 2\u0026quot;) + theme_bw() + theme(axis.text = element_text(size = 16), axis.title = element_text(size = 18), strip.text.x = element_text(size = 16), panel.grid.major = element_blank()) Done! The text around the plot is now much easier to see, and we’ve removed the major gridlines which means less distraction from the data. Everything is far clearer and easier to read.\nplot1 finalPlot One final tip, save your graphs as pdfs. PDFs are known as vector based graphics and maintain resolution far better than bitmap based graphics such as PNGs, JPEGs or TIFs. The added bonus is that many journals also request your figures to be in a vector based format, so you’re also helping your figures become published.\nggsave(\u0026quot;myPlot.pdf\u0026quot;, finalPlot, width = 8, height = 8, device = \u0026quot;pdf\u0026quot;) ","date":1544832000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544832000,"objectID":"667fc2ab4d5ffae45551a55f3f5fe1b8","permalink":"https://Dave-Clark.github.io/post/ggplot2-from-default-to-delightful/","publishdate":"2018-12-15T00:00:00Z","relpermalink":"/post/ggplot2-from-default-to-delightful/","section":"post","summary":"If you use R to analyse and plot your data, then you’ve probably heard of and used the ggplot2 package, written by Hadley Wickham. ggplot2 is a highly flexible plotting package allowing you to create just about any kind of plot you can think of, and customise just about any aspect of your plot.\nHowever, ggplot2 is also known for it’s somewhat strange choice of default options (at least, they seem strange to me!","tags":[],"title":"ggplot2; From default to delightful","type":"post"},{"authors":null,"categories":[],"content":" I was recently asked by one of my PhD supervisors to help out on a paper by doing some metagenomic analyses. My mission was essentially to perform some taxonomic analyses of metagenomes and show how a metagenome generated in our lab related to these. So, naturally, I said yes, carried out the necessary analyses and proceeded to design a figure to show the result. I figured a dendrogram would be a nice way of showing compositional similarity between the community we studied and other communities. First of all, I set about creating a distance matrix and plotting the dendrogram like so:\nlibrary(vegan) ## Loading required package: permute ## Loading required package: lattice ## This is vegan 2.5-3 data(dune) # load some dummy data commDist \u0026lt;- vegdist(dune, \u0026quot;jaccard\u0026quot;) # calculate the community similarity distance matrix plot(hclust(commDist)) There you go. It’s easy to generate a basic dendrogram using any sort of distance matrix. So I then started tidying it up to make it a bit more presentable.\nsiteLabels \u0026lt;- paste(\u0026quot;Sample \u0026quot;, rownames(dune), sep = \u0026quot;\u0026quot;) # create some sample labels plot(hclust(commDist), labels = siteLabels, main = \u0026quot;\u0026quot;, sub = \u0026quot;\u0026quot;, xlab = \u0026quot;\u0026quot;, ylab = \u0026quot;Jaccard Similarity\u0026quot;, lwd = 2, cex = 1.2) Much nicer! We’ve gotten rid of the nonsense at the bottom, given a more informative y axis label and generally made things ‘nicer’. Except, when I saw the first draft, I saw my supervisor had rotated the plot so that the leaf tips pointed to the left. For some reason this just looked wrong to my eyes and so I opted to replot the dendrogram, in a more sensible orientation.\nNote that to do this, we have to change our approach slightly. For some stupid reason, when we want to plot a dendrogram horizontally, we have to coerce the hclust output to dendrogram class. Additionally, the ‘plot’ method for a dendrogram class object no longer accepts the labels parameter, so we must rename the rows of our dataframe and recalculate the distance matrix instead.\nrownames(dune) \u0026lt;- siteLabels # rename rows of dataframe with our sample labels created earlier commDist \u0026lt;- vegdist(dune, \u0026quot;jaccard\u0026quot;) # recalculate the distance matrix so that it features our new sample names par(mar = c(5, 1, 1, 5)) # adjust margins to make room for tip labels # replot the dendrogram, note that we can now remove the \u0026quot;main =\u0026quot; and \u0026quot;sub =\u0026quot; arguments # also remember to switch the x and y labels! plot(as.dendrogram(hclust(commDist)), xlab = \u0026quot;Jaccard Similarity\u0026quot;, lwd = 2, cex = 1.2, horiz = T) Perfect! Or at least I thought so. Turns out my supervisor liked the improved orientation but didn’t like the way the branches all ended at the same point. That should be easy to correct right? Wrong!\nI spent near enough an entire day trying to get this plot perfect without success. I went as far as exploring the ggdendro package which allows plotting of dendrograms in a ggplot-esque manner.\nThen I stumbled on a solution, enter dendextend! A quick peek of the package manual reveals some really awesome capabilities, I really urge you to take a look as some of the figures you can create are amazing. For my humble needs, this package solved all my problems easily, and in a couple of lines I’d created exactly the figure my supervisor wanted.\nifelse(\u0026quot;dendextend\u0026quot; %in% rownames(installed.packages()) == T, library(dendextend), install.packages(dendextend)) ## ## --------------------- ## Welcome to dendextend version 1.9.0 ## Type citation(\u0026#39;dendextend\u0026#39;) for how to cite the package. ## ## Type browseVignettes(package = \u0026#39;dendextend\u0026#39;) for the package vignette. ## The github page is: https://github.com/talgalili/dendextend/ ## ## Suggestions and bug-reports can be submitted at: https://github.com/talgalili/dendextend/issues ## Or contact: \u0026lt;tal.galili@gmail.com\u0026gt; ## ## To suppress this message use: suppressPackageStartupMessages(library(dendextend)) ## --------------------- ## ## Attaching package: \u0026#39;dendextend\u0026#39; ## The following object is masked from \u0026#39;package:permute\u0026#39;: ## ## shuffle ## The following object is masked from \u0026#39;package:stats\u0026#39;: ## ## cutree ## [1] \u0026quot;dendextend\u0026quot; # uncomment following line if this is the fist time you\u0026#39;ve installed this package! # library(dendextend) dend1 \u0026lt;- as.dendrogram(hclust(commDist)) wellHung \u0026lt;- hang.dendrogram(dend1) # the cheeky variable names are absolutely essential! plot_horiz.dendrogram(wellHung, side = F, xlab = \u0026quot;Jaccard Dissimilarity\u0026quot;) There you go, an awesome package which save me from wasting too many more days of work!\n","date":1544572800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544627648,"objectID":"a284a2e06326c386229d86141bee39c3","permalink":"https://Dave-Clark.github.io/post/barking-up-the-wrong-tree-leaf-your-troubles-behind-with-dendextend/","publishdate":"2018-12-12T00:00:00Z","relpermalink":"/post/barking-up-the-wrong-tree-leaf-your-troubles-behind-with-dendextend/","section":"post","summary":"I was recently asked by one of my PhD supervisors to help out on a paper by doing some metagenomic analyses. My mission was essentially to perform some taxonomic analyses of metagenomes and show how a metagenome generated in our lab related to these. So, naturally, I said yes, carried out the necessary analyses and proceeded to design a figure to show the result. I figured a dendrogram would be a nice way of showing compositional similarity between the community we studied and other communities.","tags":[],"title":"Barking up the wrong tree? Leaf your troubles behind with dendextend","type":"post"},{"authors":null,"categories":[],"content":" I’ve recently started a chapter for my PhD which involves a meta-analysis of microbial ecological literature. This has involved looking through the methods, or more specifically the stats sections, of lots of studies to see whether they are suitable for inclusion in my analysis. One idiosyncracy I noted whilst doing this is the number of studies that refer to an “Adonis test”. As far as I am aware, adonis is simply the name of the function that carries out a permutational MANOVA (multivariate analysis of variance) or non-parametric MANOVA as the original author of this statistical test calls it (Anderson, 2001).\nA quick google search appears to suggest that one potential difference might be that the statistical test carried out by the adonis() function is capable of handling both categoric and continuous predictors. In contrast, non-parametric MANOVA is only able to handle categoric predictors. This difference sort of makes sense, but I’m still not convinced that we should be using the term “adonis test”.\nI was also interested to see how many microbial ecology studies actually refer to this so called “adonis test” to see whether we are to blame for this confusion. I conducted a highly rigorous and robust google scholar search (yep, really!) on the terms “adonis test” and “adonis test” + microb* and limited results from 2001-present.\nAs it turns out, 84% of the studies that mention “adonis test” also feature the word “microb*“, suggesting that we might be to blame! Also, only 11 of these studies used the term”MANOVA“, suggesting that the vast majority did not elaborate on the statistical procedure initiated by the adonis function.\nIf I’m completely wrong on this and there is such a thing as an adonis test then please do tell me. But it seems to be that we need to be a little more careful when describing the statistical methods we use…\n","date":1544572800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544627648,"objectID":"835e7d86bf1336dde4fec8bc89a2c96e","permalink":"https://Dave-Clark.github.io/post/does-the-adonis-test-really-exist/","publishdate":"2018-12-12T00:00:00Z","relpermalink":"/post/does-the-adonis-test-really-exist/","section":"post","summary":"I’ve recently started a chapter for my PhD which involves a meta-analysis of microbial ecological literature. This has involved looking through the methods, or more specifically the stats sections, of lots of studies to see whether they are suitable for inclusion in my analysis. One idiosyncracy I noted whilst doing this is the number of studies that refer to an “Adonis test”. As far as I am aware, adonis is simply the name of the function that carries out a permutational MANOVA (multivariate analysis of variance) or non-parametric MANOVA as the original author of this statistical test calls it (Anderson, 2001).","tags":[],"title":"Does the Adonis test really exist?","type":"post"},{"authors":null,"categories":[],"content":" As a microbial ecologist, part of my job is to try and assign taxonomy to all of the microbial critters living in the habitats I study. For archaeal or bacterial 16S rRNA gene sequences this is relatively easy. The Ribosomal Database Project have a naive Bayesian classifier which is trained on a large curated database of archaeal and bacterial 16S rRNA gene sequences. Best of all, it is implemented in the popular bioinformatics pipeline Qiime, making it nice and easy to apply to your own data.\nBut what if you are for dealing with fungal ITS sequences instead? Well a recent paper from Deshpande et al piqued my interest for several reasons. The paper describes a new curated database of fungal ITS sequences and demonstrates that the RDP naive Bayesian classifier does a decent job of classifying ITS sequences when trained on this database.\nUnfortunately, using the RDP classifier on ITS sequences means going off piste so to speak and using the standalone classifier outside of Qiime. Given that most people never use the classifier outside of Qiime, I thought a brief demonstration might be useful. For all of the following, I’m assuming you’re using BioLinux and are at least somewhat familiar with the Linux terminal.\nFirst of all, clone the RDPtools github repo like so:\ngit clone https://github.com/rdpstaff/RDPTools.git Then, enter the newly created directory and set up the RDPtools software:\ncd RDPTools git submodule init git submodule updat make You’ll get a lot of text whizzing through the terminal, don’t worry, you haven’t yet entered the Matrix! Once these processes finish the classifier has installed and is ready for use. However, we must now download some additional files which will allow the classifier to classify our fungal ITS sequences. To download these files, type:\nwget \u0026quot;http://rdp.cme.msu.edu/download/rdpclassifiertraindata/data.tgz\u0026quot; This may take a few minutes depending on your connection speed. The files are packaged in what we call a tarball (kind of like a windows zip folder). To unpack them:\ntar -zxvf data.tgz Now when we list the contents of our directory, we can see a new directory, “data”, has been created. This folder contains all the files we need to use the Warcup based classifier (and also the UNITE and LSU data handily).\nI’m going to assume you’ve already clustered your ITS sequences into operational taxonomic units and picked a representative sequence for each OTU using Qiime or other means.\nNow we simply call the classifier, tell it to use the Warcup classification files, and tell it where out data is. Simple:\njava -Xmx1g -jar classifier.jar classify -t data/classifier/fungalits_warcup/rRNAClassifier.properties -o classifiedFungi.txt /path/to/data/fungalITS_repSet.fasta A new file (“classifiedFungi.txt”) will have been created, containing the taxonomic assignments of each fungal sequence, complete with confidence values, pretty cool! You can of course train the classifier yourself on a custom database, in which case, follow the steps here Anyway, I hope this was useful and demonstrates that venturing outside of Qiime is not necessarily that difficult!\n","date":1544572800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544627648,"objectID":"692844bc2d09446da26b5c80481ac924","permalink":"https://Dave-Clark.github.io/post/know-your-fungi-a-brief-demo-of-the-new-fungal-classifier/","publishdate":"2018-12-12T00:00:00Z","relpermalink":"/post/know-your-fungi-a-brief-demo-of-the-new-fungal-classifier/","section":"post","summary":"As a microbial ecologist, part of my job is to try and assign taxonomy to all of the microbial critters living in the habitats I study. For archaeal or bacterial 16S rRNA gene sequences this is relatively easy. The Ribosomal Database Project have a naive Bayesian classifier which is trained on a large curated database of archaeal and bacterial 16S rRNA gene sequences. Best of all, it is implemented in the popular bioinformatics pipeline Qiime, making it nice and easy to apply to your own data.","tags":[],"title":"Know Your Fungi: A Brief Demo Of The New Fungal Classifier","type":"post"},{"authors":null,"categories":[],"content":" This is a quick post more to document some useful code than anything else. When conducting bioinformatic analyses using Qiime, one of the last steps is to cluster sequences into OTUs (operational taxonomic units) and assign taxonomy to them. You can then make an OTU table which contains all your OTUs and their associated taxonomy. Bam, easy!\nBut what to do if you haven’t/don’t want to use Qiime to cluster OTUs? For example, I often use Vsearch to cluster my OTUs and then may use Qiime to assign taxonomy via the RDP classifier. This means we need some way of uniting our already constructed OTU table with our taxonomy file.\nHere is a short piece of R code which will do the job nicely.\n# First, let\u0026#39;s read in our OTU table otuTable \u0026lt;- read.delim(\u0026quot;archOtuTab.txt\u0026quot;, header = T) # now to read in the taxonomy file generated by the RDP classifier in Qiime taxa \u0026lt;- read.table(\u0026quot;archaeaCentroids_tax_assignments.txt\u0026quot;, header = F, col.names= c(\u0026quot;OTUId\u0026quot;, \u0026quot;taxonomy\u0026quot;, \u0026quot;confidence\u0026quot;), stringsAsFactors = F, sep = \u0026quot;\\t\u0026quot;) otuTable \u0026lt;- merge(otuTable, taxa[, -3], by = \u0026quot;OTUId\u0026quot;) # this line then merges the OTU table with taxonomy. Order of OTUs/taxonomy is # not important, R will correctly match OTUs with their taxonomy string # Note that this line will not print the confidence column in the final OTU # table as it is often not that useful... # now we just need to write our completed OTU table to a file write.table(otuTable, \u0026quot;archaeaOtuTableTaxonomy.txt\u0026quot;, sep = \u0026quot;\\t\u0026quot;, quote = F, row.names = F) Done! A simple way of uniting taxonomy information with a standalone OTU table without using Qiime.\n","date":1544572800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544627648,"objectID":"c8794df960ccdc22681818ee845d4b5d","permalink":"https://Dave-Clark.github.io/post/merging-taxonomy-with-non-qiime-otu-tables/","publishdate":"2018-12-12T00:00:00Z","relpermalink":"/post/merging-taxonomy-with-non-qiime-otu-tables/","section":"post","summary":"This is a quick post more to document some useful code than anything else. When conducting bioinformatic analyses using Qiime, one of the last steps is to cluster sequences into OTUs (operational taxonomic units) and assign taxonomy to them. You can then make an OTU table which contains all your OTUs and their associated taxonomy. Bam, easy!\nBut what to do if you haven’t/don’t want to use Qiime to cluster OTUs?","tags":[],"title":"Merging Taxonomy With Non-Qiime OTU Tables","type":"post"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536447600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544102939,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"https://Dave-Clark.github.io/tutorial/example/","publishdate":"2018-09-09T00:00:00+01:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":[],"categories":null,"content":"Click on the Slides button above to view the built-in slides feature.\n Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using url_slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544102939,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://Dave-Clark.github.io/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":"","date":1461711600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544102939,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://Dave-Clark.github.io/project/external-project/","publishdate":"2016-04-27T00:00:00+01:00","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461711600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544102939,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"https://Dave-Clark.github.io/project/internal-project/","publishdate":"2016-04-27T00:00:00+01:00","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["GA Cushen"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1441062000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544102939,"objectID":"d77fa4a74076ffcd7ca6c21cfc27a4b2","permalink":"https://Dave-Clark.github.io/publication/person-re-id/","publishdate":"2015-09-01T00:00:00+01:00","relpermalink":"/publication/person-re-id/","section":"publication","summary":"Person re-identification is a critical security task for recognizing a person across spatially disjoint sensors. Previous work can be computationally intensive and is mainly based on low-level cues extracted from RGB data and implemented on a PC for a fixed sensor network (such as traditional CCTV). We present a practical and efficient framework for mobile devices (such as smart phones and robots) where high-level semantic soft biometrics are extracted from RGB and depth data. By combining these cues, our approach attempts to provide robustness to noise, illumination, and minor variations in clothing. This mobile approach may be particularly useful for the identification of persons in areas ill-served by fixed sensors or for tasks where the sensor position and direction need to dynamically adapt to a target. Results on the BIWI dataset are preliminary but encouraging. Further evaluation and demonstration of the system will be available on our website.","tags":[],"title":"A Person Re-Identification System For Mobile Devices","type":"publication"},{"authors":["GA Cushen","MS Nixon"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1372633200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544102939,"objectID":"2b4d919e3cf73dfcd0063c88fe01cb00","permalink":"https://Dave-Clark.github.io/publication/clothing-search/","publishdate":"2013-07-01T00:00:00+01:00","relpermalink":"/publication/clothing-search/","section":"publication","summary":"We present a mobile visual clothing search system whereby a smart phone user can either choose a social networking photo or take a new photo of a person wearing clothing of interest and search for similar clothing in a retail database. From the query image, the person is detected, clothing is segmented, and clothing features are extracted and quantized. The information is sent from the phone client to a server, where the feature vector of the query image is used to retrieve similar clothing products from online databases. The phone's GPS location is used to re-rank results by retail store location. State of the art work focuses primarily on the recognition of a diverse range of clothing offline and pays little attention to practical applications. Evaluated on a challenging dataset, the system is relatively fast and achieves promising results.","tags":[],"title":"Mobile visual clothing search","type":"publication"},{"authors":null,"categories":null,"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544102939,"objectID":"c2915ec5da95791851caafdcba9664af","permalink":"https://Dave-Clark.github.io/slides/example-slides/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/example-slides/","section":"slides","summary":"Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$","tags":null,"title":"Slides","type":"slides"}]