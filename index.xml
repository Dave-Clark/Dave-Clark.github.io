<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Microbial Ecologist on Microbial Ecologist</title>
    <link>https://Dave-Clark.github.io/</link>
    <description>Recent content in Microbial Ecologist on Microbial Ecologist</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-uk</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0100</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>What drives study‐dependent differences in distance–decay relationships of microbial communities?</title>
      <link>https://Dave-Clark.github.io/publication/clark_et_al_geb_2020/</link>
      <pubDate>Mon, 08 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://Dave-Clark.github.io/publication/clark_et_al_geb_2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Relationships between nitrogen cycling microbial community abundance and composition reveal the indirect effect of soil pH on oak decline</title>
      <link>https://Dave-Clark.github.io/publication/scarlett_et_al_isme/</link>
      <pubDate>Fri, 16 Oct 2020 00:00:00 +0100</pubDate>
      
      <guid>https://Dave-Clark.github.io/publication/scarlett_et_al_isme/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Bacterial Community Legacy Effects Following the Agia Zoni II Oil-Spill, Greece</title>
      <link>https://Dave-Clark.github.io/publication/thomas_et_al_frontiers/</link>
      <pubDate>Fri, 17 Jul 2020 00:00:00 +0100</pubDate>
      
      <guid>https://Dave-Clark.github.io/publication/thomas_et_al_frontiers/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Coral microbiome composition along the northern Red Sea suggests high plasticity of bacterial and specificity of endosymbiotic dinoflagellate communities</title>
      <link>https://Dave-Clark.github.io/publication/osman_et_al/</link>
      <pubDate>Thu, 06 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://Dave-Clark.github.io/publication/osman_et_al/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Nitrogen oxidation consortia dynamics influence the performance of full-scale rotating biological contactors</title>
      <link>https://Dave-Clark.github.io/publication/freeman_et_al_envint/</link>
      <pubDate>Sat, 01 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://Dave-Clark.github.io/publication/freeman_et_al_envint/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Mineralization and nitrification: Archaea dominate ammonia-oxidising communities in grassland soils</title>
      <link>https://Dave-Clark.github.io/publication/clark_et_al_sbb/</link>
      <pubDate>Mon, 20 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://Dave-Clark.github.io/publication/clark_et_al_sbb/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Extremely halophilic archaeal communities are resilient to short‐term entombment in halite</title>
      <link>https://Dave-Clark.github.io/publication/huby_et_al/</link>
      <pubDate>Thu, 09 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://Dave-Clark.github.io/publication/huby_et_al/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Impacts of Long‐Term Elevated Atmospheric CO₂ Concentrations on Communities of Arbuscular Mycorrhizal Fungi</title>
      <link>https://Dave-Clark.github.io/publication/macek_et_al/</link>
      <pubDate>Mon, 24 Jun 2019 00:00:00 +0100</pubDate>
      
      <guid>https://Dave-Clark.github.io/publication/macek_et_al/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Calculating the Ecological Hydrocarbon Exposure Index in R</title>
      <link>https://Dave-Clark.github.io/post/calculating-the-ecological-hydrocarbon-exposure-index-in-r/</link>
      <pubDate>Wed, 06 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Dave-Clark.github.io/post/calculating-the-ecological-hydrocarbon-exposure-index-in-r/</guid>
      <description>


&lt;p&gt;Recently, I’ve been collaborating with several colleagues on projects focussing on the microbial ecology of hydrocarbon degradation. One of the aspects we’ve been thinking about, is whether the composition of the microbial community can reflect the level of hydrocarbon exposure. As it turns out, we aren’t the first to consider this, as Mariana Lozada and colleagues came up with an “Ecological Hydrocarbon Exposure Index”, which uses the composition and structure of the microbial community to quantify the level of hydrocarbon exposure in a given environment &lt;span class=&#34;citation&#34;&gt;(Lozada &lt;em&gt;et al.&lt;/em&gt;, &lt;a href=&#34;#ref-lozada_2014&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt;. To their credit, the authors provided &lt;code&gt;R&lt;/code&gt; code to calculate their index in a supplementary PDF on the &lt;a href=&#34;https://www.jstage.jst.go.jp/article/jsme2/advpub/0/advpub_ME14028/_article/-char/ja/&#34;&gt;journal article page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, the original code was designed to run using a &lt;code&gt;Mothur&lt;/code&gt; formatted taxonomy file, which is not useful for users who might prefer other taxonomy assignment pipelines. Therefore, I decided to rewrite their script into a more useful function format, that can be applied to an OTU table with any taxonomy format. The function is hosted in my GitHub based package &lt;code&gt;ecolFudge&lt;/code&gt;, and an example of how to use this function is presented below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# first lets import a small OTU table as an example

otus &amp;lt;- read.csv(&amp;quot;example_OTU_table.csv&amp;quot;)

# look at structure of OTU table
# note that our genus level taxonomy assignment is in &amp;quot;Genus&amp;quot; column
# and our samples all start with &amp;quot;THx&amp;quot;
str(otus)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    3488 obs. of  26 variables:
##  $ OTU              : Factor w/ 3488 levels &amp;quot;OTU_1&amp;quot;,&amp;quot;OTU_10&amp;quot;,..: 1 1103 2205 2823 2934 3045 3156 3267 3378 2 ...
##  $ THxT1x1Bac       : int  3 3429 4653 57 1 190 555 58 0 1 ...
##  $ THxT1x2Bac       : int  1 24761 8476 144 11 1149 10748 628 3 19 ...
##  $ THxT1x3Bac       : int  0 1301 511 13 0 72 101 61 1 0 ...
##  $ THxT21x1Bac      : int  9955 5 52 332 871 127 3 33 207 52 ...
##  $ THxT21x2Bac      : int  1148 51 273 1956 2237 222 18 23 144 94 ...
##  $ THxT21x3Bac      : int  4155 17 328 2427 6246 886 34 198 1632 113 ...
##  $ THxT3x1Bac       : int  3 7232 972 392 4 397 496 3002 0 19 ...
##  $ THxT3x2Bac       : int  1 10328 939 798 22 177 962 3440 1 28 ...
##  $ THxT3x3Bac       : int  1 12733 3892 1559 15 517 1960 2511 0 61 ...
##  $ THxT7x1Bac       : int  31 1583 913 3835 2841 689 253 1392 1789 726 ...
##  $ THxT7x2Bac       : int  4774 1422 614 2181 3066 352 461 1351 15 2378 ...
##  $ THxT7x3Bac       : int  2651 2689 592 6684 1862 425 210 4090 617 1226 ...
##  $ sum              : int  572326 423114 308751 239756 239329 159273 158127 152364 137010 118647 ...
##  $ Domain           : Factor w/ 1 level &amp;quot;Bacteria&amp;quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ Confidence_domain: num  1 1 1 1 1 1 1 1 1 1 ...
##  $ Phylum           : Factor w/ 37 levels &amp;quot;Acetothermia&amp;quot;,..: 32 32 32 32 32 32 32 32 32 32 ...
##  $ Confidence_phylum: num  1 1 1 1 1 1 1 1 1 1 ...
##  $ Class            : Factor w/ 76 levels &amp;quot;Acetothermia_genera_incertae_sedis&amp;quot;,..: 45 45 45 45 45 45 39 45 15 15 ...
##  $ Confidence_class : num  1 1 1 1 1 1 1 1 1 1 ...
##  $ Order            : Factor w/ 142 levels &amp;quot;Acanthopleuribacterales&amp;quot;,..: 96 96 9 9 136 107 24 96 110 110 ...
##  $ Confidence_order : num  1 1 1 0.99 1 1 1 1 1 1 ...
##  $ Family           : Factor w/ 283 levels &amp;quot;Acanthopleuribacteraceae&amp;quot;,..: 11 183 212 64 202 214 40 183 222 222 ...
##  $ Confidence_family: num  1 1 0.98 0.94 1 1 1 1 1 1 ...
##  $ Genus            : Factor w/ 878 levels &amp;quot;Acanthopleuribacter&amp;quot;,..: 32 556 644 170 189 653 79 807 656 654 ...
##  $ Confidence_genus : num  1 1 0.98 0.94 1 0.97 1 1 0.48 0.99 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# we first need to create a vector of sample names that we wish to calculate
# the index for
samples &amp;lt;- grep(&amp;quot;THx&amp;quot;, colnames(otus), value = T)

# now we can load the ecolFudge package
# if you haven&amp;#39;t installed it, install it using the devtools package:
# devtools::install_github(&amp;quot;dave-clark/ecolFudge&amp;quot;)
library(ecolFudge)

# now we simply run the index function, giving it the name of our OTU table
# the vector of sample column names, and the column with the genus level
# taxonomy assignments
sampleExposure &amp;lt;- ehei(otus, taxonomyCol = &amp;quot;Genus&amp;quot;, sampleCols = samples)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Calculating Ecological Hydrocarbon exposure index for 12 sample(s)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# view the results
head(sampleExposure)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  sample  exposure
## THxT1x1Bac   THxT1x1Bac 0.4419401
## THxT1x2Bac   THxT1x2Bac 0.3618828
## THxT1x3Bac   THxT1x3Bac 0.3674791
## THxT21x1Bac THxT21x1Bac 0.7187549
## THxT21x2Bac THxT21x2Bac 0.3846276
## THxT21x3Bac THxT21x3Bac 0.5097866&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(sampleExposure$exposure)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.3619  0.3952  0.4759  0.4904  0.5648  0.7188&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(sampleExposure$exposure, xlab = &amp;quot;Ecological hydrocarbon exposure index&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://Dave-Clark.github.io/post/2019-03-06-calculating-the-ecological-hydrocarbon-exposure-index-in-r_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Hopefully this will be useful to any oil microbiologists out there, particularly those who are forging their own bioinformatics pipelines. As always, feel free to contact me if you have any questions/comments.&lt;/p&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-lozada_2014&#34;&gt;
&lt;p&gt;Lozada, M., Marcos, M.S., Commendatore, M.G., Gil, M.N. &amp;amp; Dionisi, H.M. (2014) The Bacterial Community Structure of Hydrocarbon-Polluted Marine Environments as the Basis for the Definition of an Ecological Index of Hydrocarbon Exposure. &lt;em&gt;Microbes and Environments&lt;/em&gt;, ME14028.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A custom R function for transposing data.tables</title>
      <link>https://Dave-Clark.github.io/post/a-custom-r-function-for-transposing-data-tables/</link>
      <pubDate>Thu, 28 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Dave-Clark.github.io/post/a-custom-r-function-for-transposing-data-tables/</guid>
      <description>


&lt;p&gt;The &lt;code&gt;data.table&lt;/code&gt; package has become my favourite &lt;code&gt;R&lt;/code&gt; package for all things data handling. Unlike the “tidyverse” suite of packages, the syntax is more akin to base &lt;code&gt;data.frame&lt;/code&gt; syntax, meaning I was able to pick it up quite quickly. It is also incredibly quick, and the parallel data import/export functions (&lt;code&gt;fread&lt;/code&gt; &amp;amp; &lt;code&gt;fwrite&lt;/code&gt;) are a real gift for working with larger data tables, like OTU tables, which can contain several hundred columns, and many thousands of rows. The only thing I found &lt;code&gt;data.table&lt;/code&gt; lacked was a function to transpose data in a convenient way.&lt;/p&gt;
&lt;p&gt;Let me demonstrate what I mean with some examples. Let’s load a small toy dataset that is topologically similar to an OTU table (e.g. samples as cols, species abundances as rows).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load data.table and vegan packages
library(data.table)
library(vegan)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: permute&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: lattice&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## This is vegan 2.5-4&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load the Barro Colorado Island tree dataset
data(BCI)

# coerce to a data.table
# keep the rownames, as this we&amp;#39;ll use this as a &amp;#39;sample&amp;#39; column
bci &amp;lt;- as.data.table(BCI, keep.rownames = T)

# make more realistic sample names and delete old col
bci[, &amp;quot;:=&amp;quot;(sampleName = paste0(&amp;quot;sample_&amp;quot;, rn), rn = NULL)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the data represent something comparable to the OTU tables I am used to working with. Species are columns, whilst each row represents a sample. However, it is common to want to work with the data in the opposite format, with samples as columns and species as rows. Intuitively, one would normally transpose the data using the &lt;code&gt;t&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;transBci &amp;lt;- t(bci)

str(transBci)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  chr [1:226, 1:50] &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot; 0&amp;quot; &amp;quot;0&amp;quot; &amp;quot; 2&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;25&amp;quot; &amp;quot;0&amp;quot; ...
##  - attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
##   ..$ : chr [1:226] &amp;quot;Abarema.macradenia&amp;quot; &amp;quot;Vachellia.melanoceras&amp;quot; &amp;quot;Acalypha.diversifolia&amp;quot; &amp;quot;Acalypha.macrostachya&amp;quot; ...
##   ..$ : NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, as you can see, this causes problems. Having a sample column present in our data means that all the counts get coerced to character class when transposed. Plus, we’d have to manually set the sample row as our new column names, and then delete it from the data.&lt;/p&gt;
&lt;p&gt;An alternate solution involves using &lt;code&gt;melt&lt;/code&gt; and &lt;code&gt;dcast&lt;/code&gt; functions to transpose the data…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;transBci2 &amp;lt;- dcast(melt(bci, id.vars = &amp;quot;sampleName&amp;quot;), variable ~ sampleName)

str(transBci2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Classes &amp;#39;data.table&amp;#39; and &amp;#39;data.frame&amp;#39;:   225 obs. of  51 variables:
##  $ variable : Factor w/ 225 levels &amp;quot;Abarema.macradenia&amp;quot;,..: 1 2 3 4 5 6 7 8 9 10 ...
##  $ sample_1 : int  0 0 0 0 0 0 2 0 0 0 ...
##  $ sample_10: int  1 0 0 0 0 1 2 0 0 0 ...
##  $ sample_11: int  0 0 0 0 0 0 10 0 0 0 ...
##  $ sample_12: int  0 0 0 0 1 1 3 0 0 2 ...
##  $ sample_13: int  0 0 0 0 1 1 1 0 1 1 ...
##  $ sample_14: int  0 0 0 0 0 0 4 0 0 0 ...
##  $ sample_15: int  0 0 0 0 2 0 2 0 0 0 ...
##  $ sample_16: int  0 0 0 0 2 0 2 0 0 3 ...
##  $ sample_17: int  0 0 0 0 0 1 2 0 0 2 ...
##  $ sample_18: int  0 0 0 0 1 1 0 0 0 0 ...
##  $ sample_19: int  0 0 0 0 0 1 1 0 0 1 ...
##  $ sample_2 : int  0 0 0 0 0 0 1 0 0 0 ...
##  $ sample_20: int  0 0 0 0 0 2 2 0 0 0 ...
##  $ sample_21: int  0 0 0 0 0 1 2 0 0 1 ...
##  $ sample_22: int  0 0 0 0 1 0 4 0 0 4 ...
##  $ sample_23: int  0 0 0 0 0 0 1 0 0 0 ...
##  $ sample_24: int  0 0 0 0 2 1 0 0 0 1 ...
##  $ sample_25: int  0 0 0 0 0 1 2 0 0 0 ...
##  $ sample_26: int  0 0 0 0 0 0 3 0 0 0 ...
##  $ sample_27: int  0 0 0 0 1 4 3 0 0 3 ...
##  $ sample_28: int  0 2 0 1 0 1 2 0 0 0 ...
##  $ sample_29: int  0 0 0 0 1 0 1 0 0 0 ...
##  $ sample_3 : int  0 0 0 0 0 0 2 0 0 0 ...
##  $ sample_30: int  0 0 0 0 14 2 6 0 0 0 ...
##  $ sample_31: int  0 0 0 0 5 0 4 0 0 0 ...
##  $ sample_32: int  0 1 0 0 7 0 6 0 0 0 ...
##  $ sample_33: int  0 0 0 0 3 1 3 0 0 1 ...
##  $ sample_34: int  0 0 1 0 3 0 5 0 0 0 ...
##  $ sample_35: int  0 0 0 0 6 0 8 0 0 0 ...
##  $ sample_36: int  0 0 0 0 1 0 3 0 0 0 ...
##  $ sample_37: int  0 0 0 0 2 0 4 0 0 0 ...
##  $ sample_38: int  0 0 0 0 6 0 2 0 0 1 ...
##  $ sample_39: int  0 0 0 0 9 0 3 0 0 1 ...
##  $ sample_4 : int  0 0 0 0 3 0 18 0 0 0 ...
##  $ sample_40: int  0 0 1 0 7 0 3 0 0 1 ...
##  $ sample_41: int  0 0 0 0 0 1 11 0 0 0 ...
##  $ sample_42: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ sample_43: int  0 0 0 0 0 1 3 0 0 0 ...
##  $ sample_44: int  0 0 0 0 4 0 4 0 0 0 ...
##  $ sample_45: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ sample_46: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ sample_47: int  0 0 0 0 2 0 1 0 0 1 ...
##  $ sample_48: int  0 0 0 0 1 0 3 0 0 1 ...
##  $ sample_49: int  0 0 0 0 0 0 6 0 0 1 ...
##  $ sample_5 : int  0 0 0 0 1 1 3 0 0 1 ...
##  $ sample_50: int  0 0 0 0 1 0 2 0 0 1 ...
##  $ sample_6 : int  0 0 0 0 0 0 2 1 0 0 ...
##  $ sample_7 : int  0 0 0 0 0 1 0 0 0 0 ...
##  $ sample_8 : int  0 0 0 0 0 0 2 0 0 0 ...
##  $ sample_9 : int  0 0 0 0 5 0 2 0 0 0 ...
##  - attr(*, &amp;quot;.internal.selfref&amp;quot;)=&amp;lt;externalptr&amp;gt; 
##  - attr(*, &amp;quot;sorted&amp;quot;)= chr &amp;quot;variable&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s better: the sample names are now in the correct place, we have a column with the species names, and the data have remained in the correct integer class. However, whilst that code may have run quite quickly, much larger datasets can slow it down. I therefore wrote a little function based on the original transpose function to get the same result as above, but quicker!&lt;/p&gt;
&lt;p&gt;The function is called &lt;code&gt;transDT&lt;/code&gt; and can be found in my GitHub hosted package, &lt;code&gt;ecolFudge&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# first install my package from github
library(devtools)

install_github(&amp;quot;dave-clark/ecolFudge&amp;quot;)

# load ecolFudge package
library(ecolFudge)

# view the transDT function
transDT&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## function (dt, transCol, rowID) 
## {
##     newRowNames &amp;lt;- colnames(dt)
##     newColNames &amp;lt;- dt[, transCol, with = F]
##     transposedDt &amp;lt;- transpose(dt[, !colnames(dt) %in% transCol, 
##         with = F])
##     colnames(transposedDt) &amp;lt;- unlist(newColNames)
##     transposedDt[, rowID] &amp;lt;- newRowNames[newRowNames != transCol]
##     return(transposedDt)
## }
## &amp;lt;bytecode: 0xa9ac768&amp;gt;
## &amp;lt;environment: namespace:ecolFudge&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, the function takes three arguments. The first, &lt;code&gt;dt&lt;/code&gt;, is simply the data.table you wish to transpose. The second, &lt;code&gt;transCol&lt;/code&gt;, is the column that you wish to become your new column names. In our example, this would be the &lt;code&gt;sampleName&lt;/code&gt; column. The third argument, &lt;code&gt;rowID&lt;/code&gt;, is simply the name you would like to call the column with your new row identifiers (e.g. the column names in your original data). In this example, our new row identifiers are the names of the species, and so it makes sense to call this column &lt;code&gt;species&lt;/code&gt; or something similar.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;transBci3 &amp;lt;- transDT(bci, transCol=&amp;quot;sampleName&amp;quot;, rowID = &amp;quot;species&amp;quot;)

# note that the species column has been placed as the last column...
str(transBci3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Classes &amp;#39;data.table&amp;#39; and &amp;#39;data.frame&amp;#39;:   225 obs. of  51 variables:
##  $ sample_1 : int  0 0 0 0 0 0 2 0 0 0 ...
##  $ sample_2 : int  0 0 0 0 0 0 1 0 0 0 ...
##  $ sample_3 : int  0 0 0 0 0 0 2 0 0 0 ...
##  $ sample_4 : int  0 0 0 0 3 0 18 0 0 0 ...
##  $ sample_5 : int  0 0 0 0 1 1 3 0 0 1 ...
##  $ sample_6 : int  0 0 0 0 0 0 2 1 0 0 ...
##  $ sample_7 : int  0 0 0 0 0 1 0 0 0 0 ...
##  $ sample_8 : int  0 0 0 0 0 0 2 0 0 0 ...
##  $ sample_9 : int  0 0 0 0 5 0 2 0 0 0 ...
##  $ sample_10: int  1 0 0 0 0 1 2 0 0 0 ...
##  $ sample_11: int  0 0 0 0 0 0 10 0 0 0 ...
##  $ sample_12: int  0 0 0 0 1 1 3 0 0 2 ...
##  $ sample_13: int  0 0 0 0 1 1 1 0 1 1 ...
##  $ sample_14: int  0 0 0 0 0 0 4 0 0 0 ...
##  $ sample_15: int  0 0 0 0 2 0 2 0 0 0 ...
##  $ sample_16: int  0 0 0 0 2 0 2 0 0 3 ...
##  $ sample_17: int  0 0 0 0 0 1 2 0 0 2 ...
##  $ sample_18: int  0 0 0 0 1 1 0 0 0 0 ...
##  $ sample_19: int  0 0 0 0 0 1 1 0 0 1 ...
##  $ sample_20: int  0 0 0 0 0 2 2 0 0 0 ...
##  $ sample_21: int  0 0 0 0 0 1 2 0 0 1 ...
##  $ sample_22: int  0 0 0 0 1 0 4 0 0 4 ...
##  $ sample_23: int  0 0 0 0 0 0 1 0 0 0 ...
##  $ sample_24: int  0 0 0 0 2 1 0 0 0 1 ...
##  $ sample_25: int  0 0 0 0 0 1 2 0 0 0 ...
##  $ sample_26: int  0 0 0 0 0 0 3 0 0 0 ...
##  $ sample_27: int  0 0 0 0 1 4 3 0 0 3 ...
##  $ sample_28: int  0 2 0 1 0 1 2 0 0 0 ...
##  $ sample_29: int  0 0 0 0 1 0 1 0 0 0 ...
##  $ sample_30: int  0 0 0 0 14 2 6 0 0 0 ...
##  $ sample_31: int  0 0 0 0 5 0 4 0 0 0 ...
##  $ sample_32: int  0 1 0 0 7 0 6 0 0 0 ...
##  $ sample_33: int  0 0 0 0 3 1 3 0 0 1 ...
##  $ sample_34: int  0 0 1 0 3 0 5 0 0 0 ...
##  $ sample_35: int  0 0 0 0 6 0 8 0 0 0 ...
##  $ sample_36: int  0 0 0 0 1 0 3 0 0 0 ...
##  $ sample_37: int  0 0 0 0 2 0 4 0 0 0 ...
##  $ sample_38: int  0 0 0 0 6 0 2 0 0 1 ...
##  $ sample_39: int  0 0 0 0 9 0 3 0 0 1 ...
##  $ sample_40: int  0 0 1 0 7 0 3 0 0 1 ...
##  $ sample_41: int  0 0 0 0 0 1 11 0 0 0 ...
##  $ sample_42: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ sample_43: int  0 0 0 0 0 1 3 0 0 0 ...
##  $ sample_44: int  0 0 0 0 4 0 4 0 0 0 ...
##  $ sample_45: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ sample_46: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ sample_47: int  0 0 0 0 2 0 1 0 0 1 ...
##  $ sample_48: int  0 0 0 0 1 0 3 0 0 1 ...
##  $ sample_49: int  0 0 0 0 0 0 6 0 0 1 ...
##  $ sample_50: int  0 0 0 0 1 0 2 0 0 1 ...
##  $ species  : chr  &amp;quot;Abarema.macradenia&amp;quot; &amp;quot;Vachellia.melanoceras&amp;quot; &amp;quot;Acalypha.diversifolia&amp;quot; &amp;quot;Acalypha.macrostachya&amp;quot; ...
##  - attr(*, &amp;quot;.internal.selfref&amp;quot;)=&amp;lt;externalptr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s see whether the &lt;code&gt;transDT&lt;/code&gt; function can be faster than the &lt;code&gt;dcast/melt&lt;/code&gt; method…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(microbenchmark)

speedTest &amp;lt;- microbenchmark(
  transDT(bci, transCol = &amp;quot;sampleName&amp;quot;, rowID = &amp;quot;species&amp;quot;),
  dcast(melt(bci, id.vars = &amp;quot;sampleName&amp;quot;), variable ~ sampleName),
  times = 50)

# rename factor levels to neaten up results plot
speedTest$expr &amp;lt;- factor(speedTest$expr,
  levels = levels(speedTest$expr),
  labels = c(&amp;quot;transDT&amp;quot;, &amp;quot;dcast/melt&amp;quot;))

boxplot(time/1000 ~ expr,
  data = speedTest,
  ylab = expression(paste(&amp;quot;Time (&amp;quot;, mu, &amp;quot;S)&amp;quot;)),
  xlab = &amp;quot;Method&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://Dave-Clark.github.io/post/2019-02-28-a-custom-r-function-for-transposing-data-tables_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There you have it, transDT gives us the same result, but in a fraction of the time compared to the &lt;code&gt;dcast/melt&lt;/code&gt; method, even on a relatively small dataset. I hope this is useful to other &lt;code&gt;R&lt;/code&gt; users other than myself, if you have any questions, do get in touch!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Are drivers of root-associated fungal community structure context specific?</title>
      <link>https://Dave-Clark.github.io/publication/alzahrani_et_al/</link>
      <pubDate>Mon, 28 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Dave-Clark.github.io/publication/alzahrani_et_al/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Further testing of quickRareCurve</title>
      <link>https://Dave-Clark.github.io/post/further-testing-of-quickrarecurve/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Dave-Clark.github.io/post/further-testing-of-quickrarecurve/</guid>
      <description>


&lt;p&gt;After my post yesterday, documenting a faster parallelised version of the &lt;code&gt;rarecurve&lt;/code&gt; function (&lt;code&gt;quickRareCurve&lt;/code&gt;), I realised it’d be good to show a real world example using it on a reasonably large OTU table, to prove that it is indeed quicker than the original function. So, here we go.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Starting with an OTU table in which rows are samples,
# Cols are OTUs/species
dim(otuTable)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]   48 5382&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the first column of my data is the sample names
# so remember [, -1] to not include it
# Lets inspect sample sizes with a simple histogram
hist(rowSums(otuTable[, -1]), xlab = &amp;quot;Sample sizes&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://Dave-Clark.github.io/post/2019-01-01-further-testing-of-quickrarecurve_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this example, our OTU table contains 48 samples and 5381 OTUs, plus a column with the sample names in.&lt;/p&gt;
&lt;p&gt;Now we can use &lt;code&gt;microbenchmark&lt;/code&gt; again to compare the performance of the original &lt;code&gt;rarecurve&lt;/code&gt; function to our faster parallel version, &lt;code&gt;quickRareCurve&lt;/code&gt;. We will then plot the results using &lt;code&gt;ggplot2&lt;/code&gt;. Warning: the code below will take some time to run!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(microbenchmark)

# benchmark the two functions with 3 replicates
testResult &amp;lt;- microbenchmark(
  rarecurve(otuTable[, -1]),
  quickRareCurve(otuTable[, -1]),
  times = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)

# convert from nanoseconds to minutes by dividing
# time by 6e+10
ggplot(testResult, aes(x = expr, y = time/6e+10)) +
  geom_boxplot() +
  labs(x = &amp;quot;Function&amp;quot;, y = &amp;quot;Time (minutes)&amp;quot;) +
  theme(axis.text = element_text(size = 16),
    axis.title = element_text(size = 18))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://Dave-Clark.github.io/post/2019-01-01-further-testing-of-quickrarecurve_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There you go, on a typical OTU table, the &lt;code&gt;quickRareCurve&lt;/code&gt; function is far quicker, reducing the processing time from ~25 minutes to &amp;lt; 5 minutes. The more samples in your OTU table, and the more CPU cores available, the greater the increase in performance.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Making maps with ggplot2 and sf</title>
      <link>https://Dave-Clark.github.io/post/making-maps-with-ggplot2-and-sf/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Dave-Clark.github.io/post/making-maps-with-ggplot2-and-sf/</guid>
      <description>


&lt;p&gt;Recently, the newest version of the popular &lt;code&gt;ggplot2&lt;/code&gt; graphics package was announced, and it has some nifty mapping features that I was keen to try out (read more &lt;a href=&#34;https://www.tidyverse.org/articles/2018/07/ggplot2-3-0-0/&#34;&gt;here&lt;/a&gt;). Mainly, I was interested in the support for &lt;code&gt;sf&lt;/code&gt;, or “simple features”, objects. This class of objects were created as part of a wider &lt;code&gt;R&lt;/code&gt; package designed to make mapping and spatial analyses far easier.&lt;/p&gt;
&lt;p&gt;The latest update of &lt;code&gt;ggplot2&lt;/code&gt; not only makes plotting from &lt;code&gt;sf&lt;/code&gt; objects trivial, but also means that some quite nice map figures can be made with relatively little effort, as you’ll hopefully see below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# first you&amp;#39;ll need to update your version of ggplot to the latest version
# install.packages(&amp;quot;ggplot2&amp;quot;)
library(ggplot2)

# now lets load some other packages that we&amp;#39;ll need to load and manipulate
# spatial data in R
library(mapdata)
library(sf)
library(lwgeom)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Assuming you managed to get those packages installed and loaded ok (if you didn’t, you almost certainly have encountered some dependency issues), we can now start playing with some data. To begin, let’s load a world map to play with.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mapBase &amp;lt;- map(&amp;quot;worldHires&amp;quot;, fill = T, plot = F)

# now we need to coerce it to an &amp;quot;sf&amp;quot; object, and fix any
mapBase &amp;lt;- st_as_sf(mapBase)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# now let&amp;#39;s try cropping it to a region of Europe
cropMap &amp;lt;- st_crop(mapBase, xmin = -15, xmax = 30, ymin = 30, ymax = 60)

# note we get an error message about Self-intersection...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# we can fix this using the lwgeom library...
mapBase &amp;lt;- st_make_valid(mapBase)
cropMap &amp;lt;- st_crop(mapBase, xmin = -15, xmax = 30, ymin = 30, ymax = 60)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in st_is_longlat(x): bounding box has potentially an invalid value
## range for longlat data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## although coordinates are longitude/latitude, st_intersection assumes that they are planar&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: attribute variables are assumed to be spatially constant
## throughout all geometries&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# now it works fine...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have a valid &lt;code&gt;sf&lt;/code&gt; object to plot, we can start using the &lt;code&gt;geom_sf&lt;/code&gt; function to start making some nice maps.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# basic map to start with
ggplot(cropMap) + geom_sf()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://Dave-Clark.github.io/post/2019-01-01-making-maps-with-ggplot2-and-sf_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Cool right? Notice a few neat things? Firstly, &lt;code&gt;ggplot&lt;/code&gt; draws a nice graticule for you, complete with the correct “degrees” symbol and N, S, E, or W to denote the correct hemisphere. Secondly, go ahead and resize the map… What do you notice? Hopefully, you’ll see that the aspect ratio of the map has been fixed so that your map always projects correctly. This is a great feature and saves you some time faffing around trying to manually correct the aspect ratio.&lt;/p&gt;
&lt;p&gt;Having explored some basic features, we can now start to tailor our map and add features to it using other packages, or data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# let&amp;#39;s say we want to highlight the location of Spain, clarify the axes,
# and remove those annoying grid lines
spainMap &amp;lt;- ggplot(cropMap,
    aes(fill = factor(ifelse(cropMap$ID == &amp;quot;Spain&amp;quot;, 1, 2)))) +
  geom_sf() +
  labs(x = &amp;quot;Longitude&amp;quot;, y = &amp;quot;Latitude&amp;quot;, fill = &amp;quot;&amp;quot;) +
  scale_fill_manual(values = c(&amp;quot;darkgrey&amp;quot;, &amp;quot;lightgrey&amp;quot;),
    labels = c(&amp;quot;Spain&amp;quot;, &amp;quot;Not Spain&amp;quot;)) +
  theme_bw() +
  theme(panel.grid = element_line(colour = &amp;quot;transparent&amp;quot;),
    axis.title = element_text(size = 18),
    axis.text = element_text(size = 16),
    legend.text = element_text(size = 14),
    legend.title = element_text(size = 14))

spainMap&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://Dave-Clark.github.io/post/2019-01-01-making-maps-with-ggplot2-and-sf_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Pretty easy so far right? But what if we want to add some points to the map, for example to highlight sampling locations? Well, this is really easy too!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# simulate some random coordinates within the limits of our map
locations &amp;lt;- data.frame(lat = runif(25, 30, 60), long = runif(25, -15, 30))

# now convert this dataframe into an sf dataframe
sfPoints &amp;lt;- st_as_sf(locations, coords = c(&amp;quot;long&amp;quot;, &amp;quot;lat&amp;quot;), crs = 4326)

# and simply add another geom_sf layer to the plot to include the points
pointsMap &amp;lt;- ggplot() +
  geom_sf(data = cropMap,
    aes(fill = factor(ifelse(cropMap$ID == &amp;quot;Spain&amp;quot;, 1, 2)))) +
  geom_sf(data = sfPoints, col = &amp;quot;red&amp;quot;, size = 3) +
  labs(x = &amp;quot;Longitude&amp;quot;, y = &amp;quot;Latitude&amp;quot;, fill = &amp;quot;&amp;quot;) +
  scale_fill_manual(values = c(&amp;quot;darkgrey&amp;quot;, &amp;quot;lightgrey&amp;quot;),
    labels = c(&amp;quot;Spain&amp;quot;, &amp;quot;Not Spain&amp;quot;)) +
  theme_bw() +
  theme(panel.grid = element_line(colour = &amp;quot;transparent&amp;quot;),
    axis.title = element_text(size = 18),
    axis.text = element_text(size = 16),
    legend.text = element_text(size = 14),
    legend.title = element_text(size = 14))

pointsMap&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://Dave-Clark.github.io/post/2019-01-01-making-maps-with-ggplot2-and-sf_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Again, super simple. Now, I’d like to create a smaller map, that just shows Spain and any points inside it, so I can make a nice panel figure with the two maps side by side. Also, don’t forget a scale bar, which we can add using the &lt;code&gt;ggspatial&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggspatial)

# work out which points are in the polygon of interest
spainPoints &amp;lt;- st_join(sfPoints, cropMap[cropMap$ID == &amp;quot;Spain&amp;quot;, ],
  join = st_intersects)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## although coordinates are longitude/latitude, st_intersects assumes that they are planar
## although coordinates are longitude/latitude, st_intersects assumes that they are planar&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spainZoom &amp;lt;- ggplot() +
  geom_sf(data = cropMap[cropMap$ID == &amp;quot;Spain&amp;quot;, ]) +
  annotation_scale(location = &amp;quot;br&amp;quot;, text_cex = 2) +
  geom_sf(data = spainPoints[spainPoints$ID == &amp;quot;Spain&amp;quot;, ], colour = &amp;quot;red&amp;quot;,
    size = 3) +
  theme_void() +
  theme(panel.grid = element_line(colour = &amp;quot;transparent&amp;quot;))

spainZoom&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Scale on map varies by more than 10%, scale bar may be inaccurate&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://Dave-Clark.github.io/post/2019-01-01-making-maps-with-ggplot2-and-sf_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So, now we have our two maps sorted, we can arrange them side by side using the &lt;code&gt;cowplot&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(cowplot)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;cowplot&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:ggplot2&amp;#39;:
## 
##     ggsave&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_grid(spainZoom, pointsMap, labels = &amp;quot;AUTO&amp;quot;, rel_widths = c(0.6, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Scale on map varies by more than 10%, scale bar may be inaccurate&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://Dave-Clark.github.io/post/2019-01-01-making-maps-with-ggplot2-and-sf_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There you have it, combining the &lt;code&gt;sf&lt;/code&gt; and newest &lt;code&gt;ggplot2&lt;/code&gt; packages allows you quickly and easily make some neat looking map figures!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The future of USEARCH: A closed source software in an open source world</title>
      <link>https://Dave-Clark.github.io/post/the-future-of-usearch-a-closed-source-software-in-an-open-source-world/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://Dave-Clark.github.io/post/the-future-of-usearch-a-closed-source-software-in-an-open-source-world/</guid>
      <description>


&lt;p&gt;Let me start off by stating that I have enormous respect for &lt;a href=&#34;http://drive5.com/index.htm&#34;&gt;Rob Edgar&lt;/a&gt; (creator of USEARCH, UPARSE etc.). His contributions to the field of bioinformatics, and indirectly to the fields of molecular and microbial ecology have been huge, you only need to look at his citation rates to see that! So this post is not intended as a criticism of him or his work in any way.&lt;/p&gt;
&lt;p&gt;That said, I’ve recently been thinking about the deluge of new algorithms for picking Operational Taxonomic Units (OTUs) from molecular sequence datasets and wondering where, and how, USEARCH, UCLUST &lt;em&gt;et al.&lt;/em&gt; will fit in.&lt;/p&gt;
&lt;p&gt;Rob Edgar only made 32-bit versions of all his software freely available for users, with users having to pay $885 for a 64-bit license. A 32-bit license essentially limits the amount of RAM your computer can access whilst using this software to 4gb. So even if you had a cluster computer at your disposal, you’d still be shackled by the 4gb limit. A few years ago, when 454 pyrosequencing was the predominant sequencing platform, this wasn’t so much of a problem because the datasets were big, but not huge. However, with the rise of Illumina’s platforms such as the HiSeq and MiSeq, molecular datasets are becoming truly enormous, meaning more RAM is needed to work with them.&lt;/p&gt;
&lt;p&gt;Furthermore, the arrival of &lt;a href=&#34;https://github.com/torognes/vsearch&#34;&gt;VSEARCH&lt;/a&gt;, a 64-bit open source work-alike of USEARCH, has meant that you now don’t need to shell out for an expensive 64-bit license to get USEARCH-esque results. This leads me to wonder, what is the future of USEARCH now that there are good, free, and open-source alternatives?&lt;/p&gt;
&lt;p&gt;Looking at the citation rates of Edgar’s papers, there are no obvious signs of it losing popularity… Data on the plot below were downloaded using Web of Science. It will be interesting to see if the popularity of this classic piece of bioinformatics software ever declines.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://Dave-Clark.github.io/post/2019-01-01-the-future-of-usearch-a-closed-source-software-in-an-open-source-world_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Speeding up rarefaction curves for microbial community ecology</title>
      <link>https://Dave-Clark.github.io/post/speeding-up-rarefaction-curves-for-microbial-community-ecology/</link>
      <pubDate>Sat, 15 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>https://Dave-Clark.github.io/post/speeding-up-rarefaction-curves-for-microbial-community-ecology/</guid>
      <description>


&lt;p&gt;When beginning analyses on microbial community data, it is often helpful to compute rarefaction curves. A rarefaction curve tells you about the rate at which new species/OTUs are detected as you increase the number of individuals/sequences sampled. It does this by taking random subsamples from 1, up to the size of your sample, and computing the number of species present in each subsample. Ideally, you want your rarefaction curves to be relatively flat, as this indicates that additional sampling would not likely yield further species.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;vegan&lt;/code&gt; package in &lt;code&gt;R&lt;/code&gt; has a nice function for computing rarefaction curves for species by site abundance tables. However, for microbial datasets this function is often prohibitively slow. This is due to the fact that we often have large numbers of samples, and each sample may contain several thousand sequences, meaning that a large number of random subsamples are taken.&lt;/p&gt;
&lt;p&gt;Part of the problem is that the original function only makes use of a single processing core, meaning that random subsamples are computed serially, and each sample is processed serially. This means we could speed the function up by splitting samples across multiple cores. In short, we are parallelising the function.&lt;/p&gt;
&lt;p&gt;Below is my attempt to modify the original code into a function that can use multiple processor cores to speed up the calculations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# you will need to install the parallel package before hand
# and the vegan package if you dont have it
library(vegan)
quickRareCurve &amp;lt;- function (x, step = 1, sample, xlab = &amp;quot;Sample Size&amp;quot;,
  ylab = &amp;quot;Species&amp;quot;, label = TRUE, col, lty, max.cores = T, nCores = 1, ...)
{
    require(parallel)
    x &amp;lt;- as.matrix(x)
    if (!identical(all.equal(x, round(x)), TRUE))
        stop(&amp;quot;function accepts only integers (counts)&amp;quot;)
    if (missing(col))
        col &amp;lt;- par(&amp;quot;col&amp;quot;)
    if (missing(lty))
        lty &amp;lt;- par(&amp;quot;lty&amp;quot;)
    tot &amp;lt;- rowSums(x) # calculates library sizes
    S &amp;lt;- specnumber(x) # calculates n species for each sample
    if (any(S &amp;lt;= 0)) {
        message(&amp;quot;empty rows removed&amp;quot;)
        x &amp;lt;- x[S &amp;gt; 0, , drop = FALSE]
        tot &amp;lt;- tot[S &amp;gt; 0]
        S &amp;lt;- S[S &amp;gt; 0]
    } # removes any empty rows
    nr &amp;lt;- nrow(x) # number of samples
    col &amp;lt;- rep(col, length.out = nr)
    lty &amp;lt;- rep(lty, length.out = nr)
    # parallel mclapply
    # set number of cores
    mc &amp;lt;- getOption(&amp;quot;mc.cores&amp;quot;, ifelse(max.cores, detectCores(), nCores))
    message(paste(&amp;quot;Using &amp;quot;, mc, &amp;quot; cores&amp;quot;))
    out &amp;lt;- mclapply(seq_len(nr), mc.cores = mc, function(i) {
        n &amp;lt;- seq(1, tot[i], by = step)
        if (n[length(n)] != tot[i])
            n &amp;lt;- c(n, tot[i])
        drop(rarefy(x[i, ], n))
    })
    Nmax &amp;lt;- sapply(out, function(x) max(attr(x, &amp;quot;Subsample&amp;quot;)))
    Smax &amp;lt;- sapply(out, max)
     plot(c(1, max(Nmax)), c(1, max(Smax)), xlab = xlab, ylab = ylab,
       type = &amp;quot;n&amp;quot;, ...)
    if (!missing(sample)) {
      abline(v = sample)
      rare &amp;lt;- sapply(out, function(z) approx(x = attr(z, &amp;quot;Subsample&amp;quot;),
         y = z, xout = sample, rule = 1)$y)
      abline(h = rare, lwd = 0.5)
      }
    for (ln in seq_along(out)) {
      N &amp;lt;- attr(out[[ln]], &amp;quot;Subsample&amp;quot;)
      lines(N, out[[ln]], col = col[ln], lty = lty[ln], ...)
      }
    if (label) {
      ordilabel(cbind(tot, S), labels = rownames(x), ...)
      }
    invisible(out)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Most of the code is verbatim to the original, but you’ll notice a few extra arguments to specify, and a few extra lines that determine how many cores the function will use.&lt;/p&gt;
&lt;p&gt;Essentially, if you do not specify a number of cores, the function will default to using all available cores, which will allow the quickest calculation. Otherwise, you can specify &lt;code&gt;max.cores = F&lt;/code&gt;, which will allow you to specify the number of cores using &lt;code&gt;nCores&lt;/code&gt;. This is handy if you need some cores to remain usable whilst running the function.&lt;/p&gt;
&lt;p&gt;Below is a usage example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load some dummy data
data(dune)

quickRareCurve(dune) # will use all cores and print how many cores you have

quickRareCurve(dune, max.cores = F, nCores = 2) # use only 2 cores&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://Dave-Clark.github.io/post/2018-12-15-speeding-up-rarefaction-curves-for-microbial-community-ecology_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can compare the new function’s performance to the original using the &lt;code&gt;microbenchmark&lt;/code&gt; package, as below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(microbenchmark)

microbenchmark(rarecurve(dune), quickRareCurve(dune), times = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://Dave-Clark.github.io/post/2018-12-15-speeding-up-rarefaction-curves-for-microbial-community-ecology_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Unit: milliseconds
##                  expr      min       lq     mean   median       uq
##       rarecurve(dune) 79.70178 80.94599 83.47126 83.08141 86.01284
##  quickRareCurve(dune) 49.67263 89.20412 89.21991 92.63656 96.96782
##        max neval
##   89.48703    10
##  100.11262    10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, for this small dataset, the original function is actually quicker. This is because it takes some time to distribute tasks among the cores. However, I’ve found for a typical OTU table (&amp;gt; 50 samples, ~ 15,000 sequences per sample), &lt;code&gt;quickRareCurve&lt;/code&gt; can be around 3 times faster.&lt;/p&gt;
&lt;p&gt;Feel free to use it as you wish, but I’d be very grateful for any credit if you do use it.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
