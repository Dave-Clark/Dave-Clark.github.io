<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Microbial Ecologist on Microbial Ecologist</title>
    <link>/</link>
    <description>Recent content in Microbial Ecologist on Microbial Ecologist</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-uk</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0100</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Impacts of Long‐Term Elevated Atmospheric CO₂ Concentrations on Communities of Arbuscular Mycorrhizal Fungi</title>
      <link>/publication/macek_et_al/</link>
      <pubDate>Mon, 24 Jun 2019 00:00:00 +0100</pubDate>
      
      <guid>/publication/macek_et_al/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Calculating the Ecological Hydrocarbon Exposure Index in R</title>
      <link>/post/calculating-the-ecological-hydrocarbon-exposure-index-in-r/</link>
      <pubDate>Wed, 06 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/calculating-the-ecological-hydrocarbon-exposure-index-in-r/</guid>
      <description>


&lt;p&gt;Recently, I’ve been collaborating with several colleagues on projects focussing on the microbial ecology of hydrocarbon degradation. One of the aspects we’ve been thinking about, is whether the composition of the microbial community can reflect the level of hydrocarbon exposure. As it turns out, we aren’t the first to consider this, as Mariana Lozada and colleagues came up with an “Ecological Hydrocarbon Exposure Index”, which uses the composition and structure of the microbial community to quantify the level of hydrocarbon exposure in a given environment &lt;span class=&#34;citation&#34;&gt;(Lozada &lt;em&gt;et al.&lt;/em&gt;, &lt;a href=&#34;#ref-lozada_2014&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt;. To their credit, the authors provided &lt;code&gt;R&lt;/code&gt; code to calculate their index in a supplementary PDF on the &lt;a href=&#34;https://www.jstage.jst.go.jp/article/jsme2/advpub/0/advpub_ME14028/_article/-char/ja/&#34;&gt;journal article page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, the original code was designed to run using a &lt;code&gt;Mothur&lt;/code&gt; formatted taxonomy file, which is not useful for users who might prefer other taxonomy assignment pipelines. Therefore, I decided to rewrite their script into a more useful function format, that can be applied to an OTU table with any taxonomy format. The function is hosted in my GitHub based package &lt;code&gt;ecolFudge&lt;/code&gt;, and an example of how to use this function is presented below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# first lets import a small OTU table as an example

otus &amp;lt;- read.csv(&amp;quot;example_OTU_table.csv&amp;quot;)

# look at structure of OTU table
# note that our genus level taxonomy assignment is in &amp;quot;Genus&amp;quot; column
# and our samples all start with &amp;quot;THx&amp;quot;
str(otus)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;#39;data.frame&amp;#39;:    3488 obs. of  26 variables:
##  $ OTU              : Factor w/ 3488 levels &amp;quot;OTU_1&amp;quot;,&amp;quot;OTU_10&amp;quot;,..: 1 1103 2205 2823 2934 3045 3156 3267 3378 2 ...
##  $ THxT1x1Bac       : int  3 3429 4653 57 1 190 555 58 0 1 ...
##  $ THxT1x2Bac       : int  1 24761 8476 144 11 1149 10748 628 3 19 ...
##  $ THxT1x3Bac       : int  0 1301 511 13 0 72 101 61 1 0 ...
##  $ THxT21x1Bac      : int  9955 5 52 332 871 127 3 33 207 52 ...
##  $ THxT21x2Bac      : int  1148 51 273 1956 2237 222 18 23 144 94 ...
##  $ THxT21x3Bac      : int  4155 17 328 2427 6246 886 34 198 1632 113 ...
##  $ THxT3x1Bac       : int  3 7232 972 392 4 397 496 3002 0 19 ...
##  $ THxT3x2Bac       : int  1 10328 939 798 22 177 962 3440 1 28 ...
##  $ THxT3x3Bac       : int  1 12733 3892 1559 15 517 1960 2511 0 61 ...
##  $ THxT7x1Bac       : int  31 1583 913 3835 2841 689 253 1392 1789 726 ...
##  $ THxT7x2Bac       : int  4774 1422 614 2181 3066 352 461 1351 15 2378 ...
##  $ THxT7x3Bac       : int  2651 2689 592 6684 1862 425 210 4090 617 1226 ...
##  $ sum              : int  572326 423114 308751 239756 239329 159273 158127 152364 137010 118647 ...
##  $ Domain           : Factor w/ 1 level &amp;quot;Bacteria&amp;quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ Confidence_domain: num  1 1 1 1 1 1 1 1 1 1 ...
##  $ Phylum           : Factor w/ 37 levels &amp;quot;Acetothermia&amp;quot;,..: 32 32 32 32 32 32 32 32 32 32 ...
##  $ Confidence_phylum: num  1 1 1 1 1 1 1 1 1 1 ...
##  $ Class            : Factor w/ 76 levels &amp;quot;Acetothermia_genera_incertae_sedis&amp;quot;,..: 45 45 45 45 45 45 39 45 15 15 ...
##  $ Confidence_class : num  1 1 1 1 1 1 1 1 1 1 ...
##  $ Order            : Factor w/ 142 levels &amp;quot;Acanthopleuribacterales&amp;quot;,..: 96 96 9 9 136 107 24 96 110 110 ...
##  $ Confidence_order : num  1 1 1 0.99 1 1 1 1 1 1 ...
##  $ Family           : Factor w/ 283 levels &amp;quot;Acanthopleuribacteraceae&amp;quot;,..: 11 183 212 64 202 214 40 183 222 222 ...
##  $ Confidence_family: num  1 1 0.98 0.94 1 1 1 1 1 1 ...
##  $ Genus            : Factor w/ 878 levels &amp;quot;Acanthopleuribacter&amp;quot;,..: 32 556 644 170 189 653 79 807 656 654 ...
##  $ Confidence_genus : num  1 1 0.98 0.94 1 0.97 1 1 0.48 0.99 ...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# we first need to create a vector of sample names that we wish to calculate
# the index for
samples &amp;lt;- grep(&amp;quot;THx&amp;quot;, colnames(otus), value = T)

# now we can load the ecolFudge package
# if you haven&amp;#39;t installed it, install it using the devtools package:
# devtools::install_github(&amp;quot;dave-clark/ecolFudge&amp;quot;)
library(ecolFudge)

# now we simply run the index function, giving it the name of our OTU table
# the vector of sample column names, and the column with the genus level
# taxonomy assignments
sampleExposure &amp;lt;- ehei(otus, taxonomyCol = &amp;quot;Genus&amp;quot;, sampleCols = samples)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Calculating Ecological Hydrocarbon exposure index for 12 sample(s)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# view the results
head(sampleExposure)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                  sample  exposure
## THxT1x1Bac   THxT1x1Bac 0.4419401
## THxT1x2Bac   THxT1x2Bac 0.3618828
## THxT1x3Bac   THxT1x3Bac 0.3674791
## THxT21x1Bac THxT21x1Bac 0.7187549
## THxT21x2Bac THxT21x2Bac 0.3846276
## THxT21x3Bac THxT21x3Bac 0.5097866&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;summary(sampleExposure$exposure)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.3619  0.3952  0.4759  0.4904  0.5648  0.7188&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;hist(sampleExposure$exposure, xlab = &amp;quot;Ecological hydrocarbon exposure index&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-03-06-calculating-the-ecological-hydrocarbon-exposure-index-in-r_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Hopefully this will be useful to any oil microbiologists out there, particularly those who are forging their own bioinformatics pipelines. As always, feel free to contact me if you have any questions/comments.&lt;/p&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-lozada_2014&#34;&gt;
&lt;p&gt;Lozada, M., Marcos, M.S., Commendatore, M.G., Gil, M.N. &amp;amp; Dionisi, H.M. (2014) The Bacterial Community Structure of Hydrocarbon-Polluted Marine Environments as the Basis for the Definition of an Ecological Index of Hydrocarbon Exposure. &lt;em&gt;Microbes and Environments&lt;/em&gt;, ME14028.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A custom R function for transposing data.tables</title>
      <link>/post/a-custom-r-function-for-transposing-data-tables/</link>
      <pubDate>Thu, 28 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/a-custom-r-function-for-transposing-data-tables/</guid>
      <description>


&lt;p&gt;The &lt;code&gt;data.table&lt;/code&gt; package has become my favourite &lt;code&gt;R&lt;/code&gt; package for all things data handling. Unlike the “tidyverse” suite of packages, the syntax is more akin to base &lt;code&gt;data.frame&lt;/code&gt; syntax, meaning I was able to pick it up quite quickly. It is also incredibly quick, and the parallel data import/export functions (&lt;code&gt;fread&lt;/code&gt; &amp;amp; &lt;code&gt;fwrite&lt;/code&gt;) are a real gift for working with larger data tables, like OTU tables, which can contain several hundred columns, and many thousands of rows. The only thing I found &lt;code&gt;data.table&lt;/code&gt; lacked was a function to transpose data in a convenient way.&lt;/p&gt;
&lt;p&gt;Let me demonstrate what I mean with some examples. Let’s load a small toy dataset that is topologically similar to an OTU table (e.g. samples as cols, species abundances as rows).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load data.table and vegan packages
library(data.table)
library(vegan)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: permute&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: lattice&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## This is vegan 2.5-6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load the Barro Colorado Island tree dataset
data(BCI)

# coerce to a data.table
# keep the rownames, as this we&amp;#39;ll use this as a &amp;#39;sample&amp;#39; column
bci &amp;lt;- as.data.table(BCI, keep.rownames = T)

# make more realistic sample names and delete old col
bci[, &amp;quot;:=&amp;quot;(sampleName = paste0(&amp;quot;sample_&amp;quot;, rn), rn = NULL)]&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now the data represent something comparable to the OTU tables I am used to working with. Species are columns, whilst each row represents a sample. However, it is common to want to work with the data in the opposite format, with samples as columns and species as rows. Intuitively, one would normally transpose the data using the &lt;code&gt;t&lt;/code&gt; function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;transBci &amp;lt;- t(bci)

str(transBci)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  chr [1:226, 1:50] &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot; 0&amp;quot; &amp;quot;0&amp;quot; &amp;quot; 2&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;0&amp;quot; &amp;quot;25&amp;quot; &amp;quot;0&amp;quot; ...
##  - attr(*, &amp;quot;dimnames&amp;quot;)=List of 2
##   ..$ : chr [1:226] &amp;quot;Abarema.macradenia&amp;quot; &amp;quot;Vachellia.melanoceras&amp;quot; &amp;quot;Acalypha.diversifolia&amp;quot; &amp;quot;Acalypha.macrostachya&amp;quot; ...
##   ..$ : NULL&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;However, as you can see, this causes problems. Having a sample column present in our data means that all the counts get coerced to character class when transposed. Plus, we’d have to manually set the sample row as our new column names, and then delete it from the data.&lt;/p&gt;
&lt;p&gt;An alternate solution involves using &lt;code&gt;melt&lt;/code&gt; and &lt;code&gt;dcast&lt;/code&gt; functions to transpose the data…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;transBci2 &amp;lt;- dcast(melt(bci, id.vars = &amp;quot;sampleName&amp;quot;), variable ~ sampleName)

str(transBci2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Classes &amp;#39;data.table&amp;#39; and &amp;#39;data.frame&amp;#39;:   225 obs. of  51 variables:
##  $ variable : Factor w/ 225 levels &amp;quot;Abarema.macradenia&amp;quot;,..: 1 2 3 4 5 6 7 8 9 10 ...
##  $ sample_1 : int  0 0 0 0 0 0 2 0 0 0 ...
##  $ sample_10: int  1 0 0 0 0 1 2 0 0 0 ...
##  $ sample_11: int  0 0 0 0 0 0 10 0 0 0 ...
##  $ sample_12: int  0 0 0 0 1 1 3 0 0 2 ...
##  $ sample_13: int  0 0 0 0 1 1 1 0 1 1 ...
##  $ sample_14: int  0 0 0 0 0 0 4 0 0 0 ...
##  $ sample_15: int  0 0 0 0 2 0 2 0 0 0 ...
##  $ sample_16: int  0 0 0 0 2 0 2 0 0 3 ...
##  $ sample_17: int  0 0 0 0 0 1 2 0 0 2 ...
##  $ sample_18: int  0 0 0 0 1 1 0 0 0 0 ...
##  $ sample_19: int  0 0 0 0 0 1 1 0 0 1 ...
##  $ sample_2 : int  0 0 0 0 0 0 1 0 0 0 ...
##  $ sample_20: int  0 0 0 0 0 2 2 0 0 0 ...
##  $ sample_21: int  0 0 0 0 0 1 2 0 0 1 ...
##  $ sample_22: int  0 0 0 0 1 0 4 0 0 4 ...
##  $ sample_23: int  0 0 0 0 0 0 1 0 0 0 ...
##  $ sample_24: int  0 0 0 0 2 1 0 0 0 1 ...
##  $ sample_25: int  0 0 0 0 0 1 2 0 0 0 ...
##  $ sample_26: int  0 0 0 0 0 0 3 0 0 0 ...
##  $ sample_27: int  0 0 0 0 1 4 3 0 0 3 ...
##  $ sample_28: int  0 2 0 1 0 1 2 0 0 0 ...
##  $ sample_29: int  0 0 0 0 1 0 1 0 0 0 ...
##  $ sample_3 : int  0 0 0 0 0 0 2 0 0 0 ...
##  $ sample_30: int  0 0 0 0 14 2 6 0 0 0 ...
##  $ sample_31: int  0 0 0 0 5 0 4 0 0 0 ...
##  $ sample_32: int  0 1 0 0 7 0 6 0 0 0 ...
##  $ sample_33: int  0 0 0 0 3 1 3 0 0 1 ...
##  $ sample_34: int  0 0 1 0 3 0 5 0 0 0 ...
##  $ sample_35: int  0 0 0 0 6 0 8 0 0 0 ...
##  $ sample_36: int  0 0 0 0 1 0 3 0 0 0 ...
##  $ sample_37: int  0 0 0 0 2 0 4 0 0 0 ...
##  $ sample_38: int  0 0 0 0 6 0 2 0 0 1 ...
##  $ sample_39: int  0 0 0 0 9 0 3 0 0 1 ...
##  $ sample_4 : int  0 0 0 0 3 0 18 0 0 0 ...
##  $ sample_40: int  0 0 1 0 7 0 3 0 0 1 ...
##  $ sample_41: int  0 0 0 0 0 1 11 0 0 0 ...
##  $ sample_42: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ sample_43: int  0 0 0 0 0 1 3 0 0 0 ...
##  $ sample_44: int  0 0 0 0 4 0 4 0 0 0 ...
##  $ sample_45: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ sample_46: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ sample_47: int  0 0 0 0 2 0 1 0 0 1 ...
##  $ sample_48: int  0 0 0 0 1 0 3 0 0 1 ...
##  $ sample_49: int  0 0 0 0 0 0 6 0 0 1 ...
##  $ sample_5 : int  0 0 0 0 1 1 3 0 0 1 ...
##  $ sample_50: int  0 0 0 0 1 0 2 0 0 1 ...
##  $ sample_6 : int  0 0 0 0 0 0 2 1 0 0 ...
##  $ sample_7 : int  0 0 0 0 0 1 0 0 0 0 ...
##  $ sample_8 : int  0 0 0 0 0 0 2 0 0 0 ...
##  $ sample_9 : int  0 0 0 0 5 0 2 0 0 0 ...
##  - attr(*, &amp;quot;.internal.selfref&amp;quot;)=&amp;lt;externalptr&amp;gt; 
##  - attr(*, &amp;quot;sorted&amp;quot;)= chr &amp;quot;variable&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;That’s better: the sample names are now in the correct place, we have a column with the species names, and the data have remained in the correct integer class. However, whilst that code may have run quite quickly, much larger datasets can slow it down. I therefore wrote a little function based on the original transpose function to get the same result as above, but quicker!&lt;/p&gt;
&lt;p&gt;The function is called &lt;code&gt;transDT&lt;/code&gt; and can be found in my GitHub hosted package, &lt;code&gt;ecolFudge&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# first install my package from github
library(devtools)

install_github(&amp;quot;dave-clark/ecolFudge&amp;quot;)

# load ecolFudge package
library(ecolFudge)

# view the transDT function
transDT&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## function (dt, transCol, rowID) 
## {
##     newRowNames &amp;lt;- colnames(dt)
##     newColNames &amp;lt;- dt[, transCol, with = F]
##     transposedDt &amp;lt;- transpose(dt[, !colnames(dt) %in% transCol, 
##         with = F])
##     colnames(transposedDt) &amp;lt;- unlist(newColNames)
##     transposedDt[, rowID] &amp;lt;- newRowNames[newRowNames != transCol]
##     return(transposedDt)
## }
## &amp;lt;bytecode: 0x563c9a12bc50&amp;gt;
## &amp;lt;environment: namespace:ecolFudge&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, the function takes three arguments. The first, &lt;code&gt;dt&lt;/code&gt;, is simply the data.table you wish to transpose. The second, &lt;code&gt;transCol&lt;/code&gt;, is the column that you wish to become your new column names. In our example, this would be the &lt;code&gt;sampleName&lt;/code&gt; column. The third argument, &lt;code&gt;rowID&lt;/code&gt;, is simply the name you would like to call the column with your new row identifiers (e.g. the column names in your original data). In this example, our new row identifiers are the names of the species, and so it makes sense to call this column &lt;code&gt;species&lt;/code&gt; or something similar.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;transBci3 &amp;lt;- transDT(bci, transCol=&amp;quot;sampleName&amp;quot;, rowID = &amp;quot;species&amp;quot;)

# note that the species column has been placed as the last column...
str(transBci3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Classes &amp;#39;data.table&amp;#39; and &amp;#39;data.frame&amp;#39;:   225 obs. of  51 variables:
##  $ sample_1 : int  0 0 0 0 0 0 2 0 0 0 ...
##  $ sample_2 : int  0 0 0 0 0 0 1 0 0 0 ...
##  $ sample_3 : int  0 0 0 0 0 0 2 0 0 0 ...
##  $ sample_4 : int  0 0 0 0 3 0 18 0 0 0 ...
##  $ sample_5 : int  0 0 0 0 1 1 3 0 0 1 ...
##  $ sample_6 : int  0 0 0 0 0 0 2 1 0 0 ...
##  $ sample_7 : int  0 0 0 0 0 1 0 0 0 0 ...
##  $ sample_8 : int  0 0 0 0 0 0 2 0 0 0 ...
##  $ sample_9 : int  0 0 0 0 5 0 2 0 0 0 ...
##  $ sample_10: int  1 0 0 0 0 1 2 0 0 0 ...
##  $ sample_11: int  0 0 0 0 0 0 10 0 0 0 ...
##  $ sample_12: int  0 0 0 0 1 1 3 0 0 2 ...
##  $ sample_13: int  0 0 0 0 1 1 1 0 1 1 ...
##  $ sample_14: int  0 0 0 0 0 0 4 0 0 0 ...
##  $ sample_15: int  0 0 0 0 2 0 2 0 0 0 ...
##  $ sample_16: int  0 0 0 0 2 0 2 0 0 3 ...
##  $ sample_17: int  0 0 0 0 0 1 2 0 0 2 ...
##  $ sample_18: int  0 0 0 0 1 1 0 0 0 0 ...
##  $ sample_19: int  0 0 0 0 0 1 1 0 0 1 ...
##  $ sample_20: int  0 0 0 0 0 2 2 0 0 0 ...
##  $ sample_21: int  0 0 0 0 0 1 2 0 0 1 ...
##  $ sample_22: int  0 0 0 0 1 0 4 0 0 4 ...
##  $ sample_23: int  0 0 0 0 0 0 1 0 0 0 ...
##  $ sample_24: int  0 0 0 0 2 1 0 0 0 1 ...
##  $ sample_25: int  0 0 0 0 0 1 2 0 0 0 ...
##  $ sample_26: int  0 0 0 0 0 0 3 0 0 0 ...
##  $ sample_27: int  0 0 0 0 1 4 3 0 0 3 ...
##  $ sample_28: int  0 2 0 1 0 1 2 0 0 0 ...
##  $ sample_29: int  0 0 0 0 1 0 1 0 0 0 ...
##  $ sample_30: int  0 0 0 0 14 2 6 0 0 0 ...
##  $ sample_31: int  0 0 0 0 5 0 4 0 0 0 ...
##  $ sample_32: int  0 1 0 0 7 0 6 0 0 0 ...
##  $ sample_33: int  0 0 0 0 3 1 3 0 0 1 ...
##  $ sample_34: int  0 0 1 0 3 0 5 0 0 0 ...
##  $ sample_35: int  0 0 0 0 6 0 8 0 0 0 ...
##  $ sample_36: int  0 0 0 0 1 0 3 0 0 0 ...
##  $ sample_37: int  0 0 0 0 2 0 4 0 0 0 ...
##  $ sample_38: int  0 0 0 0 6 0 2 0 0 1 ...
##  $ sample_39: int  0 0 0 0 9 0 3 0 0 1 ...
##  $ sample_40: int  0 0 1 0 7 0 3 0 0 1 ...
##  $ sample_41: int  0 0 0 0 0 1 11 0 0 0 ...
##  $ sample_42: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ sample_43: int  0 0 0 0 0 1 3 0 0 0 ...
##  $ sample_44: int  0 0 0 0 4 0 4 0 0 0 ...
##  $ sample_45: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ sample_46: int  0 0 0 0 0 0 0 0 0 0 ...
##  $ sample_47: int  0 0 0 0 2 0 1 0 0 1 ...
##  $ sample_48: int  0 0 0 0 1 0 3 0 0 1 ...
##  $ sample_49: int  0 0 0 0 0 0 6 0 0 1 ...
##  $ sample_50: int  0 0 0 0 1 0 2 0 0 1 ...
##  $ species  : chr  &amp;quot;Abarema.macradenia&amp;quot; &amp;quot;Vachellia.melanoceras&amp;quot; &amp;quot;Acalypha.diversifolia&amp;quot; &amp;quot;Acalypha.macrostachya&amp;quot; ...
##  - attr(*, &amp;quot;.internal.selfref&amp;quot;)=&amp;lt;externalptr&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now let’s see whether the &lt;code&gt;transDT&lt;/code&gt; function can be faster than the &lt;code&gt;dcast/melt&lt;/code&gt; method…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(microbenchmark)

speedTest &amp;lt;- microbenchmark(
  transDT(bci, transCol = &amp;quot;sampleName&amp;quot;, rowID = &amp;quot;species&amp;quot;),
  dcast(melt(bci, id.vars = &amp;quot;sampleName&amp;quot;), variable ~ sampleName),
  times = 50)

# rename factor levels to neaten up results plot
speedTest$expr &amp;lt;- factor(speedTest$expr,
  levels = levels(speedTest$expr),
  labels = c(&amp;quot;transDT&amp;quot;, &amp;quot;dcast/melt&amp;quot;))

boxplot(time/1000 ~ expr,
  data = speedTest,
  ylab = expression(paste(&amp;quot;Time (&amp;quot;, mu, &amp;quot;S)&amp;quot;)),
  xlab = &amp;quot;Method&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-02-28-a-custom-r-function-for-transposing-data-tables_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There you have it, transDT gives us the same result, but in a fraction of the time compared to the &lt;code&gt;dcast/melt&lt;/code&gt; method, even on a relatively small dataset. I hope this is useful to other &lt;code&gt;R&lt;/code&gt; users other than myself, if you have any questions, do get in touch!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Are drivers of root-associated fungal community structure context specific?</title>
      <link>/publication/alzahrani_et_al/</link>
      <pubDate>Mon, 28 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/publication/alzahrani_et_al/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Further testing of quickRareCurve</title>
      <link>/post/further-testing-of-quickrarecurve/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/further-testing-of-quickrarecurve/</guid>
      <description>


&lt;p&gt;After my post yesterday, documenting a faster parallelised version of the &lt;code&gt;rarecurve&lt;/code&gt; function (&lt;code&gt;quickRareCurve&lt;/code&gt;), I realised it’d be good to show a real world example using it on a reasonably large OTU table, to prove that it is indeed quicker than the original function. So, here we go.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Starting with an OTU table in which rows are samples,
# Cols are OTUs/species
dim(otuTable)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]   48 5382&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# the first column of my data is the sample names
# so remember [, -1] to not include it
# Lets inspect sample sizes with a simple histogram
hist(rowSums(otuTable[, -1]), xlab = &amp;quot;Sample sizes&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-01-further-testing-of-quickrarecurve_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In this example, our OTU table contains 48 samples and 5381 OTUs, plus a column with the sample names in.&lt;/p&gt;
&lt;p&gt;Now we can use &lt;code&gt;microbenchmark&lt;/code&gt; again to compare the performance of the original &lt;code&gt;rarecurve&lt;/code&gt; function to our faster parallel version, &lt;code&gt;quickRareCurve&lt;/code&gt;. We will then plot the results using &lt;code&gt;ggplot2&lt;/code&gt;. Warning: the code below will take some time to run!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(microbenchmark)

# benchmark the two functions with 3 replicates
testResult &amp;lt;- microbenchmark(
  rarecurve(otuTable[, -1]),
  quickRareCurve(otuTable[, -1]),
  times = 3)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)

# convert from nanoseconds to minutes by dividing
# time by 6e+10
ggplot(testResult, aes(x = expr, y = time/6e+10)) +
  geom_boxplot() +
  labs(x = &amp;quot;Function&amp;quot;, y = &amp;quot;Time (minutes)&amp;quot;) +
  theme(axis.text = element_text(size = 16),
    axis.title = element_text(size = 18))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-01-further-testing-of-quickrarecurve_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There you go, on a typical OTU table, the &lt;code&gt;quickRareCurve&lt;/code&gt; function is far quicker, reducing the processing time from ~25 minutes to &amp;lt; 5 minutes. The more samples in your OTU table, and the more CPU cores available, the greater the increase in performance.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Making maps with ggplot2 and sf</title>
      <link>/post/making-maps-with-ggplot2-and-sf/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/making-maps-with-ggplot2-and-sf/</guid>
      <description>


&lt;p&gt;Recently, the newest version of the popular &lt;code&gt;ggplot2&lt;/code&gt; graphics package was announced, and it has some nifty mapping features that I was keen to try out (read more &lt;a href=&#34;https://www.tidyverse.org/articles/2018/07/ggplot2-3-0-0/&#34;&gt;here&lt;/a&gt;). Mainly, I was interested in the support for &lt;code&gt;sf&lt;/code&gt;, or “simple features”, objects. This class of objects were created as part of a wider &lt;code&gt;R&lt;/code&gt; package designed to make mapping and spatial analyses far easier.&lt;/p&gt;
&lt;p&gt;The latest update of &lt;code&gt;ggplot2&lt;/code&gt; not only makes plotting from &lt;code&gt;sf&lt;/code&gt; objects trivial, but also means that some quite nice map figures can be made with relatively little effort, as you’ll hopefully see below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# first you&amp;#39;ll need to update your version of ggplot to the latest version
# install.packages(&amp;quot;ggplot2&amp;quot;)
library(ggplot2)

# now lets load some other packages that we&amp;#39;ll need to load and manipulate
# spatial data in R
library(mapdata)
library(sf)
library(lwgeom)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Assuming you managed to get those packages installed and loaded ok (if you didn’t, you almost certainly have encountered some dependency issues), we can now start playing with some data. To begin, let’s load a world map to play with.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mapBase &amp;lt;- map(&amp;quot;worldHires&amp;quot;, fill = T, plot = F)

# now we need to coerce it to an &amp;quot;sf&amp;quot; object, and fix any
mapBase &amp;lt;- st_as_sf(mapBase)&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# now let&amp;#39;s try cropping it to a region of Europe
cropMap &amp;lt;- st_crop(mapBase, xmin = -15, xmax = 30, ymin = 30, ymax = 60)

# note we get an error message about Self-intersection...&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# we can fix this using the lwgeom library...
mapBase &amp;lt;- st_make_valid(mapBase)
cropMap &amp;lt;- st_crop(mapBase, xmin = -15, xmax = 30, ymin = 30, ymax = 60)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in st_is_longlat(x): bounding box has potentially an invalid value
## range for longlat data&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## although coordinates are longitude/latitude, st_intersection assumes that they are planar&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning: attribute variables are assumed to be spatially constant
## throughout all geometries&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# now it works fine...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have a valid &lt;code&gt;sf&lt;/code&gt; object to plot, we can start using the &lt;code&gt;geom_sf&lt;/code&gt; function to start making some nice maps.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# basic map to start with
ggplot(cropMap) + geom_sf()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-01-making-maps-with-ggplot2-and-sf_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Cool right? Notice a few neat things? Firstly, &lt;code&gt;ggplot&lt;/code&gt; draws a nice graticule for you, complete with the correct “degrees” symbol and N, S, E, or W to denote the correct hemisphere. Secondly, go ahead and resize the map… What do you notice? Hopefully, you’ll see that the aspect ratio of the map has been fixed so that your map always projects correctly. This is a great feature and saves you some time faffing around trying to manually correct the aspect ratio.&lt;/p&gt;
&lt;p&gt;Having explored some basic features, we can now start to tailor our map and add features to it using other packages, or data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# let&amp;#39;s say we want to highlight the location of Spain, clarify the axes,
# and remove those annoying grid lines
spainMap &amp;lt;- ggplot(cropMap,
    aes(fill = factor(ifelse(cropMap$ID == &amp;quot;Spain&amp;quot;, 1, 2)))) +
  geom_sf() +
  labs(x = &amp;quot;Longitude&amp;quot;, y = &amp;quot;Latitude&amp;quot;, fill = &amp;quot;&amp;quot;) +
  scale_fill_manual(values = c(&amp;quot;darkgrey&amp;quot;, &amp;quot;lightgrey&amp;quot;),
    labels = c(&amp;quot;Spain&amp;quot;, &amp;quot;Not Spain&amp;quot;)) +
  theme_bw() +
  theme(panel.grid = element_line(colour = &amp;quot;transparent&amp;quot;),
    axis.title = element_text(size = 18),
    axis.text = element_text(size = 16),
    legend.text = element_text(size = 14),
    legend.title = element_text(size = 14))

spainMap&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-01-making-maps-with-ggplot2-and-sf_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Pretty easy so far right? But what if we want to add some points to the map, for example to highlight sampling locations? Well, this is really easy too!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# simulate some random coordinates within the limits of our map
locations &amp;lt;- data.frame(lat = runif(25, 30, 60), long = runif(25, -15, 30))

# now convert this dataframe into an sf dataframe
sfPoints &amp;lt;- st_as_sf(locations, coords = c(&amp;quot;long&amp;quot;, &amp;quot;lat&amp;quot;), crs = 4326)

# and simply add another geom_sf layer to the plot to include the points
pointsMap &amp;lt;- ggplot() +
  geom_sf(data = cropMap,
    aes(fill = factor(ifelse(cropMap$ID == &amp;quot;Spain&amp;quot;, 1, 2)))) +
  geom_sf(data = sfPoints, col = &amp;quot;red&amp;quot;, size = 3) +
  labs(x = &amp;quot;Longitude&amp;quot;, y = &amp;quot;Latitude&amp;quot;, fill = &amp;quot;&amp;quot;) +
  scale_fill_manual(values = c(&amp;quot;darkgrey&amp;quot;, &amp;quot;lightgrey&amp;quot;),
    labels = c(&amp;quot;Spain&amp;quot;, &amp;quot;Not Spain&amp;quot;)) +
  theme_bw() +
  theme(panel.grid = element_line(colour = &amp;quot;transparent&amp;quot;),
    axis.title = element_text(size = 18),
    axis.text = element_text(size = 16),
    legend.text = element_text(size = 14),
    legend.title = element_text(size = 14))

pointsMap&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-01-making-maps-with-ggplot2-and-sf_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Again, super simple. Now, I’d like to create a smaller map, that just shows Spain and any points inside it, so I can make a nice panel figure with the two maps side by side. Also, don’t forget a scale bar, which we can add using the &lt;code&gt;ggspatial&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggspatial)

# work out which points are in the polygon of interest
spainPoints &amp;lt;- st_join(sfPoints, cropMap[cropMap$ID == &amp;quot;Spain&amp;quot;, ],
  join = st_intersects)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## although coordinates are longitude/latitude, st_intersects assumes that they are planar
## although coordinates are longitude/latitude, st_intersects assumes that they are planar&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;spainZoom &amp;lt;- ggplot() +
  geom_sf(data = cropMap[cropMap$ID == &amp;quot;Spain&amp;quot;, ]) +
  annotation_scale(location = &amp;quot;br&amp;quot;, text_cex = 2) +
  geom_sf(data = spainPoints[spainPoints$ID == &amp;quot;Spain&amp;quot;, ], colour = &amp;quot;red&amp;quot;,
    size = 3) +
  theme_void() +
  theme(panel.grid = element_line(colour = &amp;quot;transparent&amp;quot;))

spainZoom&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Scale on map varies by more than 10%, scale bar may be inaccurate&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-01-making-maps-with-ggplot2-and-sf_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So, now we have our two maps sorted, we can arrange them side by side using the &lt;code&gt;cowplot&lt;/code&gt; package.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(cowplot)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## ********************************************************&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Note: As of version 1.0.0, cowplot does not change the&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   default ggplot2 theme anymore. To recover the previous&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   behavior, execute:
##   theme_set(theme_cowplot())&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ********************************************************&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot_grid(spainZoom, pointsMap, labels = &amp;quot;AUTO&amp;quot;, rel_widths = c(0.6, 1))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Scale on map varies by more than 10%, scale bar may be inaccurate&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-01-making-maps-with-ggplot2-and-sf_files/figure-html/unnamed-chunk-9-1.png&#34; width=&#34;1152&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There you have it, combining the &lt;code&gt;sf&lt;/code&gt; and newest &lt;code&gt;ggplot2&lt;/code&gt; packages allows you quickly and easily make some neat looking map figures!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The future of USEARCH: A closed source software in an open source world</title>
      <link>/post/the-future-of-usearch-a-closed-source-software-in-an-open-source-world/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/the-future-of-usearch-a-closed-source-software-in-an-open-source-world/</guid>
      <description>


&lt;p&gt;Let me start off by stating that I have enormous respect for &lt;a href=&#34;http://drive5.com/index.htm&#34;&gt;Rob Edgar&lt;/a&gt; (creator of USEARCH, UPARSE etc.). His contributions to the field of bioinformatics, and indirectly to the fields of molecular and microbial ecology have been huge, you only need to look at his citation rates to see that! So this post is not intended as a criticism of him or his work in any way.&lt;/p&gt;
&lt;p&gt;That said, I’ve recently been thinking about the deluge of new algorithms for picking Operational Taxonomic Units (OTUs) from molecular sequence datasets and wondering where, and how, USEARCH, UCLUST &lt;em&gt;et al.&lt;/em&gt; will fit in.&lt;/p&gt;
&lt;p&gt;Rob Edgar only made 32-bit versions of all his software freely available for users, with users having to pay $885 for a 64-bit license. A 32-bit license essentially limits the amount of RAM your computer can access whilst using this software to 4gb. So even if you had a cluster computer at your disposal, you’d still be shackled by the 4gb limit. A few years ago, when 454 pyrosequencing was the predominant sequencing platform, this wasn’t so much of a problem because the datasets were big, but not huge. However, with the rise of Illumina’s platforms such as the HiSeq and MiSeq, molecular datasets are becoming truly enormous, meaning more RAM is needed to work with them.&lt;/p&gt;
&lt;p&gt;Furthermore, the arrival of &lt;a href=&#34;https://github.com/torognes/vsearch&#34;&gt;VSEARCH&lt;/a&gt;, a 64-bit open source work-alike of USEARCH, has meant that you now don’t need to shell out for an expensive 64-bit license to get USEARCH-esque results. This leads me to wonder, what is the future of USEARCH now that there are good, free, and open-source alternatives?&lt;/p&gt;
&lt;p&gt;Looking at the citation rates of Edgar’s papers, there are no obvious signs of it losing popularity… Data on the plot below were downloaded using Web of Science. It will be interesting to see if the popularity of this classic piece of bioinformatics software ever declines.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2019-01-01-the-future-of-usearch-a-closed-source-software-in-an-open-source-world_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Speeding up rarefaction curves for microbial community ecology</title>
      <link>/post/speeding-up-rarefaction-curves-for-microbial-community-ecology/</link>
      <pubDate>Sat, 15 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/speeding-up-rarefaction-curves-for-microbial-community-ecology/</guid>
      <description>


&lt;p&gt;When beginning analyses on microbial community data, it is often helpful to compute rarefaction curves. A rarefaction curve tells you about the rate at which new species/OTUs are detected as you increase the number of individuals/sequences sampled. It does this by taking random subsamples from 1, up to the size of your sample, and computing the number of species present in each subsample. Ideally, you want your rarefaction curves to be relatively flat, as this indicates that additional sampling would not likely yield further species.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;vegan&lt;/code&gt; package in &lt;code&gt;R&lt;/code&gt; has a nice function for computing rarefaction curves for species by site abundance tables. However, for microbial datasets this function is often prohibitively slow. This is due to the fact that we often have large numbers of samples, and each sample may contain several thousand sequences, meaning that a large number of random subsamples are taken.&lt;/p&gt;
&lt;p&gt;Part of the problem is that the original function only makes use of a single processing core, meaning that random subsamples are computed serially, and each sample is processed serially. This means we could speed the function up by splitting samples across multiple cores. In short, we are parallelising the function.&lt;/p&gt;
&lt;p&gt;Below is my attempt to modify the original code into a function that can use multiple processor cores to speed up the calculations.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# you will need to install the parallel package before hand
# and the vegan package if you dont have it
library(vegan)
quickRareCurve &amp;lt;- function (x, step = 1, sample, xlab = &amp;quot;Sample Size&amp;quot;,
  ylab = &amp;quot;Species&amp;quot;, label = TRUE, col, lty, max.cores = T, nCores = 1, ...)
{
    require(parallel)
    x &amp;lt;- as.matrix(x)
    if (!identical(all.equal(x, round(x)), TRUE))
        stop(&amp;quot;function accepts only integers (counts)&amp;quot;)
    if (missing(col))
        col &amp;lt;- par(&amp;quot;col&amp;quot;)
    if (missing(lty))
        lty &amp;lt;- par(&amp;quot;lty&amp;quot;)
    tot &amp;lt;- rowSums(x) # calculates library sizes
    S &amp;lt;- specnumber(x) # calculates n species for each sample
    if (any(S &amp;lt;= 0)) {
        message(&amp;quot;empty rows removed&amp;quot;)
        x &amp;lt;- x[S &amp;gt; 0, , drop = FALSE]
        tot &amp;lt;- tot[S &amp;gt; 0]
        S &amp;lt;- S[S &amp;gt; 0]
    } # removes any empty rows
    nr &amp;lt;- nrow(x) # number of samples
    col &amp;lt;- rep(col, length.out = nr)
    lty &amp;lt;- rep(lty, length.out = nr)
    # parallel mclapply
    # set number of cores
    mc &amp;lt;- getOption(&amp;quot;mc.cores&amp;quot;, ifelse(max.cores, detectCores(), nCores))
    message(paste(&amp;quot;Using &amp;quot;, mc, &amp;quot; cores&amp;quot;))
    out &amp;lt;- mclapply(seq_len(nr), mc.cores = mc, function(i) {
        n &amp;lt;- seq(1, tot[i], by = step)
        if (n[length(n)] != tot[i])
            n &amp;lt;- c(n, tot[i])
        drop(rarefy(x[i, ], n))
    })
    Nmax &amp;lt;- sapply(out, function(x) max(attr(x, &amp;quot;Subsample&amp;quot;)))
    Smax &amp;lt;- sapply(out, max)
     plot(c(1, max(Nmax)), c(1, max(Smax)), xlab = xlab, ylab = ylab,
       type = &amp;quot;n&amp;quot;, ...)
    if (!missing(sample)) {
      abline(v = sample)
      rare &amp;lt;- sapply(out, function(z) approx(x = attr(z, &amp;quot;Subsample&amp;quot;),
         y = z, xout = sample, rule = 1)$y)
      abline(h = rare, lwd = 0.5)
      }
    for (ln in seq_along(out)) {
      N &amp;lt;- attr(out[[ln]], &amp;quot;Subsample&amp;quot;)
      lines(N, out[[ln]], col = col[ln], lty = lty[ln], ...)
      }
    if (label) {
      ordilabel(cbind(tot, S), labels = rownames(x), ...)
      }
    invisible(out)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Most of the code is verbatim to the original, but you’ll notice a few extra arguments to specify, and a few extra lines that determine how many cores the function will use.&lt;/p&gt;
&lt;p&gt;Essentially, if you do not specify a number of cores, the function will default to using all available cores, which will allow the quickest calculation. Otherwise, you can specify &lt;code&gt;max.cores = F&lt;/code&gt;, which will allow you to specify the number of cores using &lt;code&gt;nCores&lt;/code&gt;. This is handy if you need some cores to remain usable whilst running the function.&lt;/p&gt;
&lt;p&gt;Below is a usage example.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load some dummy data
data(dune)

quickRareCurve(dune) # will use all cores and print how many cores you have

quickRareCurve(dune, max.cores = F, nCores = 2) # use only 2 cores&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-15-speeding-up-rarefaction-curves-for-microbial-community-ecology_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We can compare the new function’s performance to the original using the &lt;code&gt;microbenchmark&lt;/code&gt; package, as below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(microbenchmark)

microbenchmark(rarecurve(dune), quickRareCurve(dune), times = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-15-speeding-up-rarefaction-curves-for-microbial-community-ecology_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Unit: milliseconds
##                  expr      min       lq     mean   median       uq
##       rarecurve(dune) 66.17685 68.48628 69.23183 69.31825 70.03621
##  quickRareCurve(dune) 47.80692 82.52621 80.46054 84.06839 85.01588
##       max neval
##  71.67020    10
##  86.31908    10&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, for this small dataset, the original function is actually quicker. This is because it takes some time to distribute tasks among the cores. However, I’ve found for a typical OTU table (&amp;gt; 50 samples, ~ 15,000 sequences per sample), &lt;code&gt;quickRareCurve&lt;/code&gt; can be around 3 times faster.&lt;/p&gt;
&lt;p&gt;Feel free to use it as you wish, but I’d be very grateful for any credit if you do use it.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ggplot2; From default to delightful</title>
      <link>/post/ggplot2-from-default-to-delightful/</link>
      <pubDate>Sat, 15 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/ggplot2-from-default-to-delightful/</guid>
      <description>


&lt;p&gt;If you use &lt;code&gt;R&lt;/code&gt; to analyse and plot your data, then you’ve probably heard of and used the &lt;code&gt;ggplot2&lt;/code&gt; package, written by Hadley Wickham. &lt;code&gt;ggplot2&lt;/code&gt; is a highly flexible plotting package allowing you to create just about any kind of plot you can think of, and customise just about any aspect of your plot.&lt;/p&gt;
&lt;p&gt;However, &lt;code&gt;ggplot2&lt;/code&gt; is also known for it’s somewhat strange choice of default options (at least, they seem strange to me!). Therefore, it can seem like a lot of work to go from a basic plot to something that is approaching publication quality. Whilst there are many excellent guides out there to help with this process, I thought I’d weigh in with a few tips and personal preferences which have helped me to elevate my plots to another level.&lt;/p&gt;
&lt;p&gt;Let’s begin by loading the package and simulating some data to play around with.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)

myData &amp;lt;- data.frame(var1 =  runif(1000, 0, 100), var2 = runif(1000, -10, 10),
  catVar = rep(c(1:4), times = 250))

head(myData)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##       var1       var2 catVar
## 1 85.24026 -2.1282043      1
## 2 37.48931 -3.9379807      2
## 3 49.32137  0.8810468      3
## 4 35.36416 -1.3170033      4
## 5 88.40480 -6.7371313      1
## 6 97.84301 -4.8468379      2&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Hopefully that code should be fairly easy to understand, we’ve simulated two continuous variables (with a random distribution) and created a categorical variable.&lt;/p&gt;
&lt;p&gt;Now let’s create the most basic scatter plot possible with this data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot1 &amp;lt;- ggplot(myData, aes(x = var1, y = var2)) + geom_point()
plot1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-15-ggplot2-from-default-to-delightful_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt; That looks ok, but it’s not perfect. The text and points are quite small, the black points are bit abrasive on the eye and the grey background is a little distracting. Additionally, the axis labels are not informative and we are also obscuring potentially interesting trends by plotting all of the data from our different groups together. So, lets set about fixing those things.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(myData, aes(x = var1, y = var2)) +
  geom_point(size = 2, color = &amp;quot;grey&amp;quot;) +
  facet_wrap(~catVar)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-15-ggplot2-from-default-to-delightful_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;So now the points are a little bigger and grey, which makes them easier to look at and with an aesthetically nicer muted contrast. We’ve also split the data into 4 separate panels, so that we can seen any group specific trends far easier, and without the visual clutter of lots of different colour points (ideal when there is a monetary cost to publishing color figures). But that grey background is now obscuring things, and our axis labels are still hard to read and uninformative. Let’s fix those.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(myData, aes(x = var1, y = var2)) +
  geom_point(size = 2, color = &amp;quot;grey&amp;quot;) +
  facet_wrap(~catVar) +
  labs(x = &amp;quot;Variable 1&amp;quot;, y = &amp;quot;Variable 2&amp;quot;) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-15-ggplot2-from-default-to-delightful_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We’re starting to look quite good now, the points are now much more visible, yet not too harsh against the white background, and our axis labels now tell us something about the data (if it were real!). Now we can apply some final polish to really get this looking great!&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;finalPlot &amp;lt;- ggplot(myData, aes(x = var1, y = var2)) +
  geom_point(size = 2, color = &amp;quot;grey&amp;quot;) +
  facet_wrap(~catVar) +
  labs(x = &amp;quot;Variable 1&amp;quot;, y = &amp;quot;Variable 2&amp;quot;) +
  theme_bw() +
  theme(axis.text = element_text(size = 16),
    axis.title = element_text(size = 18),
    strip.text.x = element_text(size = 16),
    panel.grid.major = element_blank())&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Done! The text around the plot is now much easier to see, and we’ve removed the major gridlines which means less distraction from the data. Everything is far clearer and easier to read.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;plot1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-15-ggplot2-from-default-to-delightful_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;finalPlot&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-15-ggplot2-from-default-to-delightful_files/figure-html/unnamed-chunk-6-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;One final tip, save your graphs as pdfs. PDFs are known as vector based graphics and maintain resolution far better than bitmap based graphics such as PNGs, JPEGs or TIFs. The added bonus is that many journals also request your figures to be in a vector based format, so you’re also helping your figures become published.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggsave(&amp;quot;myPlot.pdf&amp;quot;, finalPlot, width = 8, height = 8, device = &amp;quot;pdf&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Barking up the wrong tree? Leaf your troubles behind with dendextend</title>
      <link>/post/barking-up-the-wrong-tree-leaf-your-troubles-behind-with-dendextend/</link>
      <pubDate>Wed, 12 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/barking-up-the-wrong-tree-leaf-your-troubles-behind-with-dendextend/</guid>
      <description>


&lt;p&gt;I was recently asked by one of my PhD supervisors to help out on a paper by doing some metagenomic analyses. My mission was essentially to perform some taxonomic analyses of metagenomes and show how a metagenome generated in our lab related to these. So, naturally, I said yes, carried out the necessary analyses and proceeded to design a figure to show the result. I figured a dendrogram would be a nice way of showing compositional similarity between the community we studied and other communities. First of all, I set about creating a distance matrix and plotting the dendrogram like so:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(vegan)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: permute&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Loading required package: lattice&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## This is vegan 2.5-6&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;data(dune)  # load some dummy data
commDist &amp;lt;- vegdist(dune, &amp;quot;jaccard&amp;quot;)  # calculate the community similarity distance matrix
plot(hclust(commDist))&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-12-barking-up-the-wrong-tree-leaf-your-troubles-behind-with-dendextend_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There you go. It’s easy to generate a basic dendrogram using any sort of distance matrix. So I then started tidying it up to make it a bit more presentable.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;siteLabels &amp;lt;- paste(&amp;quot;Sample &amp;quot;, rownames(dune), sep = &amp;quot;&amp;quot;)  # create some sample labels
plot(hclust(commDist), labels = siteLabels, main = &amp;quot;&amp;quot;, sub = &amp;quot;&amp;quot;, xlab = &amp;quot;&amp;quot;, ylab = &amp;quot;Jaccard Similarity&amp;quot;, lwd = 2, cex = 1.2)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-12-barking-up-the-wrong-tree-leaf-your-troubles-behind-with-dendextend_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Much nicer! We’ve gotten rid of the nonsense at the bottom, given a more informative y axis label and generally made things ‘nicer’. Except, when I saw the first draft, I saw my supervisor had rotated the plot so that the leaf tips pointed to the left. For some reason this just looked wrong to my eyes and so I opted to replot the dendrogram, in a more sensible orientation.&lt;/p&gt;
&lt;p&gt;Note that to do this, we have to change our approach slightly. For some stupid reason, when we want to plot a dendrogram horizontally, we have to coerce the hclust output to dendrogram class. Additionally, the ‘plot’ method for a dendrogram class object no longer accepts the labels parameter, so we must rename the rows of our dataframe and recalculate the distance matrix instead.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rownames(dune) &amp;lt;- siteLabels  # rename rows of dataframe with our sample labels created earlier

commDist &amp;lt;- vegdist(dune, &amp;quot;jaccard&amp;quot;)  # recalculate the distance matrix so that it features our new sample names

par(mar = c(5, 1, 1, 5))  # adjust margins to make room for tip labels

# replot the dendrogram, note that we can now remove the &amp;quot;main =&amp;quot; and &amp;quot;sub =&amp;quot; arguments
# also remember to switch the x and y labels!
plot(as.dendrogram(hclust(commDist)), xlab = &amp;quot;Jaccard Similarity&amp;quot;, lwd = 2, cex = 1.2, horiz = T)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-12-barking-up-the-wrong-tree-leaf-your-troubles-behind-with-dendextend_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Perfect! Or at least I thought so. Turns out my supervisor liked the improved orientation but didn’t like the way the branches all ended at the same point. That should be easy to correct right? Wrong!&lt;/p&gt;
&lt;p&gt;I spent near enough an entire day trying to get this plot perfect without success. I went as far as exploring the &lt;code&gt;ggdendro&lt;/code&gt; package which allows plotting of dendrograms in a ggplot-esque manner.&lt;/p&gt;
&lt;p&gt;Then I stumbled on a solution, enter &lt;code&gt;dendextend&lt;/code&gt;! A quick peek of the package manual reveals some really awesome capabilities, I really urge you to take a &lt;a href=&#34;https://cran.r-project.org/web/packages/dendextend/vignettes/introduction.html&#34;&gt;look&lt;/a&gt; as some of the figures you can create are amazing. For my humble needs, this package solved all my problems easily, and in a couple of lines I’d created exactly the figure my supervisor wanted.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ifelse(&amp;quot;dendextend&amp;quot; %in% rownames(installed.packages()) == T, library(dendextend), install.packages(dendextend))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Registered S3 method overwritten by &amp;#39;dendextend&amp;#39;:
##   method     from 
##   rev.hclust vegan&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## ---------------------
## Welcome to dendextend version 1.12.0
## Type citation(&amp;#39;dendextend&amp;#39;) for how to cite the package.
## 
## Type browseVignettes(package = &amp;#39;dendextend&amp;#39;) for the package vignette.
## The github page is: https://github.com/talgalili/dendextend/
## 
## Suggestions and bug-reports can be submitted at: https://github.com/talgalili/dendextend/issues
## Or contact: &amp;lt;tal.galili@gmail.com&amp;gt;
## 
##  To suppress this message use:  suppressPackageStartupMessages(library(dendextend))
## ---------------------&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;dendextend&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:permute&amp;#39;:
## 
##     shuffle&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:stats&amp;#39;:
## 
##     cutree&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;dendextend&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# uncomment following line if this is the fist time you&amp;#39;ve installed this package!
# library(dendextend)
dend1 &amp;lt;- as.dendrogram(hclust(commDist))
wellHung &amp;lt;- hang.dendrogram(dend1)  # the cheeky variable names are absolutely essential!
plot_horiz.dendrogram(wellHung, side = F, xlab = &amp;quot;Jaccard Dissimilarity&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-12-barking-up-the-wrong-tree-leaf-your-troubles-behind-with-dendextend_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;There you go, an awesome package which save me from wasting too many more days of work!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Does the Adonis test really exist?</title>
      <link>/post/does-the-adonis-test-really-exist/</link>
      <pubDate>Wed, 12 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/does-the-adonis-test-really-exist/</guid>
      <description>


&lt;p&gt;I’ve recently started a chapter for my PhD which involves a meta-analysis of microbial ecological literature. This has involved looking through the methods, or more specifically the stats sections, of lots of studies to see whether they are suitable for inclusion in my analysis. One idiosyncracy I noted whilst doing this is the number of studies that refer to an “Adonis test”. As far as I am aware, &lt;code&gt;adonis&lt;/code&gt; is simply the name of the function that carries out a permutational MANOVA (multivariate analysis of variance) or non-parametric MANOVA as the original author of this statistical test calls it (Anderson, 2001).&lt;/p&gt;
&lt;p&gt;A quick google search appears to suggest that one potential difference might be that the statistical test carried out by the &lt;code&gt;adonis()&lt;/code&gt; function is capable of handling both categoric and continuous predictors. In contrast, non-parametric MANOVA is only able to handle categoric predictors. This difference sort of makes sense, but I’m still not convinced that we should be using the term “adonis test”.&lt;/p&gt;
&lt;p&gt;I was also interested to see how many microbial ecology studies actually refer to this so called “adonis test” to see whether we are to blame for this confusion. I conducted a highly rigorous and robust google scholar search (yep, really!) on the terms “adonis test” and “adonis test” + microb* and limited results from 2001-present.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-12-12-does-the-adonis-test-really-exist_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;As it turns out, 84% of the studies that mention “adonis test” also feature the word “microb*“, suggesting that we might be to blame! Also, only 11 of these studies used the term”MANOVA“, suggesting that the vast majority did not elaborate on the statistical procedure initiated by the &lt;code&gt;adonis&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;If I’m completely wrong on this and there is such a thing as an adonis test then please do tell me. But it seems to be that we need to be a little more careful when describing the statistical methods we use…&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Know Your Fungi: A Brief Demo Of The New Fungal Classifier</title>
      <link>/post/know-your-fungi-a-brief-demo-of-the-new-fungal-classifier/</link>
      <pubDate>Wed, 12 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/know-your-fungi-a-brief-demo-of-the-new-fungal-classifier/</guid>
      <description>


&lt;p&gt;As a microbial ecologist, part of my job is to try and assign taxonomy to all of the microbial critters living in the habitats I study. For archaeal or bacterial 16S rRNA gene sequences this is relatively easy. The Ribosomal Database Project have a naive Bayesian classifier which is trained on a large curated database of archaeal and bacterial 16S rRNA gene sequences. Best of all, it is implemented in the popular bioinformatics pipeline &lt;code&gt;Qiime&lt;/code&gt;, making it nice and easy to apply to your own data.&lt;/p&gt;
&lt;p&gt;But what if you are for dealing with fungal ITS sequences instead? Well a recent paper from &lt;a href=&#34;http://www.mycologia.org/content/early/2015/10/28/14-293.abstract&#34;&gt;Deshpande et al&lt;/a&gt; piqued my interest for several reasons. The paper describes a new curated database of fungal ITS sequences and demonstrates that the RDP naive Bayesian classifier does a decent job of classifying ITS sequences when trained on this database.&lt;/p&gt;
&lt;p&gt;Unfortunately, using the RDP classifier on ITS sequences means going off piste so to speak and using the standalone classifier outside of &lt;code&gt;Qiime&lt;/code&gt;. Given that most people never use the classifier outside of &lt;code&gt;Qiime&lt;/code&gt;, I thought a brief demonstration might be useful. For all of the following, I’m assuming you’re using BioLinux and are at least somewhat familiar with the Linux terminal.&lt;/p&gt;
&lt;p&gt;First of all, clone the RDPtools github repo like so:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;git clone https://github.com/rdpstaff/RDPTools.git&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, enter the newly created directory and set up the RDPtools software:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;cd RDPTools
git submodule init
git submodule updat
make&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You’ll get a lot of text whizzing through the terminal, don’t worry, you haven’t yet entered the Matrix! Once these processes finish the classifier has installed and is ready for use. However, we must now download some additional files which will allow the classifier to classify our fungal ITS sequences. To download these files, type:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;wget &amp;quot;http://rdp.cme.msu.edu/download/rdpclassifiertraindata/data.tgz&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This may take a few minutes depending on your connection speed. The files are packaged in what we call a tarball (kind of like a windows zip folder). To unpack them:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;tar -zxvf data.tgz&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now when we list the contents of our directory, we can see a new directory, “data”, has been created. This folder contains all the files we need to use the Warcup based classifier (and also the UNITE and LSU data handily).&lt;/p&gt;
&lt;p&gt;I’m going to assume you’ve already clustered your ITS sequences into operational taxonomic units and picked a representative sequence for each OTU using &lt;a href=&#34;http://qiime.org/scripts/pick_rep_set.html&#34;&gt;Qiime&lt;/a&gt; or other means.&lt;/p&gt;
&lt;p&gt;Now we simply call the classifier, tell it to use the Warcup classification files, and tell it where out data is. Simple:&lt;/p&gt;
&lt;pre class=&#34;bash&#34;&gt;&lt;code&gt;java -Xmx1g -jar classifier.jar classify -t data/classifier/fungalits_warcup/rRNAClassifier.properties -o classifiedFungi.txt /path/to/data/fungalITS_repSet.fasta&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A new file (“classifiedFungi.txt”) will have been created, containing the taxonomic assignments of each fungal sequence, complete with confidence values, pretty cool! You can of course train the classifier yourself on a custom database, in which case, follow the steps &lt;a href=&#34;https://github.com/rdpstaff/classifier&#34;&gt;here&lt;/a&gt; Anyway, I hope this was useful and demonstrates that venturing outside of &lt;code&gt;Qiime&lt;/code&gt; is not necessarily that difficult!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Merging Taxonomy With Non-Qiime OTU Tables</title>
      <link>/post/merging-taxonomy-with-non-qiime-otu-tables/</link>
      <pubDate>Wed, 12 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/merging-taxonomy-with-non-qiime-otu-tables/</guid>
      <description>


&lt;p&gt;This is a quick post more to document some useful code than anything else. When conducting bioinformatic analyses using &lt;code&gt;Qiime&lt;/code&gt;, one of the last steps is to cluster sequences into OTUs (operational taxonomic units) and assign taxonomy to them. You can then make an OTU table which contains all your OTUs and their associated taxonomy. Bam, easy!&lt;/p&gt;
&lt;p&gt;But what to do if you haven’t/don’t want to use &lt;code&gt;Qiime&lt;/code&gt; to cluster OTUs? For example, I often use &lt;code&gt;Vsearch&lt;/code&gt; to cluster my OTUs and then may use &lt;code&gt;Qiime&lt;/code&gt; to assign taxonomy via the RDP classifier. This means we need some way of uniting our already constructed OTU table with our taxonomy file.&lt;/p&gt;
&lt;p&gt;Here is a short piece of &lt;code&gt;R&lt;/code&gt; code which will do the job nicely.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# First, let&amp;#39;s read in our OTU table
otuTable &amp;lt;- read.delim(&amp;quot;archOtuTab.txt&amp;quot;, header = T)

# now to read in the taxonomy file generated by the RDP classifier in Qiime
taxa &amp;lt;- read.table(&amp;quot;archaeaCentroids_tax_assignments.txt&amp;quot;,
    header = F, col.names= c(&amp;quot;OTUId&amp;quot;, &amp;quot;taxonomy&amp;quot;, &amp;quot;confidence&amp;quot;),
    stringsAsFactors = F, sep = &amp;quot;\t&amp;quot;)

otuTable &amp;lt;- merge(otuTable, taxa[, -3], by = &amp;quot;OTUId&amp;quot;)
# this line then merges the OTU table with taxonomy. Order of OTUs/taxonomy is
# not important, R will correctly match OTUs with their taxonomy string
# Note that this line will not print the confidence column in the final OTU
# table as it is often not that useful...

# now we just need to write our completed OTU table to a file
write.table(otuTable, &amp;quot;archaeaOtuTableTaxonomy.txt&amp;quot;,
    sep = &amp;quot;\t&amp;quot;, quote = F, row.names = F)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Done! A simple way of uniting taxonomy information with a standalone OTU table without using &lt;code&gt;Qiime&lt;/code&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Privacy Policy</title>
      <link>/privacy/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      
      <guid>/privacy/</guid>
      <description>&lt;p&gt;&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Streams of data from drops of water: 21st century molecular microbial ecology</title>
      <link>/publication/clark_et_al_wires/</link>
      <pubDate>Thu, 01 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/publication/clark_et_al_wires/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
